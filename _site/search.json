[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHC 6099: R Computing for Health Sciences",
    "section": "",
    "text": "These are the written lecture materials for the class PHC 6099 at Florida International University’s Stempel College of Public Health. This is the second semester of the “R” course sequence (the first semester is PHC6701; the text for that class is available here: https://gabrielodom.github.io/PHC6701_r4ds/) The source code and data sets for this book are available here: https://github.com/gabrielodom/PHC6099_rBiostat."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lessons_original/01_ggplot2.html",
    "href": "lessons_original/01_ggplot2.html",
    "title": "How to create a scatterplot",
    "section": "",
    "text": "Scatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#introduction",
    "href": "lessons_original/01_ggplot2.html#introduction",
    "title": "How to create a scatterplot",
    "section": "",
    "text": "Scatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#data",
    "href": "lessons_original/01_ggplot2.html#data",
    "title": "How to create a scatterplot",
    "section": "Data",
    "text": "Data\nData was obtained from GapMinder for each health and non-health indicator and combined into one data set. The data includes information on over 180 countries and territories from the years 1800 to 2099. Countries and territories with missing information were not excluded from the data set as the lack of information can also be looked into and shed light on why data was not collected or provided.\nTo determine whether a country’s health and income outcomes are influenced by population sizes and GDP per capita, the data will be used to create a series of graphs to view different trends. It is important to note certain analysis’ will only be done on specific countries and on certain years. Predictive values were provided up until 2099 however, we will focus on years with full and current data.\n\n\nCode\n# Contains colour palette for ggplot\nlibrary(viridis)\n\n# Contains \"gganimate\"\nlibrary(ggplot2)\nlibrary(gganimate)\n\nlibrary(tidyverse)\n\n\n# Reads csv file\ngapminder_data &lt;-  \n  # read_csv(\"clean_data/gapminder_scatterplot.csv\")\n  read_csv(\"../data/gapminder_2024spring.csv\")"
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#how-to-create-a-scatter-plot",
    "href": "lessons_original/01_ggplot2.html#how-to-create-a-scatter-plot",
    "title": "How to create a scatterplot",
    "section": "How to create a scatter-plot",
    "text": "How to create a scatter-plot\n\nIntro to ggplot2\nGgplot2 is a package used to create graphs and visualize data. The main three components of ggplot2 are the data, aesthetics and geom layers.\n\nThe data layer - states what data will be used to graph\nThe aesthetics layer - specifies the variables that are being mapped\nThe geom layer - specifies the type of graph to be produced\n\n\n\n\n\n\n\n\nBasic scatter-plot using ggplot2\nIn order to create a scatter-plot using ggplot, you must specify what data you will be using, state which variables will be mapped and how under aesthetics. What differentiates the scatter-plot from any other type of graph will be specified under the geom layer. For the scatter-plot, geom_point will be used.\nIn this example, we will analyze the relationship between fertility rates and gdp per capita for each country in 2011.\n\nfig_bubble_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(x = gdp_per_capita, y = fertility) +\n  geom_point()\n\nfig_bubble_2011 \n\n\n\n\n\n\n\n\n\n\nElevating your scatter-plot\nIn the example above, we have mapped out fertility as our y-axis and gdp per capita as our x-axis. However, at it’s very basic level, there is not enough information provided to accurately analyze the relationship between the two. For this reason, we can add additional layers that will provide more information to properly analyze the scatter-plot.\n\n\nCode\nfig_bubble_pretty_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    # will change the size of the point based on population size \n    size = population, \n    # will assign colors based on the continent the country is in \n    color = continent\n  ) +\n  # gives a range as to how big or small the points of population should be\n  scale_size(range = c(1, 20)) + \n  # removes N/A from the legend and titles it Continent \n  scale_colour_discrete(na.translate = F, name = \"Continent\") +\n  # removes population size from the legend \n  guides(size = \"none\") +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    # transforms numbers from scientific notation to regular number \n    labels = scales::comma\n  ) +\n  labs(\n    title = \"Fertility rate descreases as GDP per capita increases in 2011\",\n    y = \"Fertility rates\",\n    caption = \"Source: Gapminder\"\n  ) +\n  # the ylim was set based on the fertility, lowest was 1.15 & highest was 7.25\n  ylim(1.2, 8.0) +\n  # alpha increases transparency of the points to ensure they can all be seen\n  geom_point(alpha = 0.5) \n\nfig_bubble_pretty_2011\n\n\n\n\n\n\n\n\n\n[@ggplot-2011-adv] builds on the previous scatterplot of Fertility Rates (y axis) against GDP per capita (x axis) for 2011. The bubble size depicts respective country populations, and continents are coded by colors according to the key. This figure displays a negative relationship between GDP per capita and Fertility Rates. It supports the Hypothesis which states that as GDP per capita increases, Fertility Rates decreases. This trend can be confirmed for all continents, however, the degree to which fertility rates drop between continents varies. Most European country appear below a fertility rate of 2 babies per woman. The Americas appear to follow closely behind (under 4), followed by Oceania and Asia. A significant number of African countries still maintained higher fertility rates with lower GDP per capita for 2011.\n\n\nFacets\nHere is an example of wanting to create four separate graphs to see the relationship between fertility rates and GDP per capita based on the years 1860, 1910, 1960 and 2010. In this example we omitted the facet argument.\n\n\nCode\nfig_bubble_multiple &lt;-\n  ggplot(data = filter(gapminder_data, year %in% c(1860, 1910, 1960, 2010))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE, name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    subtitle = \"throughout 1860, 1910, 1960 and 2010\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) \n\nfig_bubble_multiple\n\n\n\n\n\n\n\n\n\nWithout having used the facet argument, all points of all four years have been included into one graph. This graph does not provide us with the information we were looking for.\n\n\nCode\nfig_bubble_multiple_facet &lt;-\n  ggplot(data = filter(gapminder_data, year %in% c(1860, 1910, 1960, 2010))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE , name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    subtitle = \"throughout 1860, 1910, 1960 and 2010\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) +\n  # specifiying we want the graphs split based on year\n  facet_wrap(~ year)\n\nfig_bubble_multiple_facet\n\n\n\n\n\n\n\n\n\nNow that we’ve specified the facet argument, we now have four seperate graphs that can be properly analysed. In [@ggplot-facet-years] we see an increasingly negative relationship between the two variables over time. This observation is congruent with the hypothesis that as GDP per capita increases, fertility decreases.\nThis global trend can be attributed to the increasing proportion of women in the workforce in the mid to late 20th century. As a result of World War II (1939-1945), women took on roles outside the home to compensate for men at war. Despite increased GDP per capita, this may have contributed to reduced fertility (babies per woman) over time. During 1860 - 1910, the scatter-plot figures remained in the upper left quadrant with the numbers remaining between 2 - 8 babies per woman. In 1960, a clear disparity among continents is seen. Most European countries’ fertility rates fell below 5, while their GDP per capita increased. Most African countries maintained high fertility rates above 5, but little change is seen in GDP per capita. The Asian continent shows the most variation among countries during that year. Some smaller Asian countries continued to maintain high fertility rates as GDP per capita increased in 1960. However, others displayed a drastic decrease in fertility rates by 1960. The Americas followed a steady decline over the years. By 2010, an overall negative relationship can be seen with most countries’ fertility rates below 5 babies per woman.\n\nfig_bubble_row_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent, nrow = 1)\n\nfig_bubble_row_2011 \n\n\n\n\n\n\n\n\nIn the graph above, we see an example of seperating the single graph into graphs based on continent. It has also been specified to have all graphs appear in one single row through the nrow argument. However, this graph is also unclear and cannot be used to compare the relationship between fertility and gdp per capita.\n\nfig_bubble_facet_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent)\n\nfig_bubble_facet_2011 \n\n\n\n\n\n\n\n\nIn the next example above, we removed the nrow argument and the system automatically seperated the graphs into three columns with two rows. However, again, there is no way to clearly determine any relationship between fertility and gdp per capita."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#scatterplot-animation",
    "href": "lessons_original/01_ggplot2.html#scatterplot-animation",
    "title": "How to create a scatterplot",
    "section": "Scatterplot animation",
    "text": "Scatterplot animation\nGGplot2 contains the “gganimate” package that allows for animation of data. It enhances data visualization through real-time outputs. In this case the gapminder data will be filtered to 2011 and below (full data available).\n\n\nCode\ngapminder_df &lt;- \n  gapminder_data %&gt;% \n  # Excludes data beyond 2011 (last year with complete data)\n  filter(year &lt;= \"2011\")\n\nfig_animate &lt;- \n  ggplot(gapminder_df) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent \n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  # Assigns color palette \n  scale_color_viridis_d() +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  geom_point(show.legend = TRUE, alpha = 0.7) +\n  # Assigns the gganimate features\n  transition_time(year) +\n  ease_aes('linear', interval = 2.0) +\n  # Prints time of current frame\n  labs(title = \"Year: {frame_time}\", x = \"GDP per capita\", y = \"Fertility\")\n  \n\nfig_animate\n\n\nNULL\n\n\n@plot-animate depicts the changes between Fertility and GDP per capita as the years increase from 1799 to 2011 (last full data year). This allows real-time visualization of the decrease in fertility and increase in GDP per capita.\n\n\nCode\nfacet_animate &lt;- \n  ggplot(gapminder_df) +\n  aes(\n    x = gdp_per_capita, \n    y = fertility,\n    size = population, \n    colour = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  # Groups output by continents\n  facet_wrap(~continent) +\n  labs(\n    title = 'Year: {closest_state}', \n    x = 'GDP per capita', \n    y = 'fertility'\n  ) +\n  geom_point(alpha = 0.7, show.legend = TRUE) +\n  # Contains gganimate features\n  transition_states(year, transition_length = 3, state_length = 1) +\n  # Animation pattern, time between each state\n  ease_aes('linear', interval = 2.0)\n\nfacet_animate\n\n\nNULL\n\n\nIn @animate-facets, the ggplot data for various continents as time passes is shown to support the initial hypothesis."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#conclusion",
    "href": "lessons_original/01_ggplot2.html#conclusion",
    "title": "How to create a scatterplot",
    "section": "Conclusion",
    "text": "Conclusion\nA global negative trend is depicted between GDP per capita and fertility over time. Such changes were due to wars as well as social, cultural and economic changes that incentivize smaller families especially in Asian countries. Most European, American and Asian countries depicted significant decreases in fertility rates over time as GDP per capita increased. On the other hand, African countries remain in the top rank for fertility over the years. These differences are depicted in the population pyramid changes of developed vs developing countries. Public health policies can be tailored to incentivizing increased fertility in developed countries to ensure generation continuity, and effective family planning strategies in developing countries."
  },
  {
    "objectID": "lessons_original/01_rayshader.html",
    "href": "lessons_original/01_rayshader.html",
    "title": "R Rayshader Overview",
    "section": "",
    "text": "R rayshader is an R package that allows users to generate high-quality 3D maps, visualizations, and animations.\nrayshader also allows the user to translate ggplot2 objects into beautiful 3D data visualizations.\n\nTo install rayshader, you can use the following code in R:\n\n# remotes::install_github(\n#   \"tylermorganwall/rayshader\"\n# )\n\n# remotes::install_cran(\"rayrender\")"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#overview",
    "href": "lessons_original/01_rayshader.html#overview",
    "title": "R Rayshader Overview",
    "section": "",
    "text": "R rayshader is an R package that allows users to generate high-quality 3D maps, visualizations, and animations.\nrayshader also allows the user to translate ggplot2 objects into beautiful 3D data visualizations.\n\nTo install rayshader, you can use the following code in R:\n\n# remotes::install_github(\n#   \"tylermorganwall/rayshader\"\n# )\n\n# remotes::install_cran(\"rayrender\")"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#functions",
    "href": "lessons_original/01_rayshader.html#functions",
    "title": "R Rayshader Overview",
    "section": "Functions",
    "text": "Functions\n\nrayshader 0.35. 1 has 56 functions and 4 datasets\nseven functions related to mapping\nalso has functions to add water and generate overlays\nalso included are functions to add additional effects and information to 3D visualizations\nfunctions for converting rasters to matrices\nfunctions to display and save your visualizations\nrayshader has a function to generate 3D plots using ggplot2 objects"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#example",
    "href": "lessons_original/01_rayshader.html#example",
    "title": "R Rayshader Overview",
    "section": "Example",
    "text": "Example\nFirst we load all the required libraries. These libraries are required for various functions and operations used in creating 3D maps with rayshader.\n\nlibrary(rayshader)\nlibrary(rayrender) \nlibrary(reshape2)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#example-1",
    "href": "lessons_original/01_rayshader.html#example-1",
    "title": "R Rayshader Overview",
    "section": "Example",
    "text": "Example\nThen, we download and load the data\n\n# Here, I load a map with the raster package.\nloadzip &lt;- tempfile() \n\ndownload.file(\"https://tylermw.com/data/dem_01.tif.zip\", loadzip)\n\nlocaltif &lt;- raster::raster(\n  unzip(loadzip, \"dem_01.tif\")\n)\n\nunlink(loadzip)\n\n# write_rds(localtif, \"../data/01_rayshader_eg_20240503.rds\")\n\nIn this code snippet, we create a temporary file (loadzip) to store the downloaded zip file from the specified URL. The download.file() function is used to download the file, and unzip() is used to extract the “dem_01.tif” file from the downloaded zip. Finally, we load the raster data into the localtif object."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#create-map",
    "href": "lessons_original/01_rayshader.html#create-map",
    "title": "R Rayshader Overview",
    "section": "Create Map",
    "text": "Create Map\nTo create a map first we need to convert this raster data file into a matrix using raster_to_matrix()\n\n#And convert it to a matrix:\nelmat &lt;- raster_to_matrix(localtif)\n\nLoading required package: raster\n\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nThen we use sphere_shade() and plot_map() to create our base map\n\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nHere, elmat is a matrix created from the raster data using the raster_to_matrix() function. sphere_shade() applies shading to the elevation matrix, giving it a 3D effect. The texture parameter specifies the type of texture to be applied. In this case, it uses the “desert” texture. Finally, plot_map() is used to display the shaded map."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#add-water-layer",
    "href": "lessons_original/01_rayshader.html#add-water-layer",
    "title": "R Rayshader Overview",
    "section": "Add Water Layer",
    "text": "Add Water Layer\nWe can add a water layer to the map using detect_water() and add_water()\n\n# detect_water and add_water adds a water layer to the map:\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nIn this code snippet, detect_water() function detects water areas in the elevation matrix. Then, add_water() adds a water layer to the map using the detected water areas. The color parameter specifies the color of the water. Finally, plot_map() is used to display the map with the water layer."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#add-shadow-layer",
    "href": "lessons_original/01_rayshader.html#add-shadow-layer",
    "title": "R Rayshader Overview",
    "section": "Add Shadow Layer",
    "text": "Add Shadow Layer\nWe can also add shadow layer in the map.\n\n# And here we add an ambient occlusion shadow layer, which models lighting\n#   from atmospheric scattering:\n\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  add_shadow(ray_shade(elmat), 0.5) %&gt;%\n  add_shadow(ambient_shade(elmat), 0) %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nHere, add_shadow() is used to add a shadow layer to the map. ray_shade() calculates shadows based on the elevation matrix (elmat). The zscale parameter controls the strength of the shadows. ambient_shade() generates ambient lighting for the map. The second parameter of add_shadow() specifies the opacity of the shadows. Finally, plot_map() displays the map with shadows."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#convert-to-3d",
    "href": "lessons_original/01_rayshader.html#convert-to-3d",
    "title": "R Rayshader Overview",
    "section": "Convert to 3D",
    "text": "Convert to 3D\nWe can convert this 2D map into 3D mapping using plot_3d() (by passing a texture map into the plot_3d function)\n\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  add_shadow(ray_shade(elmat, zscale = 3), 0.5) %&gt;%\n  add_shadow(ambient_shade(elmat), 0) %&gt;%\n  plot_3d(\n    elmat, zscale = 10, fov = 0, theta = 135,\n    zoom = 0.75, phi = 45, windowsize = c(1000, 800)\n  )\nSys.sleep(0.2)\nrender_snapshot()\n\n\n\n\n\n\n\n\nWe can add a scale bar, as well as a compass using render_scalebar() and render_compass()\n\nrender_camera(fov = 0, theta = 60, zoom = 0.75, phi = 45)\nrender_scalebar(\n  limits = c(0, 5, 10),\n  label_unit = \"km\",\n  position = \"W\",\n  y = 50,\n  scale_length = c(0.33,1)\n)\nrender_compass(position = \"E\")\nrender_snapshot(clear = TRUE)\n\n\n\n\n\n\n\n\nHere, render_camera() sets the camera properties for the 3D map. render_scalebar() adds a scale bar to the map. The limits parameter specifies the limits of the scale bar, label_unit provides the label for the scale, position sets the position of the scale bar, y controls the vertical position, and scale_length determines the length of the scale bar. render_compass() adds a compass to the map, and render_snapshot() captures the final image of the map."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#d-plotting-with-rayshader-and-ggplot2",
    "href": "lessons_original/01_rayshader.html#d-plotting-with-rayshader-and-ggplot2",
    "title": "R Rayshader Overview",
    "section": "3D plotting with rayshader and ggplot2",
    "text": "3D plotting with rayshader and ggplot2\nRayshader can also be used to make 3D plots out of ggplot2 objects using the plot_gg() function\n\nggdiamonds = ggplot(diamonds) +\n  stat_density_2d(\n    aes(\n      x = x, y = depth, fill = stat(nlevel)\n    ), \n    geom = \"polygon\", n = 200, bins = 50,contour = TRUE\n  ) +\n  facet_wrap(clarity~.) +\n  scale_fill_viridis_c(option = \"A\")\n\npar(mfrow = c(1, 2))\n\nplot_gg(ggdiamonds, width = 5, height = 5, raytrace = FALSE, preview = TRUE)\n\nWarning: `stat(nlevel)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(nlevel)` instead.\n\n\n\n\n\n\n\n\nplot_gg(\n  ggdiamonds, \n  width = 5, \n  height = 5, \n  multicore = TRUE, \n  scale = 250, \n  zoom = 0.7, \n  theta = 10, \n  phi = 30, \n  windowsize = c(800, 800)\n)\nSys.sleep(0.2)\nrender_snapshot(clear = TRUE)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#contour-plot",
    "href": "lessons_original/01_rayshader.html#contour-plot",
    "title": "R Rayshader Overview",
    "section": "Contour Plot",
    "text": "Contour Plot\nRayshader will automatically ignore lines and other elements that should not be mapped to 3D.\nHere’s a contour plot of the volcano dataset.\n\n# Contours and other lines will automatically be ignored. Here is the volcano\n#   dataset:\n\nggvolcano &lt;- volcano %&gt;% \n  melt() %&gt;%\n  ggplot() +\n  geom_tile(aes(x = Var1, y = Var2, fill = value)) +\n  geom_contour(aes(x = Var1, y = Var2, z = value), color = \"black\") +\n  scale_x_continuous(\"X\", expand = c(0, 0)) +\n  scale_y_continuous(\"Y\", expand = c(0, 0)) +\n  scale_fill_gradientn(\"Z\", colours = terrain.colors(10)) +\n  coord_fixed()\n\npar(mfrow = c(1, 2))\nplot_gg(ggvolcano, width = 7, height = 4, raytrace = FALSE, preview = TRUE)\n\nWarning: Removed 1861 rows containing missing values or values outside the scale range\n(`geom_contour()`).\n\n\n\n\n\n\n\n\n\n\nplot_gg(\n  ggvolcano,\n  multicore = TRUE, \n  raytrace = TRUE, \n  width = 7, \n  height = 4, \n  scale = 300, \n  windowsize = c(1400, 866), \n  zoom = 0.6, \n  phi = 30, \n  theta = 30\n)\n\nWarning: Removed 1861 rows containing missing values or values outside the scale range\n(`geom_contour()`).\n\nSys.sleep(0.2)\n\nrender_snapshot(clear = TRUE)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#mtcars-data-example",
    "href": "lessons_original/01_rayshader.html#mtcars-data-example",
    "title": "R Rayshader Overview",
    "section": "mtcars Data Example",
    "text": "mtcars Data Example\nRayshader also detects when the user passes the color aesthetic, and maps those values to 3D\n\nmtplot = ggplot(mtcars) + \n  geom_point(\n    aes(x = mpg, y = disp, color = cyl)\n  ) + \n  scale_color_continuous(limits = c(0, 8))\n\npar(mfrow = c(1, 2))\nplot_gg(mtplot, width = 3.5, raytrace = FALSE, preview = TRUE)\n\n\n\n\n\n\n\nplot_gg(mtplot)\nSys.sleep(0.2)\nrender_snapshot(clear = TRUE)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#reference",
    "href": "lessons_original/01_rayshader.html#reference",
    "title": "R Rayshader Overview",
    "section": "Reference",
    "text": "Reference\n\nhttps://www.rayshader.com/\nhttps://www.youtube.com/watch?v=zgFXVhmKNbU"
  },
  {
    "objectID": "lessons_original/01_skimr.html",
    "href": "lessons_original/01_skimr.html",
    "title": "Skimr Package",
    "section": "",
    "text": "Skimr is an R package designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The function is modifiable where you can add additional variables, which are not a part of default summary function within R. Skimr allows us to quickly assess data quality by feature and type in a quick report. This is a critical step in Data Exploration, where Understanding our data helps us to generate a hypothesis and determine what data analysis are appropriate.\nThis presentation will cover the simplest and most effective ways to explore data in R.\n\n\nTo begin we will upload the packages necessary for the lesson, this includes the following:\n\nreadr() to import our data file\nknitr() that houses the kable() feature that allows us to construct and customize tables.\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n\nCode\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n\n\n\nFor this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n\nCode\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\""
  },
  {
    "objectID": "lessons_original/01_skimr.html#packages",
    "href": "lessons_original/01_skimr.html#packages",
    "title": "Skimr Package",
    "section": "",
    "text": "To begin we will upload the packages necessary for the lesson, this includes the following:\n\nreadr() to import our data file\nknitr() that houses the kable() feature that allows us to construct and customize tables.\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n\nCode\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lessons_original/01_skimr.html#census-data",
    "href": "lessons_original/01_skimr.html#census-data",
    "title": "Skimr Package",
    "section": "",
    "text": "For this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n\nCode\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\""
  },
  {
    "objectID": "lessons_original/01_skimr.html#separate-dataframes-by-type",
    "href": "lessons_original/01_skimr.html#separate-dataframes-by-type",
    "title": "Skimr Package",
    "section": "4.1 Separate dataframes by type",
    "text": "4.1 Separate dataframes by type\nThe data frames produced by skim() are wide and sparse, filled with columns that are mostly NA. For that reason, it can be convenient to work with “by type” subsets of the original data frame. These smaller subsets have their NA columns removed.\nFeatures:\n\npartition() - Creates a list of smaller data frames. Each entry in the list is a data type from the original dataframe\nbind() - Takes the list and rebuilds the original dataframe.\nyank() - Extract a subtable from a dataframe with a particular type.\n\nThe following syntax is using partition() to separate the large census_df.\n\n\nCode\n# split the character and numeric data\nseparate_df &lt;- partition(skim(census_2010))\n# check only the character data\nseparate_df$character\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\n\nCode\n\n# create summary statistics for only numeric variables\nnumeric_separate_df &lt;- separate_df[2]\n# pull out the desired summary statistics in the nested list\nhead(numeric_separate_df$numeric[\"mean\"]) %&gt;% \n  kable(digits = 1) \n\n\n\n\n\nmean\n\n\n\n\n49.8\n\n\n2.7\n\n\n5.2\n\n\n30.3\n\n\n101.9\n\n\n193387.1\n\n\n\n\n\nThe following syntax is using bind() to combine the smaller character and numeric lists into the desired df.\n\n\nCode\n# combine the character and numeric data\nhead(bind(separate_df))\n\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\n\n\n\nCode\n\n# confirm that the bound table is the same as the original skimmed table\nidentical(bind(separate_df), skim(census_2010)) \n[1] TRUE\n\n\nThe following syntax is using yank() to extract a specific table eg.character to examine.\n\n\nCode\n# Extract character data\nyank(skim(census_2010), \"character\")\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0"
  },
  {
    "objectID": "lessons_original/01_skimr.html#skimr-with-dplyr",
    "href": "lessons_original/01_skimr.html#skimr-with-dplyr",
    "title": "Skimr Package",
    "section": "4.2 Skimr with Dplyr",
    "text": "4.2 Skimr with Dplyr\nSkimr functions can be used in combination with Dplyr functions to examine specific variables within the census dataset.\nThe following example used skim() with filter() to display the variable CENSUS2010POP. The dataframe was further customized to display variable name and data type using select().\n\n\nCode\n# use dplyr functions on the statistics summary table\ncensus_filter &lt;- skim(census_2010) %&gt;% \n  filter(skim_variable == \"CENSUS2010POP\")\ncensus_filter\n\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCENSUS2010POP\n0\n1\n193387\n1176201\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\n\n\n\nCode\n\ncensus_select &lt;- skim(census_2010) %&gt;% \n  select(skim_type, skim_variable)\nhead(census_select)\n# A tibble: 6 × 2\n  skim_type skim_variable\n  &lt;chr&gt;     &lt;chr&gt;        \n1 character STNAME       \n2 character CTYNAME      \n3 numeric   SUMLEV       \n4 numeric   REGION       \n5 numeric   DIVISION     \n6 numeric   STATE        \n\n\nYou can also customize the output of the skim() function by using various arguments. For example, you can use the numeric argument to specify which variables should be treated as numeric variables, or use the ranges argument to specify custom ranges for variables.\nUsing skim() in combination with mutate() we will compute a new variable to add to our skim dataframe.\n\n\nCode\n# create a new variable calculate the change in birth rate from 2010 to 2011\ncensus_2010 %&gt;% \n  # new variable\n  mutate(net_birth = BIRTHS2011 - BIRTHS2010) %&gt;% \n  # move the variable to the beginning of the dataset\n  relocate(net_birth, .after = CENSUS2010POP) %&gt;% \n  # summary statistics table\n  skim() %&gt;% \n  # only the first fifteen variables\n  head(n = 15) %&gt;% \n  # change the formatting \n  kable(digit = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nSTNAME\n0\n1\n4\n20\n0\n51\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSUMLEV\n0\n1\nNA\nNA\nNA\nNA\nNA\n4.98e+01\n1.25e+00\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nnumeric\nREGION\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.67e+00\n8.10e-01\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nnumeric\nDIVISION\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.19e+00\n1.97e+00\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nnumeric\nSTATE\n0\n1\nNA\nNA\nNA\nNA\nNA\n3.03e+01\n1.52e+01\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nnumeric\nCOUNTY\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.02e+02\n1.08e+02\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nnumeric\nCENSUS2010POP\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.93e+05\n1.18e+06\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nnumeric\nnet_birth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.87e+03\n1.18e+04\n-3\n96\n232\n639\n386443\n▇▁▁▁▁\n\n\nnumeric\nESTIMATESBASE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.93e+05\n1.18e+06\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.94e+05\n1.18e+06\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2011\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.95e+05\n1.19e+06\n90\n11277\n26417\n72387\n37700034\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2012\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.97e+05\n1.20e+06\n81\n11195\n26362\n72496\n38056055\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2013\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.98e+05\n1.21e+06\n89\n11180\n26519\n72222\n38414128\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2014\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.00e+05\n1.22e+06\n87\n11121\n26483\n72257\n38792291\n▇▁▁▁▁"
  },
  {
    "objectID": "lessons_original/01_skimr.html#adding-variables",
    "href": "lessons_original/01_skimr.html#adding-variables",
    "title": "Skimr Package",
    "section": "4.3 Adding Variables",
    "text": "4.3 Adding Variables\n\nbase - An sfl that sets skimmers for all column types.\nappend - Whether the provided options should be in addition to the defaults already in skim. Default is TRUE.\n\nAs mentioned, skim() is designed to display default statistics, however you can use this function to change the summary statistics that it returns.\nskim_with() is type closure: a function that returns adds a new variable to the table. This lets you have several skimming functions in a single R session, but it also means that you need to assign the return of skim_with() before you can use it.\nYou assign values within skim_with() by using the sfl() helper (skimr function list). It identifies which skimming functions you want to remove, by setting them to NULL. Assign an sfl to each column type that you wish to modify.\nFor example, we will add the following variables to the dataframe: median, min, max, IQR, length.\n\n\nCode\nmy_skim &lt;- skim_with(\n  numeric = sfl(median, min, max, IQR),\n  character = sfl(length), \n  append = TRUE\n)\n\n# add new variables into the summary table\ncensus_2010 %&gt;% \n  my_skim() %&gt;% \n  head(n = 10)\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nlength\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n3193\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n3193\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nmedian\nmin\nmax\nIQR\n\n\n\n\nSUMLEV\n0\n1\n4.98e+01\n1.25e+00\n40\n50\n50\n50\n50\n▁▁▁▁▇\n50\n40\n50\n0\n\n\nREGION\n0\n1\n2.67e+00\n8.10e-01\n1\n2\n3\n3\n4\n▁▆▁▇▂\n3\n1\n4\n1\n\n\nDIVISION\n0\n1\n5.19e+00\n1.97e+00\n1\n4\n5\n7\n9\n▂▇▅▆▃\n5\n1\n9\n3\n\n\nSTATE\n0\n1\n3.03e+01\n1.52e+01\n1\n18\n29\n45\n56\n▃▇▆▆▇\n29\n1\n56\n27\n\n\nCOUNTY\n0\n1\n1.02e+02\n1.08e+02\n0\n33\n77\n133\n840\n▇▁▁▁▁\n77\n0\n840\n100\n\n\nCENSUS2010POP\n0\n1\n1.93e+05\n1.18e+06\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n26424\n82\n37253956\n60105\n\n\nESTIMATESBASE2010\n0\n1\n1.93e+05\n1.18e+06\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n26446\n82\n37254503\n60192\n\n\nPOPESTIMATE2010\n0\n1\n1.94e+05\n1.18e+06\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n26467\n83\n37334079\n60446"
  },
  {
    "objectID": "lessons_original/01_table1.html",
    "href": "lessons_original/01_table1.html",
    "title": "Demographics table with table1",
    "section": "",
    "text": "Code\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"lag\", \"dplyr\", quiet = TRUE)\n\nsuppressPackageStartupMessages(library(tidyverse))\n\n# suppress \"`summarise()` has grouped output by \" messages\noptions(dplyr.summarise.inform = FALSE)"
  },
  {
    "objectID": "lessons_original/01_table1.html#necessary-packages",
    "href": "lessons_original/01_table1.html#necessary-packages",
    "title": "Demographics table with table1",
    "section": "Necessary Packages",
    "text": "Necessary Packages\nThe htmlTable package allows for the usage of the table1() function to create a table 1, while also making life easy when attempting to copy this table into a Word document.\nThe boot package was created to aid in performing bootstrapping analysis. With it comes numerous data sets, specifically clinical trial data sets to make this possible. However, there is no code book provided within the package when the data is downloaded as a csv file. This is a link on Github that explains and elaborates on every data within the package itself2.\n\n#install.packages(\"htmlTable\")\n#install.packages(\"boot\")\n\n# Load libraries\nlibrary(htmlTable)\nlibrary(table1)\nlibrary(boot)"
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html",
    "href": "lessons_original/02_fisher_exact_test.html",
    "title": "Fisher’s Exact Test",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated, which are also somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n\n\n\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:\\((H_0)\\) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: \\((H_1)\\) There is a relationship between the categorical variables, the variables are dependent."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#introduction",
    "href": "lessons_original/02_fisher_exact_test.html#introduction",
    "title": "Fisher’s Exact Test",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated, which are also somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n\n\n\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:\\((H_0)\\) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: \\((H_1)\\) There is a relationship between the categorical variables, the variables are dependent."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#fishers-exact-test-equation",
    "href": "lessons_original/02_fisher_exact_test.html#fishers-exact-test-equation",
    "title": "Fisher’s Exact Test",
    "section": "Fisher’s Exact Test Equation",
    "text": "Fisher’s Exact Test Equation\nFisher’s exact test for a one-tailed p-value is calculated using the following formula:\n\\[   p = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!} \\] - n = population size/ total frequency\n- a + b = “successes” values in the contingency table\n- a + c = sample size / draws from the population\n- a = sample successes\n\nFormula description\nthis test is usually used as a one-tailed test but it can also be used as a two tailed test as well, \\(a\\),\\(b\\),\\(c\\), and \\(d\\) are the individual frequencies on the 2x2 contingency table and \\(n\\) is our total frequency. This particular test is used to obtain the probability of the combination of frequencies that we can actually obtain.\n\n\nWhat is a contingency table?\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables. The table() function is used to create a contingency table in R. When the variables of interest are summarized in a contingency table it is easier to run the Fisher’s Exact test.\n\nExample: Creating a contingency table\nLets say we have information on the gender of participants in a clinical trial and the type of drug administered to them we can create the following contingency table for further analysis.\n\n# Example R code to create a contingency table\n\n# Creating a data frame\n df &lt;- data.frame(\n   \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\"),\n   \"Gender\" = c(\"Male\", \"Male\", \"Female\")\n )\n \n# Creating contingency table using table()\nctable &lt;- table(df)\nprint(ctable)\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n\n\n\n\n\nPerforming Fisher’s Exact Test in R\nWe will need to install the ggstatplot package to visualize the statistical results.\n\n# install.packages(\"ggstatplot\") \n# install.packages(\"summarytools\")\n# install.packages(\"gmodels\")\n# install.packages(tidyverse)\n\n\n\nData Source: GMP2017\nFor this example we will be using the Greater Manchester Police’s UK stop and search data from 2017(December) sourced from the Sage Research Methods Dataset Part 2 (https://methods.sagepub.com/dataset/fishers-exact-gmss-2017). This data has information on stop and search events, gender and ethnicity. For this example we would like to access whether there is a significant relationship between gender and stop and search events (having controlled drugs vs harmful weapons)?\n\nGMP17 &lt;- read.csv(\n  \"../data/02_dataset-gmss-2017-subset1_jittered_20240503.csv\"\n)\n\n\n\nLoad in libraries\n\nlibrary(gmodels)\nlibrary(ggstatsplot)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(katex)\nlibrary(tidyverse)\n\n\n\nDescriptive summary\n\n\nCode\nhead(GMP17)\n\n\n  Gender Ethnicity ObjectSearch\n1      1         1            1\n2      1         1           -9\n3      1         1            1\n4      1         1            1\n5      1         1           -9\n6      1         1            1\n\n\nCode\nstr(GMP17)\n\n\n'data.frame':   186 obs. of  3 variables:\n $ Gender      : int  1 1 1 1 1 1 1 1 1 -9 ...\n $ Ethnicity   : int  1 1 1 1 1 1 2 1 1 1 ...\n $ ObjectSearch: int  1 -9 1 1 -9 1 1 1 -9 -9 ...\n\n\nCode\n# determining the number of rows\nNROW(GMP17)\n\n\n[1] 186\n\n\n\n\nAssessing frequencies to answer research question\nFor this analysis we will use the Gender variable and the ObjectSearch variable\n\n# Dropping the Ethnicity variable to remain with variables of interest for for the 2x2 table\n\nnewGMP17 &lt;-GMP17[ -c(2) ]\n \nhead(newGMP17)\n\n  Gender ObjectSearch\n1      1            1\n2      1           -9\n3      1            1\n4      1            1\n5      1           -9\n6      1            1\n\n\nThe data contains missing values categorized as -9 that we need to drop and we need to rename our variables based on the data dictionary provided https://methods.sagepub.com/dataset/download/fishers-exact-gmss-2017/guide/codebook.\n\n# Exclude rows that have missing data in both variables\nnewGMP17_nom &lt;- subset(newGMP17, Gender &gt; 0)\nnewGMP17_nom2 &lt;- subset(newGMP17_nom, ObjectSearch  &gt; 0)\nsummary(newGMP17_nom2)\n\n     Gender       ObjectSearch  \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :1.000  \n Mean   :1.052   Mean   :1.267  \n 3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000  \n\nnrow(newGMP17_nom2)\n\n[1] 116\n\n\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$Gender &lt;- recode_factor(\n  newGMP17_nom2$Gender,\n  \"1\" = \"Male\",\n  \"2\" = \"Female\"\n)\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$ObjectSearch &lt;- recode_factor(\n  newGMP17_nom2$ObjectSearch,\n  \"1\" = \"Controlled_Drugs\",\n  \"2\" = \"Harmful_Objects\"\n)\n\n\n# Creating the contingency table for subset data\ncGMP17 = table(newGMP17_nom2)\nprint(cGMP17)\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male                 83              27\n  Female                2               4\n\n\n\n\nVisualizing data using mosaic plot\n\nwe can use the mosaic plot to represent the data.\n\n\nmosaicplot(\n  cGMP17,\n  main = 'Mosaic Plot',\n  color = TRUE\n)\n\n\n\n\n\n\n\n\n\n\nRunning the Fisher’s exact test using fisher.test()\nWhat if we just run a Chi-square test?\nUsing our GMP17 dataset we can try to run a Chi-square test instead of the Fisher’s Exact test and see what happens.\nThe R output gives us a warning that the Chi Square is not appropriate hence we should use another test in this case the Fisher’s Exact Test.\n\nchisq.test(cGMP17)$expected\n\nWarning in chisq.test(cGMP17): Chi-squared approximation may be incorrect\n\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male          80.603448       29.396552\n  Female         4.396552        1.603448\n\n\n\n\nRunning the test\n\n# running the fisher's exact test\n\ntest &lt;- fisher.test(cGMP17)\ntest\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  cGMP17\np-value = 0.04297\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.8133673 70.2637501\nsample estimates:\nodds ratio \n  6.030297 \n\n\nUsing the gt summary to view results.\n\nnewGMP17_nom2 |&gt; \n  tbl_summary(by = Gender) |&gt; \n  add_p() |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 1161\nMale, N = 1101\nFemale, N = 61\np-value2\n\n\n\n\nObjectSearch\n\n\n\n\n\n\n0.043\n\n\n    Controlled_Drugs\n85 (73%)\n83 (75%)\n2 (33%)\n\n\n\n\n    Harmful_Objects\n31 (27%)\n27 (25%)\n4 (67%)\n\n\n\n\n\n1 n (%)\n\n\n2 Fisher’s exact test\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of results\nThe most important test statistic is the p - value therefore we can retrieve the specific result using the following code;\n\ntest$p.value \n\n[1] 0.04297268\n\n\nOdds ratio = 6.33, 95% CI = 0.85-73.59], we reject the null hypothesis (p &lt; 0.05) and conclude that there is a strong association between the two categorical independent variables (gender and object search events)\nTherefore the odds ratio indicates that the odds of having controlled drugs at a stop and search is 6.33 times as likely for males compared to females. In other words, males are more likely of having controlled drugs at a stop and search than females.\n\n\nVisualizing statistical results with plots using ggstatsplot\n\nwe download the ggsattsplot package to visualize the results in a plot.\n\n\n# Fisher's exact test \n\ntest &lt;- fisher.test(cGMP17)\n\n# combine plot and statistical test with ggbarstats\n\nggbarstats(\n  newGMP17_nom2, Gender, ObjectSearch,\n  results.subtitle = FALSE,\n  subtitle = paste0(\n    \"Fisher's exact test\", \", p-value = \",\n    ifelse(test$p.value &lt; 0.001, \"&lt; 0.001\", round(test$p.value, 3))\n  )\n)\n\n\n\n\n\n\n\n\nFrom the plot, it is clear that the proportion of males among object search events is higher compared to females, suggesting that there is a relationship between the two variables.\nThis is confirmed thanks to the p-value displayed in the subtitle of the plot. As previously, we reject the null hypothesis and we conclude that the variables gender and stop and search events are not independent (p-value = 0.038)."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#what-if-we-have-more-than-two-levels",
    "href": "lessons_original/02_fisher_exact_test.html#what-if-we-have-more-than-two-levels",
    "title": "Fisher’s Exact Test",
    "section": "What if we have more than two levels?",
    "text": "What if we have more than two levels?\nUsing the drug example used previously lets say we have 3 drugs ‘Drug A, Drug B or Drug C’ and we want to see if there is any relationship with gender ‘Male/Female’.\n\n# Creating a data frame\ndf &lt;- data.frame (\n  \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\", \"Drug C\", \"Drug C\"),\n  \"Gender\" = c(\"Male\", \"Male\", \"Female\", \"Female\", \"Female\")\n)\n \n# Creating contingency table using table()\nctable &lt;- table(df)\nprint(ctable)\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n  Drug C      2    0\n\n\n\n# Running the Fisher's Exact test for the 3x2 table\nfisher.test(ctable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value = 0.6\nalternative hypothesis: two.sided\n\n\nThe p-value is non-significant [p = 0.6], we fail to reject the null hypothesis (p &lt; 0.05) and conclude that there is no association between the drug treatments and gender. If the results had been significant we would have gone ahead and conducted a post hoc analysis using pairwise_fisher_test to asses each combination.\nSummary\nThis article describe the assumptions and hypotheses of the Fisher’s Exact test. It also provides examples on how it can be applied."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#references",
    "href": "lessons_original/02_fisher_exact_test.html#references",
    "title": "Fisher’s Exact Test",
    "section": "References",
    "text": "References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "What is bootstrapping?\nBootstrapping is a technique from Efron (1979) that is built on a simple idea: if the data we have is a sample from a population, why don’t we sample from our own data to make more samples? Now, because we don’t have access to any new data, we’re going to take samples of our data set with replacement.\n\n\nThe purpose of bootstrapping is to increase the sample size for our analysis when the sample we have been given is small."
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#when-to-use-bootstrapping",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#when-to-use-bootstrapping",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "The purpose of bootstrapping is to increase the sample size for our analysis when the sample we have been given is small."
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#distribution",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#distribution",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.1 Distribution",
    "text": "3.1 Distribution\nBoxplots and histograms will be useful to understand the distribution of the data.\nOur data is not normal based on the distribution.\n\n\nCode\n# check the boxplot of the data\nboxplot(\n  new_penguins_df$flipper_length_mm ~ new_penguins_df$island, las = 1, \n  ylab = \"Flipper Length (mm)\",\n  xlab = \"Island\",\n  main = \"Flipper Length by Island\"\n)\n\n\n\n\n\n\n\n\n\nCode\n\n# check the histogram of the data\nhist(\n  x = new_penguins_df$flipper_length_mm,\n  main = \"Distribution of Flipper Length (mm)\",\n  xlab = \"Flipper Length\"\n)"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#bootstrapping-test",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#bootstrapping-test",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.2 Bootstrapping Test",
    "text": "3.2 Bootstrapping Test\nWe need the difference in means in order to conduct our permutation test. We will test whether the difference is significant so that we can reject the null. This indicates that there is a different in flipper length among the same species that come from different islands.\n\n\nCode\n# set a seed so that our random results can be replicated by other people:\nset.seed(20150516)\n\n# take a random re-sample of the data that is the *same size*\nN &lt;- length(new_penguins_df$flipper_length_mm)\n\n# a random sample:\nsample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n [1] 184 192 198 195 195 176 188 183 184 193 199 198 184 190 198 195 195 199 193\n[20] 197 198 189 197 188 189 199 200 190 183 198 194 190 191 196 189 195 198 197\n[39] 191 184 198 180 195 186 193 193 191 195 190 198 189 181 197 196 182 200 188\n[58] 184 202 189 197 186 181 195 181 191 185 193 196 185 192 199 186 196 180 190\n[77] 190 195 197 193 191 181 195 190 186 189 192 187 190 195 195 182 172 194 181\n\n# number of bootstrap samples\nB_int &lt;- 10000\n\n# create a list of these thousands of samples \nbootstrapSamples_ls &lt;- map(\n  .x = 1:B_int,\n  .f = ~{\n    sample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n  }\n)\n\n# subset of the random samples \nbootstrapSamples_ls[1:3]\n[[1]]\n [1] 183 190 189 188 181 198 181 172 187 189 189 193 180 197 191 190 196 191 195\n[20] 181 193 190 190 186 188 195 190 197 198 190 180 198 194 188 195 191 203 199\n[39] 190 189 195 186 189 199 202 197 189 190 194 190 181 190 190 181 186 196 174\n[58] 185 174 202 191 184 181 184 193 190 190 190 191 196 189 195 195 198 193 190\n[77] 197 184 186 188 193 190 191 195 198 180 191 185 189 192 183 192 199 186 195\n\n[[2]]\n [1] 187 194 187 189 184 188 187 187 184 197 193 191 187 189 190 172 187 186 180\n[20] 193 191 195 195 180 184 189 197 191 187 186 186 187 184 188 190 193 198 190\n[39] 195 198 184 197 195 195 195 198 194 191 198 197 198 186 194 195 189 186 181\n[58] 180 191 180 191 193 196 191 202 191 187 181 199 172 181 191 195 195 194 198\n[77] 191 191 190 192 190 199 195 193 195 197 188 181 190 185 186 191 174 193 195\n\n[[3]]\n [1] 191 196 203 195 185 195 193 186 186 202 186 203 187 180 185 186 192 202 186\n[20] 192 200 195 184 185 195 193 199 190 189 185 181 181 188 197 181 190 188 185\n[39] 187 184 184 195 199 186 200 186 192 195 190 182 189 191 203 193 195 191 191\n[58] 199 195 198 187 191 195 190 190 187 189 192 186 199 193 190 187 181 190 191\n[77] 190 190 183 193 190 197 181 190 187 198 187 190 200 184 190 184 186 191 193"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-1",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-1",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.3 Building Confidence Intervals for Various Statistics: Example 1",
    "text": "3.3 Building Confidence Intervals for Various Statistics: Example 1\n\n\nCode\n\n# The Sample Mean\nbootMeans_num &lt;-\n  bootstrapSamples_ls %&gt;%\n  # the map_dbl() function takes in a list and returns an atomic vector of type\n  #   double (numeric)\n  map_dbl(mean)\n\n# a normally distributed histogram using the samples from bootstrapping\nhist(bootMeans_num)\n\n\n\n\n\n\n\n\n\nCode\n\n# 95% confidence interval?\nquantile(bootMeans_num, probs = c(0.025, 0.975))\n 2.5% 97.5% \n  189   191"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-2",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-2",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.4 Building Confidence Intervals for Various Statistics: Example 2",
    "text": "3.4 Building Confidence Intervals for Various Statistics: Example 2\nSource: https://www.geeksforgeeks.org/bootstrap-confidence-interval-with-r-programming/\n\n\nCode\n\n# Custom function to find correlation between the bill length and depth \ncorr.fun &lt;- function(data, idx) {\n  \n# vector of indices that the boot function uses\n  df &lt;- data[idx, ]\n\n# Find the spearman correlation between\n# the 3rd (length) and 5th (depth) columns of dataset\n  cor(df[, 3], df[, 4], method = 'spearman')\n}\n\n# Setting the seed for reproducability of results\nset.seed(42)\n\n# Calling the boot function with the dataset\nbootstrap &lt;- boot(iris, corr.fun, R = 1000)\n\n# Display the result of boot function\nbootstrap\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = iris, statistic = corr.fun, R = 1000)\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1*    0.938 -0.00272     0.00944\n\n# Plot the bootstrap sampling distribution using ggplot\nplot(bootstrap)\n\n\n\n\n\n\n\n\n\nCode\n\n# Function to find the bootstrap CI\nboot.ci(\n  boot.out = bootstrap,\n    type = \"perc\"\n)\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.914,  0.952 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html",
    "href": "lessons_original/03_two_sample_ttest.html",
    "title": "Two sample t test",
    "section": "",
    "text": "This is also called the independent sample t test. It is used to see whether the unknown population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If you have more than two groups then analysis of variance (ANOVA) will be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test or a permutation test.(cressie1986?).\nThere are two types of t tests, the first being the Student’s t test, which assumes the variance of the two groups is equal, the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.\n\n\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n\n\n\n\n(H_0): the mean of group A (m_A) is equal to the mean of group B (m_B)- two tailed test,\n(H_0): (m_A)\\ge (m_B)- one tailed test.\n(H_0): (m_A)\\le (m_B)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n(H_1): (m_A)\\neq(m_B)- two tailed test.\n(H_1): (m_A)&lt;(m_B)- one tailed test.\n(H_1): (m_A)&gt; (m_B)- one tailed test.\n\n\n\n\nFor the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\nt ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\nm_A and m_B are the mean values of A and B,\nn_A and n_B are the seize of group A and B,\nS^2 is the estimator for the pooled variance,\nwith the degrees of freedom (df) = n_A + n_B - 2,\nand S^2 is calculated as follows:\nS^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.(fralick?)\n\n\n\n\n\n\n\ntidyverse: data manipulation and visualization.\nrstatix: providing pipe friendly R functions for easy statistical analyses.\ncar: providing variance tests.\n\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n\n\n\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.(cortez2008?)\n\n\nCode\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n\n\nCode\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n\nCode\n# check the data\nglimpse(stu_math)\n\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\nH_0: There is no statistical difference between the final grades between boys and girls.\nH_1: There is a statistically significant difference in the final grades between the two groups.\n\n\nCode\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n\nCode\n# summarizing our data\n summary(math)\n\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n\nCode\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n\nCode\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n\nCode\n# plotting distribution for females\nplotNormalHistogram(\n  female, breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n\nCode\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n\nCode\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n\nCode\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n\n[1] 9.966346\n\n\nCode\nmean(math$G3[math$sex==\"M\"])\n\n\n[1] 10.91444\n\n\nCode\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n\n[1] 11.20541\n\n\nCode\nmean(math2$G3[math2$sex==\"M\"])\n\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n\nCode\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n\nCode\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n\n\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s t test.\n\n\n\nSince the default is the Welch t test we use the \\color{blue}{\\text{var.eqaul = TRUE }} code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n\nCode\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\n\nCode\nstat.test$statistic\n\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n\nCode\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOverall, N = 3571\nF, N = 1851\nM, N = 1721\nDifference2\n95% CI2,3\np-value2\n\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n1 Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p = 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96 or greater).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\nd= {m_A-m_B \\over SD_pooled}\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n\nCode\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\color{blue}{\\text{alternative =}} option to determine if one group is \\color{blue}{\\text{\"less\"}} or \\color{blue}{\\text{\"greater\"}}. For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n\nCode\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\color{blue}{\\text{paired= TRUE}} to our t_test() to run the analysis for matched pairs data.\n\n\n\n\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#assumptions",
    "href": "lessons_original/03_two_sample_ttest.html#assumptions",
    "title": "Two sample t test",
    "section": "",
    "text": "Measurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#hypotheses",
    "href": "lessons_original/03_two_sample_ttest.html#hypotheses",
    "title": "Two sample t test",
    "section": "",
    "text": "(H_0): the mean of group A (m_A) is equal to the mean of group B (m_B)- two tailed test,\n(H_0): (m_A)\\ge (m_B)- one tailed test.\n(H_0): (m_A)\\le (m_B)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n(H_1): (m_A)\\neq(m_B)- two tailed test.\n(H_1): (m_A)&lt;(m_B)- one tailed test.\n(H_1): (m_A)&gt; (m_B)- one tailed test."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#statistical-hypotheses-formula",
    "href": "lessons_original/03_two_sample_ttest.html#statistical-hypotheses-formula",
    "title": "Two sample t test",
    "section": "",
    "text": "For the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\nt ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\nm_A and m_B are the mean values of A and B,\nn_A and n_B are the seize of group A and B,\nS^2 is the estimator for the pooled variance,\nwith the degrees of freedom (df) = n_A + n_B - 2,\nand S^2 is calculated as follows:\nS^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.(fralick?)"
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#example",
    "href": "lessons_original/03_two_sample_ttest.html#example",
    "title": "Two sample t test",
    "section": "",
    "text": "tidyverse: data manipulation and visualization.\nrstatix: providing pipe friendly R functions for easy statistical analyses.\ncar: providing variance tests.\n\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n\n\n\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.(cortez2008?)\n\n\nCode\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n\n\nCode\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n\nCode\n# check the data\nglimpse(stu_math)\n\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\nH_0: There is no statistical difference between the final grades between boys and girls.\nH_1: There is a statistically significant difference in the final grades between the two groups.\n\n\nCode\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n\nCode\n# summarizing our data\n summary(math)\n\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n\nCode\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n\nCode\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n\nCode\n# plotting distribution for females\nplotNormalHistogram(\n  female, breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n\nCode\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n\nCode\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n\nCode\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n\n[1] 9.966346\n\n\nCode\nmean(math$G3[math$sex==\"M\"])\n\n\n[1] 10.91444\n\n\nCode\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n\n[1] 11.20541\n\n\nCode\nmean(math2$G3[math2$sex==\"M\"])\n\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n\nCode\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n\nCode\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n\n\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s t test.\n\n\n\nSince the default is the Welch t test we use the \\color{blue}{\\text{var.eqaul = TRUE }} code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n\nCode\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\n\nCode\nstat.test$statistic\n\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n\nCode\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOverall, N = 3571\nF, N = 1851\nM, N = 1721\nDifference2\n95% CI2,3\np-value2\n\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n1 Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p = 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96 or greater).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\nd= {m_A-m_B \\over SD_pooled}\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n\nCode\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\color{blue}{\\text{alternative =}} option to determine if one group is \\color{blue}{\\text{\"less\"}} or \\color{blue}{\\text{\"greater\"}}. For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n\nCode\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\color{blue}{\\text{paired= TRUE}} to our t_test() to run the analysis for matched pairs data."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#conclusion",
    "href": "lessons_original/03_two_sample_ttest.html#conclusion",
    "title": "Two sample t test",
    "section": "",
    "text": "This article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results."
  },
  {
    "objectID": "lessons/00_lesson_template.html",
    "href": "lessons/00_lesson_template.html",
    "title": "The Method",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\n1 Introduction to &lt;the method&gt;\n\n\n2 Mathematical definition of &lt;the method&gt;\n\n\n3 Data source and description\n\n\n4 Cleaning the data to create a model data frame\n\n\n5 Assumptions of &lt;the method&gt;\n\n\n6 Checking the assumptions with plots\n\n\n7 Code to run &lt;the method&gt;\n\n\n8 Code output\nNOTE: this section will be created automatically by the Quarto document. You should not create a section specifically for this. When you run the code in the previous section, you will get the output automatically.\n\n\n9 Brief interpretation of the output"
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html",
    "href": "lessons/02_z-test_one_prop.html",
    "title": "Z-Test for One Proportion",
    "section": "",
    "text": "The one-sample \\(Z\\)-test is used to compare a sample proportion to a population proportion."
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#independence-and-randomness",
    "href": "lessons/02_z-test_one_prop.html#independence-and-randomness",
    "title": "Z-Test for One Proportion",
    "section": "6.1 Independence and Randomness",
    "text": "6.1 Independence and Randomness\nBecause the samples were collected at random via an FDA approved clinical trial protocol, we assume that all the participants were randomly selected and are independent of each other."
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#extreme-proportions",
    "href": "lessons/02_z-test_one_prop.html#extreme-proportions",
    "title": "Z-Test for One Proportion",
    "section": "6.2 “Extreme” Proportions",
    "text": "6.2 “Extreme” Proportions\nAccording to Ling et al. (2020), the 12-month abstinence proportion of all 533 participants in their study was 40.5 percent. As we can see here, our abstinence rates are 39.4. Neither these proportions are smaller than 5% or greater than 95%.\n\n(pExpected &lt;- 0.508 * (425/533))\n\n[1] 0.4050657\n\n# Count the number of TRUE values\n(nAbstinent &lt;- sum(outcomesCTN0094$kosten1993_isAbs))\n\n[1] 1402"
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#type-and-counts-of-data",
    "href": "lessons/02_z-test_one_prop.html#type-and-counts-of-data",
    "title": "Z-Test for One Proportion",
    "section": "6.3 Type and Counts of Data",
    "text": "6.3 Type and Counts of Data\nWe observe binary data, and we see at least 10 successes and at least 10 failures."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html",
    "href": "lessons_original/04_anova_random_intercept.html",
    "title": "Random Intercept Model",
    "section": "",
    "text": "Code\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#libraries-used",
    "href": "lessons_original/04_anova_random_intercept.html#libraries-used",
    "title": "Random Intercept Model",
    "section": "",
    "text": "Code\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#what-is-a-random-intercept-model",
    "href": "lessons_original/04_anova_random_intercept.html#what-is-a-random-intercept-model",
    "title": "Random Intercept Model",
    "section": "2 What is a Random Intercept model",
    "text": "2 What is a Random Intercept model\nBefore talking about a random intercept model, let’s understand why they are necessary and important in the real world by discussing a variance component model first. This will make sense as we go along in this lecture.\n\n2.1 Variance component model\nWe are familiar with a fixed level of a factor or variable. Which means that the factor level in an experiment is the only thing we are interested. For example, let’s say we are interested in measuring the difference in resistance resulting from putting identical resistors to three different temperatures for a period of 24 hours. Let’s say we have three different groups, and each of these three different groups have a sample size of 5. So each of the three treatment groups was replicated 5 times.\n\n\n\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\n6.9\n8.3\n8.0\n\n\n5.4\n6.8\n10.5\n\n\n5.8\n7.8\n8.1\n\n\n4.6\n9.2\n6.9\n\n\n4.0\n6.5\n9.3\n\n\nmean\nmean\nmean\n\n\n5.34\n7.72\n8.56\n\n\n\nIn this example, the level of the temperature is considered fixed meaning, the three temperatures were the only ones that we are interested in. This is called a fixed effects model.\na fixed effect model is a statistical model in which the parameters are fixed or non-random. This can also be referred to a regression model, in which group mean are “fixed” (non-random) or in simpler, terms something that is “fixed” in analysis is constant like sex assigned at birth or ethnicity.\n y_i = \\beta_0 + X_i\\beta_i + \\alpha_u + \\epsilon_i \nNow, let’s say we want to look at different levels of factors that were chosen because of random sampling, like number of operators working that day, lot batches, days etc. So in this case we are now regarding factors not related to themselves (variables) but we are now trying to represent all possible levels that these factors may take, the appropriate model is now a random effects model.\nfitting these random effects models are important because we want to obtain estimates of different contributions that experimental factors make to the variability of our data! (we can represent this as the variance) this is what is called variance component\n\n\n2.2 Why this is relevant\nWell, a variance component model helps us see how much variance in our response at the different levels. But what if you are interested in seeing the effects of the explanatory variables? Or, what if your observations are NOT randomly sampled from simple random sample but instead from a cluster or a multi-level sampling design? Random intercept models or random effects models are important.\n\n\n2.3 Example 1: School level data\nLet’s say we have some data on exam results of students within a school and we use a variance component model and see that 15% of the variance is at the school level. Like for example, differing school districts, differing school policies etc. However, is it fair to really say that 15% of the variance in example scores is caused by schools? you could also say that maybe that part of the variance could be cause by the students being different themselves as well before taking the exam.\nIn this case, it might be important to control for the previous exams the students took, so you can look at the variance that is due to the things that happened when the students were at that school."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#fitting-a-single-level-regression-model",
    "href": "lessons_original/04_anova_random_intercept.html#fitting-a-single-level-regression-model",
    "title": "Random Intercept Model",
    "section": "3 Fitting a single-level regression model",
    "text": "3 Fitting a single-level regression model\nwhen we want to control for something (like previous exams students took) we can fit a single-level regression model that looks something like this:\ny_1 = \\beta_0 + \\beta_1x_i + e_i where\n\ny_1 is your dependent variable\n\\beta_0 is your intercept and\n\\beta_1 is your slope parameter (which is also your slope treatment effect).\nand e_i is your random error\n\nWhen you have clustered data fitting this model causes problems. Clustered data is data where you observation or participants are related. Like exam results for students within a school, height of children within a family etc.\nif we try to fit this clustered data:\n\nour standard errors will be wrong.\nthis single level data model doesn’t show up how much variation is at the school level and how much much of the variation is at the student level.\n\nSo fitting this type of data in this regression we wont know how much of an effect the school level has on the exam score, after controlling for the previous score."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#solution-fitting-a-random-intercept-model",
    "href": "lessons_original/04_anova_random_intercept.html#solution-fitting-a-random-intercept-model",
    "title": "Random Intercept Model",
    "section": "4 Solution: Fitting a Random Intercept model",
    "text": "4 Solution: Fitting a Random Intercept model\nSo what we can do is combine the variance component and single-level regression model to build a random intercept model. So this random intercept model has 2 random terms. the level one random term: e_{ij} \\sim N(0, \\sigma_e^2) and the N(0, \\sigma_u^2) and has two parts:\n\na fixed part\na random part\n\n y_{ij} = \\overbrace{\\beta_0 + \\beta_1X_{ij}}^{\\text{fixed part}} + \\underbrace{u_j + e_{ij}}_{\\text{random part}} where the fixed parts includes our parameters that we estimate as our coefficients, and the random part is the parameter we estimate as the variance e_{ij} \\sim N(0, \\sigma_e^2) and the N(0, \\sigma_u^2) and these are allowed to vary and u_j and e_{ij} are normally distributed.\nwhere:\n\ny_{ij} is your dependent variable at i individual and j level\nN(0, \\sigma_u^2) is the measurement at the school level\nand e_{ij} \\sim N(0, \\sigma_e^2) is the measurement at the student level\nand i subscript is for the students\nand j is the school subscript\n\nwe can also write this equation like so:\n Y_{ij} = \\mu + b_i + \\varepsilon_{ij} \nwhere\n\nY_{ij} is your dependent outcome of intested for a subject i at school j\n\\mu is the population average mean\nb_i is the random students effects (you have a random effect for every student)\n\\varepsilon_{ij} is your random error."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#final-key-points",
    "href": "lessons_original/04_anova_random_intercept.html#final-key-points",
    "title": "Random Intercept Model",
    "section": "5 Final key points",
    "text": "5 Final key points\n\nrandom intercept models are used for answering questions about clustered data, and at different levels. For example, what is the relationship between exam scores at 11 and at age 16? how much variation is there between students progress from 11 to 16 at the school level?\nb_i is the error associated with the students.\n\\varepsilon_{ij} is the random error.\nfor a random intercept model, each individual will have a random intercept, but the sample slope."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#assumptions-of-a-random-effects-model",
    "href": "lessons_original/04_anova_random_intercept.html#assumptions-of-a-random-effects-model",
    "title": "Random Intercept Model",
    "section": "6 Assumptions of a random effects model:",
    "text": "6 Assumptions of a random effects model:\n\nunobserved cluster effects is not correlated with observed variables (all u_{ij} terms are not correlated with the your predictors.)\nthe within and between effects are the same.\nyour error term is independent with your constant term.\nyou have homoscedasticity\nb_i and \\varepsilon are independent of each other"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#hypothesis-of-a-random-effects-model",
    "href": "lessons_original/04_anova_random_intercept.html#hypothesis-of-a-random-effects-model",
    "title": "Random Intercept Model",
    "section": "7 hypothesis of a random effects model:",
    "text": "7 hypothesis of a random effects model:\nhypothesis testing for a random effects model runs as follows:\n H_0: \\sigma^2_u = 0 H_1: \\sigma^2_u \\not = 0 the null hypothesis states that if \\sigma^2_u is true, then the random component is not needed in this model. so you can fit a single level regression model. to do this, you would can do a likelihood ratio test comparing the two model to see if sigma is significant. In other words, seeing if there is no difference in intercepts. If there is NO difference in intercepts (or the slopes are similar), then a random intercept model or random component is not needed."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "href": "lessons_original/04_anova_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "title": "Random Intercept Model",
    "section": "8 Example 2: Planktonic larval duration (PLD)",
    "text": "8 Example 2: Planktonic larval duration (PLD)\nthis is example is from O’Connor et al (2007). A brief intro on this study, temperature is important for the development of organisms. In marine species, temperature is very important and linked to growth. So the time spent as a planktonic larvae can have associations on mortality and regulation on the species. Previous research has looked at the association between species comparison but not within species comparisons. What if we are interest in within and between species variation?\n\n8.1 load PLD data\n\n\nCode\nPLD &lt;- read_table(\"../data/04_PLD.txt\")\n\n\nI am curious about the structure of this data and how it briefly looks.\n\n\nCode\n#strcuture \nstr(PLD)\n\n\nspc_tbl_ [214 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ phylum : chr [1:214] \"Annelida\" \"Annelida\" \"Annelida\" \"Annelida\" ...\n $ species: chr [1:214] \"Circeus.spirillum\" \"Circeus.spirillum\" \"Circeus.spirillum\" \"Hydroides.elegans\" ...\n $ D.mode : chr [1:214] \"L\" \"L\" \"L\" \"P\" ...\n $ temp   : num [1:214] 5 10 15 15 20 25 30 5 10 17 ...\n $ pld    : num [1:214] 16 16 4 8 6.5 5 3.2 15 9.5 4.5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   phylum = col_character(),\n  ..   species = col_character(),\n  ..   D.mode = col_character(),\n  ..   temp = col_double(),\n  ..   pld = col_double()\n  .. )\n\n\nCode\n# just the top - seeing how it looks\nhead(PLD)\n\n\n# A tibble: 6 × 5\n  phylum   species           D.mode  temp   pld\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Annelida Circeus.spirillum L          5  16  \n2 Annelida Circeus.spirillum L         10  16  \n3 Annelida Circeus.spirillum L         15   4  \n4 Annelida Hydroides.elegans P         15   8  \n5 Annelida Hydroides.elegans P         20   6.5\n6 Annelida Hydroides.elegans P         25   5  \n\n\nCode\n# brief summary\nsummary(PLD)\n\n\n    phylum            species             D.mode               temp      \n Length:214         Length:214         Length:214         Min.   : 2.50  \n Class :character   Class :character   Class :character   1st Qu.:12.28  \n Mode  :character   Mode  :character   Mode  :character   Median :20.00  \n                                                          Mean   :18.59  \n                                                          3rd Qu.:25.00  \n                                                          Max.   :32.00  \n      pld        \n Min.   :  1.00  \n 1st Qu.: 11.00  \n Median : 19.30  \n Mean   : 26.28  \n 3rd Qu.: 33.45  \n Max.   :129.00  \n\n\nI am curious about how this would look just plotting the variable pld or planktonic larvae duration and the temperature. So i am interested in seeing how the temperature is associated with their their survival duration.\n\n\nCode\n# how to plot in base R \nplot(pld ~ temp, data = PLD,\n     xlab = \"temperature (C)\",\n     ylab = \"PLD survival (Days)\",\n     pch = 16)\n# add lm line in base R\nabline(lm(pld ~ temp, data = PLD))\n\n\n\n\n\n\n\n\n\nyou can also do this in ggplot plot like so:\n\n\nCode\nggplot(data = PLD) +\n  aes(y = pld, x = temp) +\n  stat_smooth(method = \"lm\") +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n8.2 Fit first model: Linear model\nLet’s first fit a linear model and check any assumptions. Why are we fitting a linear model first? It might be important to check the standard error of this model and compare to the next model, that might be a better fit later on.\n\n\nCode\n# fitting linear model first \n\nLinearModel_1 &lt;- lm(pld ~ temp, data = PLD)\n\nsummary(LinearModel_1)\n\n\n\nCall:\nlm(formula = pld ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.158 -11.351  -0.430   7.684  93.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.8390     3.7954  14.449  &lt; 2e-16 ***\ntemp         -1.5361     0.1899  -8.087 4.65e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.36 on 212 degrees of freedom\nMultiple R-squared:  0.2358,    Adjusted R-squared:  0.2322 \nF-statistic:  65.4 on 1 and 212 DF,  p-value: 4.652e-14\n\n\nWe can fit this output in a gtsummary to make it nicer looking:\n\n\nCode\nLinearModel_1 |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ntemp\n-1.5\n-1.9, -1.2\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nI am interested in checking out visually, the equal variance (homoscedasticity) and so i will plot a a base residual graph:\n\n\nCode\n# better to do a scatter plot \nLinearModel_res &lt;- resid(LinearModel_1)\n\n# plot residual \nplot(PLD$temp, LinearModel_res,\n     ylab = \"residuals\",\n     xlab = \"temp\",\n     main = \"Residual graph\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\n\nJust upon visual observation, it seems like this assumption may be violated so i think it might be important to do some transformations.\n\n\n8.3 Log transformation\n\n\nCode\nLinearMode_2Log &lt;- lm(log(pld) ~ log(temp), data= PLD)\n\nsummary(LinearMode_2Log)\n\n\n\nCall:\nlm(formula = log(pld) ~ log(temp), data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0768 -0.3956  0.1802  0.5461  1.9656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.6946     0.3128  15.011  &lt; 2e-16 ***\nlog(temp)    -0.6308     0.1093  -5.771 2.77e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8236 on 212 degrees of freedom\nMultiple R-squared:  0.1358,    Adjusted R-squared:  0.1317 \nF-statistic: 33.31 on 1 and 212 DF,  p-value: 2.767e-08\n\n\n\n\n8.4 residual of new log transformed graph\n\n\nCode\n# better to do a scatter plot \nLinearModel2_res &lt;- resid(LinearMode_2Log)\n\n# plot residual \nplot(PLD$temp, LinearModel2_res,\n     ylab = \"residuals(log)\",\n     xlab = \"temp(log)\",\n     main = \"Residual graph (log transformation)\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\n\nA bit better! Now i kinda want to see the original plot i plotted with PLD and temperature:\n\n\nCode\nplot(log(pld) ~ log(temp), data = PLD,\n     xlab = \"Temperature in C\",\n     ylab = \"PLD in days\")\nabline(LinearMode_2Log)\n\n\n\n\n\n\n\n\n\nin ggplot you can use the facet_wrap() function to separate by phylum:\n\n\nCode\nggplot(data = PLD) +\n  aes(x = log(temp), y = log(pld)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  facet_wrap(~phylum) +\n  theme_classic()"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "href": "lessons_original/04_anova_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "title": "Random Intercept Model",
    "section": "9 Fitting a random intercept model (random intercept, same slope)",
    "text": "9 Fitting a random intercept model (random intercept, same slope)\nI am interested in seeing if the overall temperature and the PLD relationship is similar among species, but not the same. We are interested in plotting a mixed effects model with a random intercept but fixed/same slope. I am only interested in the species-specific plot for now with the phylum Mollusca.\n\n\nCode\n# filter to only mollusca\n\nMollusca_subset &lt;- \n  PLD |&gt; \n  filter(phylum == \"Mollusca\")\n\nggplot(data = Mollusca_subset) +\n  aes(x = log(pld), y = log(temp)) +\n  geom_point() +\n  labs(x = \"Log(temperature)\",\n       y = \"Log(PLD)\") + \n  stat_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, fullrange = TRUE) +\n  theme_classic() +\n  facet_wrap(~ species)\n\n\n\n\n\n\n\n\n\n\n9.1 fitting model\nWe can use the library lme4 to fit a model of a linear regression with a random effect\n\n\nCode\n# creating log -transformed variables \nMollusca_subset$log_pld &lt;- log(Mollusca_subset$pld)\nMollusca_subset$log_temp &lt;- log(Mollusca_subset$temp)\n\n# mixed model with random intercept only \nRandIntModel_Mollusca &lt;- lmer(log_pld ~ log_temp + (1 | species), data = Mollusca_subset)\n\nsummary(RandIntModel_Mollusca)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\n\nthe fixed effects: section in the output is the estimate for the fixed slope, and the grand mean of the intercept. in the fixed effects section the intercept here is the random effect, and you can see this\nin the Random effects section in this output, in this case our random effect was specified in the the Names sections, which tells us the parameter is the intercept.\nthe Group section tells us we have a random intercept for each species. We also have the variance and standard deviation for the random effects (as well as the residuals)\n\nSince we have a random effect at the individual level, we can subset this section out so it is clear to see that these organism will have a random intercept and fixed slope.\n\n\nCode\n# subset of the coefficients for random intercept and fixed slop\n\ncoef(RandIntModel_Mollusca)$species\n\n\n                      (Intercept) log_temp\nChlamys.hastata          7.612748 -1.51751\nCrassostrea.virginica    7.592515 -1.51751\nCrepidula.fornicata.     8.045609 -1.51751\nCrepidula.plana          8.063220 -1.51751\nHaliotis.asinina         5.807247 -1.51751\nHaliotis.fulgens         6.083069 -1.51751\nHaliotis.sorenseni.      6.688210 -1.51751\nMactra.solidissima       7.589660 -1.51751\nMopalia.muscosa          7.061086 -1.51751\nMytilus.edulis           7.141801 -1.51751\nNassarius.obsoletus      7.370159 -1.51751\nOstrea.lurida            6.967626 -1.51751\nPerna.viridis            8.144314 -1.51751\nStrombus.gigas           8.048942 -1.51751\nTivela.mactroides        7.677603 -1.51751\nTonicella.lineata        4.871674 -1.51751\n\n\nSo now we can see that in the Mollusca subset, we have all random intercepts for individual specifies, but the same slope.\n\n\n9.2 Inter class correlation coefficient (ICC)\nfor a random intercept model, we can run a diagnostic called the inter-class correlation coefficient (ICC), which lets us know how much group specific information is available for the random effect. this is somewhat similar to the ANOVA, in which it looks at the variability within groups compared to the variability between groups. Low ICC means that observation within group don’t really cluster.\n ICC = {\\sigma^2_{\\alpha} \\over \\sigma^2 + \\sigma^2_\\alpha} \n\n\nCode\n# creating data frame\nvar &lt;- as.data.frame(VarCorr(RandIntModel_Mollusca))\n\n#check our data frame\nvar\n\n\n       grp        var1 var2       vcov     sdcor\n1  species (Intercept) &lt;NA&gt; 0.86457141 0.9298233\n2 Residual        &lt;NA&gt; &lt;NA&gt; 0.03309183 0.1819116\n\n\nCode\n#ICC equation\nICC &lt;- var$vcov[1] / (var$vcov[1] + var$vcov[2])\n\n# ICC value \nICC\n\n\n[1] 0.9631356\n\n\nIn our model, the \\sigma^2_{\\alpha} is 0.8645 (also the vcov part) and the \\sigma^2 is 0.033. so once we do the mathematics we get 0.9631. Which is the proportion of the total variance in Y that is accounted for by clustering. This is a high value and therefore, suggesting we have within-group variability, so it might be good we are running this random effects model."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#interpretation-of-results",
    "href": "lessons_original/04_anova_random_intercept.html#interpretation-of-results",
    "title": "Random Intercept Model",
    "section": "10 Interpretation of results",
    "text": "10 Interpretation of results\n\n\nCode\nsummary(RandIntModel_Mollusca)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\nInterpretation: Summary of this PLD data includes information about the random effects. Here we can see that the column groups shows the random effect variable. in the name section, you can see that the random effect is our intercept. so we have the variation due to the species. in the Residuals section, this is the variation that cannot be explained by the model (the error). As you will notice our Standard error is smaller compared to the ordinary regression we ran in the previous one. Standard error for this model is 0.15 and the previous standard error for the first model we ran was 0.18.\nSo 0.86 / 0.86 + 0.03 = 0.96 , so the difference between between species can explain 96% of the variance that is is left over after the variance is explained by our fixed effect. since the random effects of the species explain most.\nthere is a very long description on the why the lmer() function doesn’t include the p-value that can be found here.\ninterpretation of temp variable for the fixed part, we can interpret this parameter the same as a single-level regression model, so \\beta_1 is the increase/decrease in response for 1 unit increase/decrease in x. In other words, for one unit increase in the degrees of temperature, there is a -1.5 decrease in Planktonic larval duration. (or, as the temperature increase, the plankton duration is lower.)"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#conclusion",
    "href": "lessons_original/04_anova_random_intercept.html#conclusion",
    "title": "Random Intercept Model",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nIn this lecture you learned about the importance of a random intercept model, when it is appropriate to use a random intercept model, the difference between an ordinary single-level model, and a random intercept model, the assumptions of the random intercept model, hypothesis testing for the variation, the Interclass correlation coefficient (ICC) and finally, how to interpret results from the fixed part and the random part of a random intercept model."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#references",
    "href": "lessons_original/04_anova_random_intercept.html#references",
    "title": "Random Intercept Model",
    "section": "12 References",
    "text": "12 References\n\nAbedin, Jaynal, and Kishor Kumar Das. 2015. Data Manipulation with r. Packt Publishing Ltd.\nAnnesley, Thomas M. 2010. “Bars and Pies Make Better Desserts Than Figures.” Clinical Chemistry 56 (9): 1394–1400.\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\nBorer, Elizabeth T, Eric W Seabloom, Matthew B Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14.\nBorghi, John, Stephen Abrams, Daniella Lowenberg, Stephanie Simms, and John Chodacki. 2018. “Support Your Data: A Research Data Management Guide for Researchers.” Research Ideas and Outcomes 4: e26439.\nBroman, Karl W, and Kara H Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10.\nChamberlin, Thomas C. 1890. “The Method of Multiple Working Hypotheses.” Science 15 (366): 92–96.\nsupplementary material for Tom Snijders and Roel Bosker textbook - Shjders, T Bosker R, 1999. Multilevel analysis: an introduction to basic and advanced mltilevel modeling, London, Sage, including updates and corrections data set examples http://stat.gamma.rug.nl/multilevel.htm\nUniversity of Bristol, Random Intercept model, (2018). http://www.bristol.ac.uk/cmm/learning/videos/random-intercepts.html\nMidway, S. (2019). “Data Analysis in R.” https://bookdown.org/steve_midway/DAR/random-effects.html"
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html",
    "title": "Chi-Squared Independence",
    "section": "",
    "text": "The Chi-square test of independence (also known as the Pearson Chi-square test, or simply the Chi-square) is one of the most useful statistics for testing hypotheses when the variables are nominal. It is a non-parametric tool that does not require equality of variances among the study groups or homoscedasticity in the data.(mchugh2013?)\nBeing a non-parametric test tool, Chi-square test can be used when any one of the following conditions pertains to the data:(mchugh2013?)\n\nVariables are nominal or ordinal.\nThe frequency count is &gt;5 for more than 80% of the cells in a contingency table.\nThe sample sizes of the study groups may be of equal size or unequal size but samples do not have equal variance.\nThe original data were measured at an interval or ratio level, but violate one of the following assumptions of a parametric test:\n\nThe distribution of the data was seriously skewed or kurtotic.\nThe data violate the assumptions of equal variance or homoscedasticity.\nFor any reasons , the continuous data were collapsed into a small number of categories, and thus the data are no longer interval or ratio.(miller1982?)\n\n\n\nThe data in the cells should be frequencies, or counts of cases rather than % or some other transformation of the data.\nThe levels (or categories) of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.\nThe study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "title": "Chi-Squared Independence",
    "section": "",
    "text": "The Chi-square test of independence (also known as the Pearson Chi-square test, or simply the Chi-square) is one of the most useful statistics for testing hypotheses when the variables are nominal. It is a non-parametric tool that does not require equality of variances among the study groups or homoscedasticity in the data.(mchugh2013?)\nBeing a non-parametric test tool, Chi-square test can be used when any one of the following conditions pertains to the data:(mchugh2013?)\n\nVariables are nominal or ordinal.\nThe frequency count is &gt;5 for more than 80% of the cells in a contingency table.\nThe sample sizes of the study groups may be of equal size or unequal size but samples do not have equal variance.\nThe original data were measured at an interval or ratio level, but violate one of the following assumptions of a parametric test:\n\nThe distribution of the data was seriously skewed or kurtotic.\nThe data violate the assumptions of equal variance or homoscedasticity.\nFor any reasons , the continuous data were collapsed into a small number of categories, and thus the data are no longer interval or ratio.(miller1982?)\n\n\n\nThe data in the cells should be frequencies, or counts of cases rather than % or some other transformation of the data.\nThe levels (or categories) of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.\nThe study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "title": "Chi-Squared Independence",
    "section": "Hypotheses",
    "text": "Hypotheses\nNull Hypothesis (H0): Outcome variable is independent of type of exposure variables. There is no significant difference in the association of group A/B with outcome variable.\nAlternate Hypothesis: (H1) Outcome variable varies significantly depending upon the type of exposure variable. There is a significant difference in the association of group A/B with outcome variable."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "title": "Chi-Squared Independence",
    "section": "Chi-Squared Independence Test Equation",
    "text": "Chi-Squared Independence Test Equation\n\\(\\chi^2 = \\sum \\frac{{(O_{ij} - E_{ij})^2}}{{E_{ij}}}\\)\nWhere:\n\n\\(\\chi^2\\) The Chi-Squared test statistic.\n\\(\\sum\\chi^2\\) Formula instructions to sum all the cell Chi-square values.\n\\(O_{ij}\\) Observed (the actual frequency in each cell (i, j) of the contingency table.\n\\(E_{ij}\\) Expected frequency in cell (i, j) calculated below.\n\\(\\chi^2{i-j}\\) i-j is the correct notation to represent all the cells, from the first cell (i) to the last cell(j).\n\nCalculating Expected Value\n\\(E = \\frac{M{r} * M{c}}n\\)\nWhere:\n\n\\(E\\) represents the cell expected value,\n\\(M{r}\\) represents the row marginal for that cell,\n\\(M{c}\\) represents the column marginal for that cell, and\n\\(n\\) represents the total sample size.\n\n\nFormula Description\n\nThe first step in calculating a χ2 is to calculate the sum of each row, and the sum of each column. These sums are called the “marginals” and there are row marginal values and column marginal values. \nThe second step is to calculate the expected values for each cell. In the Chi-square statistic, the “expected” values represent an estimate of how the cases would be distributed if there were no effect of exposure variables.\nThen third step is to compute the \\(\\chi^2\\) with above formula."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "title": "Chi-Squared Independence",
    "section": "Performing Chi-Squared Independence Test in R",
    "text": "Performing Chi-Squared Independence Test in R\nThe first step is to load the required packages that will allow us to conduct the test statistic.\n\n\nCode\n# install.package(\"openintro\")\n# install.package(\"gtsummary\")\n# install.packages(\"rstatix\")\n# install.packages(\"vcd\")\n# install.package(\"tidyverse\")\n\n\n\n\nCode\n\nlibrary(openintro)  # for data\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\nlibrary(gtsummary)  # for tables\nlibrary(vcd)        # for mosaic plot\nLoading required package: grid\nlibrary(rstatix)    # for post hoc tests\n\nAttaching package: 'rstatix'\nThe following object is masked from 'package:stats':\n\n    filter\nlibrary(tidyverse)  # for data wrangling and visualization\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks rstatix::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nData Source\nThe openintro package contains data sets used in open-source textbooks such as Introduction to Modern Statistics (1st Ed). (mineçetinkaya-rundel2023?) It is often used for teaching purposes and create examples for how to run various test statistics and functions using R. This package can be installed using the install.packages(\"openintro\") feature. You can also find more information about this package here.\nFor the purposes of this presentations we will be using the diabetes2 dataset found within this package.\nIn the data there are 699 diabetes patients. Each of the 699 patients in the experiment were randomized to one of the following treatments: (1) continued treatment with metformin (coded as met), (2) formin combined with rosiglitazone (coded as rosi), or or (3) a lifestyle-intervention program (coded as lifestyle).Three treatments were compared to test their relative efficacy (effectiveness) in treating Type 2 Diabetes in patients aged 10-17 who were being treated with metformin. The primary outcome was lack of glycemic control (or not); lacking glycemic control means the patient still needed insulin, which is not the preferred outcome for a patient.(todaystudygroup2012?)\n\n\nCode\n\nstr(diabetes2)\ntibble [699 × 2] (S3: tbl_df/tbl/data.frame)\n $ treatment: Factor w/ 3 levels \"lifestyle\",\"met\",..: 2 3 3 1 2 1 1 3 3 2 ...\n $ outcome  : Factor w/ 2 levels \"failure\",\"success\": 2 1 2 2 2 2 2 2 2 1 ...\nprint(diabetes2)\n# A tibble: 699 × 2\n   treatment outcome\n * &lt;fct&gt;     &lt;fct&gt;  \n 1 met       success\n 2 rosi      failure\n 3 rosi      success\n 4 lifestyle success\n 5 met       success\n 6 lifestyle success\n 7 lifestyle success\n 8 rosi      success\n 9 rosi      success\n10 met       failure\n# ℹ 689 more rows\n\n\n\n\nContingency Tables\nThe first step in a Chi-Squared Independence Test involves creating a contingency table that is used to calculate the expected frequencies for each variable. This will help us summarize the data and show the distribution of the variables. This is done using the table() function as seen in the code below.\n\n\nCode\n\n\n# Create the table\ndiabetes_table &lt;- table(\n  diabetes2$outcome,\n  diabetes2$treatment\n)\n\nprint(diabetes_table)\n         \n          lifestyle met rosi\n  failure       109 120   90\n  success       125 112  143\n\n\n\n\nMosaic Plots\nYou can also use a mosaic plot to visualize the data better. Our data is in interger format so we first need to reformat it into factor form. This can we done with the code below.\n\n\nCode\n\n#reformat treatment\ndiabetes2$treatment &lt;- \n  as.factor(diabetes2$treatment)\n\n#print\nhead(diabetes2$treatment)\n[1] met       rosi      rosi      lifestyle met       lifestyle\nLevels: lifestyle met rosi\n\n\n#recode treatment\ndiabetes2$treatment &lt;- \n  recode_factor(\n    diabetes2$treatment,\n            \"lifestyle\" = \"Lifestyle\",\n            \"met\" = \"Metform\",\n            \"rosi\" = \"Rosiglitazone Plus Metformin\"\n)\n\n#print\nhead(diabetes2$treatment)\n[1] Metform                      Rosiglitazone Plus Metformin\n[3] Rosiglitazone Plus Metformin Lifestyle                   \n[5] Metform                      Lifestyle                   \nLevels: Lifestyle Metform Rosiglitazone Plus Metformin\n\n#reformat outcome\ndiabetes2$outcome &lt;- \n  as.factor(diabetes2$outcome)\n\n#print\nhead(diabetes2$outcome)\n[1] success failure success success success success\nLevels: failure success\n\n\n#recode\ndiabetes2$outcome &lt;- \n  recode_factor(\n    diabetes2$outcome,\n            \"failure\" = \"Failure\",\n            \"success\" = \"Success\"\n)\n\n#print\nhead(diabetes2$outcome)\n[1] Success Failure Success Success Success Success\nLevels: Failure Success\n\n\nNext, we can create the mosaic plot using the mosaic() function.\n\n\nCode\n\n#Creating the mosaic plot\nmosaic(\n  ~ treatment + outcome, \n       data = diabetes2,\n          highlighting = \"outcome\", \n          highlighting_fill = c(\"lightgrey\", \"black\"),\n          main = \"Outcome of Interventions for Type 2 Diabetes\",\n          gp_varnames = gpar(fontsize = 14, fontface = 2),\n          gp_labels = gpar(fontsize = 10)\n)\n\n\n\n\n\n\n\n\n\n\n\nRunning the Chi-Squared Test\nThe next step is to run our actual test statistic. This is done using the chisq.test() function as seen in the code below.The correct argument is used to indicate whether to apply continuity correction when computing the test. We are setting this to TRUE since we are dealing with a 2x2 contingency table where we have two categorical variables, each with two levels.\n\n\nCode\n\nchi_sq_result &lt;- chisq.test(diabetes_table, \n                           correct = TRUE)\n\nprint(chi_sq_result)\n\n    Pearson's Chi-squared test\n\ndata:  diabetes_table\nX-squared = 8, df = 2, p-value = 0.02\n\n\n\nTabulating the chisq output in a publishable format using gt_summary\n\n\nCode\n\ntable1 &lt;-   \n  tbl_summary(\n    diabetes2,\n    by = treatment\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Results of Chi Square Test\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\n\nResults of Chi Square Test\n\n\n\n\n\n\n\n\n\nCharacteristic\nLifestyle, N = 2341\nMetform, N = 2321\nRosiglitazone Plus Metformin, N = 2331\np-value2\n\n\n\n\noutcome\n\n\n\n\n\n\n0.017\n\n\n    Failure\n109 (47%)\n120 (52%)\n90 (39%)\n\n\n\n\n    Success\n125 (53%)\n112 (48%)\n143 (61%)\n\n\n\n\n\n1 n (%)\n\n\n2 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\nTest Options\nThe Chi-Squared Independence test statistic has various options in R. A brief description of those options is summarized in the table below.\n\n\n\n\n\n\n\n\nOption\nDescription\nExample\n\n\n\n\nx\nA numeric vector or matrix. x and y can also both be factors.\nx &lt;- matrix(c(10, 20, 30, 40), nrow = 2)\n\n\ncorrect\nA logical indicating whether to apply continuity correction – This is done when the expected frequencies in the contingency table are small (&lt;5).\ncorrect &lt;- TRUE\n\n\np\nA vector of probabilities of the same length as x. An error is given if any entry of p is negative.\np &lt;- c(0.4, 0.6)\n\n\nrescale.p\nA logical scalar; if TRUE then p is rescaled (if necessary) to sum to 1. If rescale.p is FALSE, and p does not sum to 1, an error is given.\nrescale.p &lt;- FALSE\n\n\nsimulate.p.value\nA logical indicating whether to compute p-values by Monte Carlo simulation.\nsimulate.p.value &lt;- FALSE\n\n\nB\nAn integer specifying the number of replicates used in the Monte Carlo test.*\nB &lt;- 1000\n\n\n\n*The Monte Carlo test is a technique that involves simulation to estimate the p-value or test statistic for hypothesis testing. This is used when using complex models and exact p-values cannot be calculated or when the distribution assumptions are violated. (christianp.robert2010?)"
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "title": "Chi-Squared Independence",
    "section": "Interpretation",
    "text": "Interpretation\nOur results give us a chi-squared test statistic of 8.1645 with a p-value of 0.017. Since p value is smaller than critical p value (0.05), we have enough evidence to reject the null hypothesis and conclude that there is a strong association between the type of treatment on Type 2 diabetes. However, we dont’t know which treatment option is significantly different so we are going to do a post hoc test in below code chunk."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html",
    "href": "lessons_original/03_two_sample_mann_whitney.html",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "",
    "text": "Mann Whitney U test, also known as the Wilcoxon Rank-Sum test, is commonly used to compare the means or medians of two independent groups with the assumption that the at least one group is not normally distributed and when sample size is small.\n\nThe Welch U test should be used when there exists signs of skewness and variance of heterogeneity.fagerland2009?\n\nIt is useful for numerical/continuous variables.\n\nFor example, if researchers want to compare two different groups’ age or height (continuous variables) in a study with non-normally distributed data.sundjaja2023?\n\nWhen conducting this test, aside from reporting the p-value, the spread, and the shape of the data should be described.hart2001?\n\nOverall goal: Identify whether the distribution of two groups significantly differs.\n\n\nNull Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equalchiyau2020?\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly differentchiyau2020?\n\n\n\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups;(Male & Female).\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.\n\n\n\n\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#hypotheses",
    "href": "lessons_original/03_two_sample_mann_whitney.html#hypotheses",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "",
    "text": "Null Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equalchiyau2020?\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly differentchiyau2020?\n\n\n\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups;(Male & Female).\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.\n\n\n\n\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#data",
    "href": "lessons_original/03_two_sample_mann_whitney.html#data",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Data",
    "text": "Data\nIn this example, we will perform the Mann-Whitney U Test using wave 8 (2012-2013) data of a longitudinal epidemiological study titled Hispanic Established Populations For the Epidemiological Study of Elderly (HEPESE).\nThe HEPESE provides data on risk factors for mortality and morbidity in Mexican Americans in order to contrast how these factors operate differently in non-Hispanic White Americans, African Americans, and other major ethnic groups.The data is publicly available and can be obtained from the University of Michigan website.kyriakoss.markides2016?\nUsing this data, we want to explore whether there are significant gender differences in age when Type 2 diabetes mellitus (T2DM) is diagnosed. Type 2 diabetes is a chronic disease condition that has affected 37 million people living in the United States. Type 2 diabetes is the eighth leading cause of death and disability in US. Type 2 diabetes generally occurs among adults aged 45 or older although, young adults and children are also diagnosed with it these days. Diabetes and its complications are preventable when following proper lifestyles and timely medications. 1 in 5 of US people don’t know they have diabetes.national2020?\nResearch has shown that men are more likely to develop type 2 diabetes while women are more likely to experience complications, including heart and kidney disease.meissner2021?\nIn this report, we want to test whether there are significant differences in age at which diabetes is diagnosed among males and females.\nDependent Response Variable\nageAtDx = Age_Diagnosed = Age at which diabetes is diagnosed.\nIndependent Variable\nisMale = Gender\nResearch Question:\nDoes the age at which diabetes is diagnosed significantly differ among Men and Women?\nNull Hypothesis (H0): Mean rank of age at which diabetes is diagnosed is equal among men and women.\nAlternate Hypothesis (H1): Mean rank of age at which diabetes is diagnosed is not equal among men and women."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#packages",
    "href": "lessons_original/03_two_sample_mann_whitney.html#packages",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Packages",
    "text": "Packages\n\ngmodels: It helps to compute and display confidence intervals (CI) for model estimates.warnes2022?\nDescTools: It provides tools for basic statistics e.g. to compute Median CI for an efficient data description.andrisignorell2023?\nggplot2: It helps to create Boxplots.\nqqplotr: It helps to create QQ plot.\ndplyr: It is used to manipulate data and provide summary statistics.\nhaven: It helps to import spss data into r.\nDependencies = TRUE : It indicates that while installing packages, it must also install all dependencies of the specified package.\n\n\n\nCode\n# install.packages(\"gmodels\", dependencies = TRUE)\n# install.packages(\"car\", dependencies = TRUE)\n# install.packages(\"DescTools\", dependencies = TRUE)\n# install.packages(\"ggplot2\", dependencies = TRUE)\n# install.packages(\"qqplotr\", dependencies = TRUE)\n# install.packages(\"gtsummary\", dependencies = TRUE)\n\n\nLoading Library\n\n\nCode\n\nsuppressPackageStartupMessages(library(haven))\nsuppressPackageStartupMessages(library(ggpubr))\nsuppressPackageStartupMessages(library(gmodels))\nsuppressPackageStartupMessages(library(DescTools))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(qqplotr))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyverse))\n\n\nData Importing\n\n\nCode\n\n# Mann_W_U &lt;- read_sav(\"data\\\\36578-0001-Data.sav\")\nMann_W_U &lt;- read_sav(\"../data/03_HEPESE_synthetic_20240510.csv\")\nError: Failed to parse /Users/gabrielodom/Documents/GitHub/PHC6099_rBiostat/data/03_HEPESE_synthetic_20240510.csv: Invalid file, or file has unsupported features."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#data-exploration",
    "href": "lessons_original/03_two_sample_mann_whitney.html#data-exploration",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n\nCode\n# str(Mann_W_U)\nstr(Mann_W_U$isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\nstr(Mann_W_U$ageAtDx)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nAfter inspecting the data, we found that values of our dependent and independent variable values are in character form. We want them to be numerical and categorical, respectively. First, we will convert dependent variable into numerical form and our independent variable into categorical. Next, we will recode the factors as male and female. Also for ease, we will rename our dependent and independent variable.\n\n\nCode\n\n# convert to number and factor\nMann_W_U$ageAtDx &lt;- as.numeric(Mann_W_U$ageAtDx)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\nclass(Mann_W_U$ageAtDx)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nMann_W_U$isMale &lt;- as_factor(Mann_W_U$isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\nclass(Mann_W_U$isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nThe next step is to calculate some of the descriptive data to give us a better idea of the data that we are dealing with. This can be done using the summarise function.\nDescriptive Data\n\n\nCode\nDes &lt;- \n Mann_W_U %&gt;% \n select(isMale, ageAtDx) %&gt;% \n group_by(isMale) %&gt;%\n summarise(\n   n = n(),\n   mean = mean(ageAtDx, na.rm = TRUE),\n   sd = sd(ageAtDx, na.rm = TRUE),\n   stderr = sd/sqrt(n),\n   LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,\n   UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,\n   median = median(ageAtDx, na.rm = TRUE),\n   min = min(ageAtDx, na.rm = TRUE), \n   max = max(ageAtDx, na.rm = TRUE),\n   IQR = IQR(ageAtDx, na.rm = TRUE),\n   LCLmed = MedianCI(ageAtDx, na.rm = TRUE)[2],\n   UCLmed = MedianCI(ageAtDx, na.rm = TRUE)[3]\n )\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nDes\nError in eval(expr, envir, enclos): object 'Des' not found\n\n\n\nn: The number of observations for each gender.\nmean: The mean age when diabetes is diagnosed for each gender.\nsd: The standard deviation of each gender.\nstderr: The standard error of each gender level.  That is the standard deviation / sqrt (n).\nLCL, UCL: The upper and lower confidence intervals of the mean.  This values indicates the range at which we can be 95% certain that the true mean falls between the lower and upper values specified for each gender group assuming a normal distribution. \nmedian: The median value for each gender.\nmin, max: The minimum and maximum value for each gender.\nIQR: The interquartile range of each gender. That is the 75th percentile –  25th percentile.\nLCLmed, UCLmed: The 95% confidence interval for the median.\n\nVisual exploration of data\nThe next step is to visualize the data. This can be done using different functions under the ggplot package.\n1) Box plot\n\n\nCode\nggplot(\n Mann_W_U, \n aes(\n   x = isMale, \n   y = ageAtDx, \n   fill = isMale\n )\n) +\n stat_boxplot(\n   geom = \"errorbar\", \n   width = 0.5\n ) +\n geom_boxplot(\n   fill = \"light blue\"\n ) + \n stat_summary(\n   fun.y = mean, \n   geom = \"point\", \n   shape = 10, \n   size = 3.5, \n   color = \"black\"\n ) + \n ggtitle(\n   \"Boxplot of Gender\"\n ) + \n theme_bw() + \n theme(\n   legend.position = \"none\"\n )\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\n2) QQ plot\n\n\nCode\n \nlibrary(conflicted)\nconflict_prefer(\"stat_qq_line\", \"qqplotr\", quiet = TRUE)\n\n\n# Perform QQ plots by group\nQQ_Plot &lt;- \nggplot(\n data = Mann_W_U, \n aes(\n   sample = ageAtDx, \n   color = isMale, \n   fill = isMale\n )\n) +\n stat_qq_band(\n   alpha = 0.5, \n   conf = 0.95, \n   qtype = 1, \n   bandType = \"boot\"\n ) +\n stat_qq_line(\n   identity = TRUE\n ) +\n stat_qq_point(\n   col = \"black\"\n ) +\n facet_wrap(\n   ~ isMale, scales = \"free\"\n ) +\n labs(\n   x = \"Theoretical Quantiles\", \n   y = \"Sample Quantiles\"\n ) + theme_bw()\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nQQ_Plot\nError in eval(expr, envir, enclos): object 'QQ_Plot' not found\n\n\n\nstat_qq_line: Draws a reference line based on the data quantiles.\nStat_qq_band: Draws confidence bands based on three methods; “pointwise”/“boot”,“Ks” and “ts”.\n\n\"pointwise\" constructs simultaneous confidence bands based on the normal distribution;\n\"boot\" creates pointwise confidence bands based on a parametric boostrap;\n\"ks\" constructs simultaneous confidence bands based on an inversion of the Kolmogorov-Smirnov test;\n\"ts\" constructs tail-sensitive confidence bandsaldor-noiman2013?\n\nStat_qq_Point: It is a modified version of ggplot: : stat_qq with some parameters adjustments and a new option to detrend the points.\n3) Histogram\nA histogram is the most commonly used graph to show frequency distributions.\n\n\nCode\nHist &lt;- \n ggplot(\nMann_W_U,\naes(\n  x = ageAtDx,\n  fill = isMale\n)\n  ) +\ngeom_histogram() +\nfacet_wrap(~ isMale) \nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nHist\nError in eval(expr, envir, enclos): object 'Hist' not found\n\n\n3b) Density curve in Histogram\n\nA density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.zach2020?\n\n\nCode\nggplot(\n  Mann_W_U, \n  aes(\n    x = ageAtDx,\n    fill = isMale\n  )\n) + \n  geom_density() +\n  labs(\n    x = \"Age When diabetes is diagnosed\",\n    y = \"Density\",\n    fill = \"Gender\",\n    title = \"A Density Plot of Age when diabetes is diagnosed\",\n    caption = \"Data Source: HEPESE Wave 8 (ICPSR 36578)\"\n  ) +\n  facet_wrap(~isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nThe density curve provided us idea that our data do not have bell shaped distribution and it is slightly skewed towards left.\n4) Statistical test for normality\n\n\nCode\n   Mann_W_U %&gt;%\n group_by(\n   isMale\n ) %&gt;%\n summarise(\n   `W Stat` = shapiro.test(ageAtDx)$statistic,\n    p.value = shapiro.test(ageAtDx)$p.value,\n   options(scipen = 999)\n )\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nInterpretation\nFrom the above table, we see that the value of the Shapiro-Wilk Test is 0.0006 and 0.000002 which are both less than 0.05, therefore we have enough evidence to reject the null hypothesis and confirm that the data significantly deviate from a normal distribution."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#mann-whitney-u-test",
    "href": "lessons_original/03_two_sample_mann_whitney.html#mann-whitney-u-test",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Mann Whitney U Test",
    "text": "Mann Whitney U Test\n\n\nCode\n\nresult &lt;-\n wilcox.test(\n   ageAtDx ~ isMale, \n   data = Mann_W_U, \n   na.rm = TRUE, \n   paired = FALSE, \n   exact = FALSE, \n   conf.int = TRUE\n )\nError in wilcox.test.formula(ageAtDx ~ isMale, data = Mann_W_U, na.rm = TRUE, : cannot use 'paired' in formula method\n\ntest_statistics &lt;- result$statistic\nError in eval(expr, envir, enclos): object 'result' not found\n\np_values &lt;- result$p.value\nError in eval(expr, envir, enclos): object 'result' not found\n\nmethod_used &lt;- result$method\nError in eval(expr, envir, enclos): object 'result' not found\n\nresult_df &lt;- \n  data.frame(\n    Test_Statistic = test_statistics,\n    P_Value = p_values,\n    Method = method_used\n  )\nError in eval(expr, envir, enclos): object 'test_statistics' not found\n\ntbl &lt;- \n  tbl_df(\n    data = result_df\n  ) \nError in eval(expr, envir, enclos): object 'result_df' not found\n\ntbl\nfunction (src, ...) \n{\n    UseMethod(\"tbl\")\n}\n&lt;bytecode: 0x7fbccdb2ee10&gt;\n&lt;environment: namespace:dplyr&gt;"
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html",
    "href": "lessons_original/04_anova_two_way.html",
    "title": "Two-Way ANOVA",
    "section": "",
    "text": "A two-way ANOVA is used to estimate how the mean of a quantitative variable changes according to the levels of two categorical variables.\n\nIt is used to determine how independent (grouping) variables, in combination, affect a dependent (response) variable.\nGrouping variables are also called factors. Levels are the categories of each component."
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#two-way-anova",
    "href": "lessons_original/04_anova_two_way.html#two-way-anova",
    "title": "Two-Way ANOVA",
    "section": "",
    "text": "A two-way ANOVA is used to estimate how the mean of a quantitative variable changes according to the levels of two categorical variables.\n\nIt is used to determine how independent (grouping) variables, in combination, affect a dependent (response) variable.\nGrouping variables are also called factors. Levels are the categories of each component."
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#how-does-it-work",
    "href": "lessons_original/04_anova_two_way.html#how-does-it-work",
    "title": "Two-Way ANOVA",
    "section": "How does it work?",
    "text": "How does it work?\nThe F test is used in ANOVA to determine statistical significance. The F test compares the variance in each group mean to the overall variance in the dependent variable because it is a group-wise comparison test.\nWhen the variance is higher between groups than within the groups, the F test value will be greater, and therefore a higher likelihood that the difference observed is real and not due to chance.\nThere are three null hypotheses tested while doing a two-way ANOVA:\n\nThere is no difference between the group means at any level of the first independent variable.\nThere is no difference between the group means at any level of the second independent variable.\nThe effect of one independent variable does not depend on the effect of the other independent variable (also viewed as no interaction effect),\n\n\nAssumptions needed\nThere are certain assumptions that must be considered before using a two-way ANOVA.\n\nHomogenuity of variance: The variances for each group should be roughly equal.\nIndependence of observations: The observations in each group are independent of each other and the observations within groups were obtained by a random sample.\nNormally distributed: The response variable is approximately normally distributed for each group.\n\nBelow we will see an example on how to test for these assumptions and how to conduct a two-way ANOVA test once we know they have been met."
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#two-way-anova-table",
    "href": "lessons_original/04_anova_two_way.html#two-way-anova-table",
    "title": "Two-Way ANOVA",
    "section": "Two-Way ANOVA Table",
    "text": "Two-Way ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of freedom\nMean Squares\nF value\n\n\n\n\nFactor A\nSS_A\nk-1\nMS_A\nF_A\n\n\nFactor B\nSS_B\nl-1\nMS_B\nF_B\n\n\nInteraction AB\nSS_{AB}\n(k-1)(l-1)\nMS_{AB}\nF_{AB}\n\n\nError\nSS_E\nkl(m-1)\n\n\n\n\nTotal\nSS_T\nklm-1\n\n\n\n\n\nWhere\n\n MS_E := \\frac{SS_E}{kl(m-1)} \n MS_A := \\frac{SS_A}{k-1} \\text{ and } F_A := \\frac{MS_A}{MS_E} \n MS_B := \\frac{SS_B}{l-1} \\text{ and } F_B := \\frac{MS_B}{MS_E} \n MS_{AB} := \\frac{SS_{AB}}{(k-1)(l-1)} \\text{ and } F_{AB} := \\frac{MS_{AB}}{MS_E} \n\nWe explain these components as:\n\nSS_A: Factor A main effect sums of squares with associated df k-1\nSS_B: Factor B main effect sums of squares, with associated df l-1\nSS_{AB}: interaction sum of squares, with associated df (k-1)(l-1)\nSS_E: error sum of squares with associated df kl(m-1)\nSS_T: Total sums of squares, associated with df klm-1\n\n\nExample\nFor this example, an agricultural crop yield dataset was sourced from Scribbr.\nThe dataset contains:\n\nType of fertilizer (1,2,3)\nPlanting density (1 = low, 2 = high)\nBlock number in the field (1,2,3,4)\n\nThis two-way ANOVA will examine whether the type of fertilizer and planting density (independent variables) have an effect on the average crop yield (dependent variable).\n\nLoading Libraries and Data\n\n\nCode\n# needed to create presentation ready table \nlibrary(gt)\n# needed to create presentation ready table \nlibrary(tidymodels)\n# needed to create AIC table to compare all three models \nlibrary(AICcmodavg)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\ncrop_data_df &lt;- read_csv(\"../data/04_crop_data.csv\")\n\n\n\n\nData Exploration\n\n# show first six rows of the dataset\nhead(crop_data_df)\n\n# A tibble: 6 × 4\n  density block fertilizer yield\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1       1     1          1  177.\n2       2     2          1  178.\n3       1     3          1  176.\n4       2     4          1  178.\n5       1     1          1  177.\n6       2     2          1  177.\n\n# overview of summary statistics\nsummary(crop_data_df)\n\n    density        block        fertilizer     yield      \n Min.   :1.0   Min.   :1.00   Min.   :1    Min.   :175.4  \n 1st Qu.:1.0   1st Qu.:1.75   1st Qu.:1    1st Qu.:176.5  \n Median :1.5   Median :2.50   Median :2    Median :177.1  \n Mean   :1.5   Mean   :2.50   Mean   :2    Mean   :177.0  \n 3rd Qu.:2.0   3rd Qu.:3.25   3rd Qu.:3    3rd Qu.:177.4  \n Max.   :2.0   Max.   :4.00   Max.   :3    Max.   :179.1  \n\n\n\nBoxplots\n\n\nCode\nplot_fertilizer &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(fertilizer),\n      y = yield,\n      color = as.factor(fertilizer) \n    ) +\n  labs(\n    x = \"Fertilizer Type\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_fertilizer\n\n\n\n\n\n\n\n\n\nCode\nplot_density &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(density),\n      y = yield,\n      color = as.factor(density)\n    ) +\n  labs(\n    x = \"Planting Density\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_density\n\n\n\n\n\n\n\n\n\n\n\n\nNull\n\nH01: There is no statistical difference in average yield for any fertilizer type\nH02: There is no difference in average yield between either planting density\nH03: The effect of fertilizer type on average yield is not dependent on the effect of planting density - no interaction effect\n\n\n\nAlternative\n\nH11: There is a difference in the average yield for fertilizer types\nH12: There is a difference in the average yield based on planting density\nH13: There is an interaction effect between planting density and fertilizer type average yield\n\n\n\nPerforming the two-way ANOVA\nFirst we want to test without interaction between the two independent variables for our first model. The code below is\n\n\nCode\n# converting into a factor for post-hoc assessment \ncrop_data_df$fertilizer &lt;- as.factor(crop_data_df$fertilizer)\ncrop_data_df$density &lt;- as.factor(crop_data_df$density)\n\n# performing the two-way ANOVA\ntwo_way &lt;- \n  aov(yield ~ fertilizer + density, data = crop_data_df)\n\n# creating a tidy table using gt\ntable_1 &lt;- two_way %&gt;% \n  tidy() %&gt;% \n  gt()\n\n# customizing table \ntable_1 |&gt;\n   tab_header(\n      title = \"ANOVA Results\",\n      subtitle = \"Two-way ANOVA for yield\"\n    )\n\n\n\n\n\n\n\n\n\nANOVA Results\n\n\nTwo-way ANOVA for yield\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.068047\n3.0340233\n9.073123\n0.0002532992\n\n\ndensity\n1\n5.121681\n5.1216812\n15.316179\n0.0001741418\n\n\nResiduals\n92\n30.764505\n0.3343968\nNA\nNA\n\n\n\n\n\n\n\n\nWe also want to test a model that shows interaction between the two independent variables.\n\n\nCode\n# performing the two-way ANOVA with interaction \ninteraction &lt;- \n  aov(yield ~ fertilizer * density, data = crop_data_df)\n\n# creating a tidy table using gt\ntable_int &lt;- interaction %&gt;% \n  tidy() %&gt;% \n  gt()\n\n# customizing table \ntable_int |&gt;\n   tab_header(\n      title = \"ANOVA Results\",\n      subtitle = \"Two-way ANOVA for yield with interaction term\"\n    )\n\n\n\n\n\n\n\n\n\nANOVA Results\n\n\nTwo-way ANOVA for yield with interaction term\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0010522\n0.0002731890\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.1945174\n0.0001864075\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6346053\n0.5325000914\n\n\nResiduals\n90\n30.3366866\n0.3370743\nNA\nNA\n\n\n\n\n\n\n\n\nThe p-value for both independent variables are less than 0.05, therefore we reject the null hypotheses (H01 and H02).\nThe p value for the interaction term is greater than 0.05, hence we fail to reject the null hypothesis (H03). Hence not much variation can be explained by the interaction term.\n\n\nBlocking Variable - 3rd Model\nThe crops were planted in across various blocks that may differ in other factors such as sunlight, moisture etc. This could possibly lead to confounding. Therefore it is important to control for the effect of differences between blocks by adding the third variable to our tests.\n\n\nCode\n# performing two-way anova with interaction and blocking\nblocking &lt;- \n  aov(yield ~ fertilizer * density + block, data = crop_data_df)\n\n# creating a tidy table using gt \ntable_block &lt;- blocking %&gt;% \n  tidy() %&gt;% \n  gt()\n\n# customizes gt table\ntable_block |&gt;\n   tab_header(\n      title = \"Two- way ANOVA Results\",\n      subtitle = \"with interaction term & blocking variable\"\n    )\n\n\n\n\n\n\n\n\n\nTwo- way ANOVA Results\n\n\nwith interaction term & blocking variable\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0459852\n0.0002652607\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.2703680\n0.0001813274\n\n\nblock\n1\n0.4860877\n0.4860877\n1.4492776\n0.2318360383\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6377732\n0.5308657318\n\n\nResiduals\n89\n29.8505990\n0.3354000\nNA\nNA\n\n\n\n\n\n\n\n\nFor the block variable the sum of squares is low and the p value is greater than 0.05. Hence not much information is added to the model. The sum of square for both independent variables also remain unchanged.\n\n\nDetermining the Best Fit Model\nThe Akaike information criterion (AIC) can be used to determine the best model. AIC balances the variation explained by the number of parameters used to calculate the information value of each model. The lower the AIC value, the more information explained.\n\n\nCode\nmodel_set &lt;- list(two_way, interaction, blocking)\nmodel_names &lt;- c(\"two_way\", \"interaction\", \"blocking\")\n \ngt_fmt &lt;-\n  aictab(model_set, modnames = model_names) \n\ngt_print &lt;- \n  gt(gt_fmt)\n\ngt_print\n\n\n\n\n\n\n\n\n\nModnames\nK\nAICc\nDelta_AICc\nModelLik\nAICcWt\nLL\nCum.Wt\n\n\n\n\ntwo_way\n5\n173.8562\n0.000000\n1.0000000\n0.75476252\n-81.59474\n0.7547625\n\n\ninteraction\n7\n177.1178\n3.261693\n0.1957638\n0.14775516\n-80.92256\n0.9025177\n\n\nblocking\n8\n177.9496\n4.093464\n0.1291563\n0.09748232\n-80.14722\n1.0000000\n\n\n\n\n\n\n\n\nAs shown in this table, the two_way model is the best fit for our crop data analysis.\n\n\n\nChecking for homoscedasticity\n\n\nCode\npar(mfrow=c(2,2))\nplot(two_way)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\n\n\nThe output of the residual means shows no large outliers that could possibly skew the data. Hence we can assume equal variances. The Q-Q plot also depicts normality."
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "href": "lessons_original/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "title": "Two-Way ANOVA",
    "section": "Post-hoc testing (Tukey HSD)",
    "text": "Post-hoc testing (Tukey HSD)\nIn order to determine how the levels differ from one another the Tukey’s Honestly-Significant-difference test can be used.\n\n\nCode\ntukey_crop &lt;- TukeyHSD(two_way)  \n  \ntukey_crop %&gt;% \n  tidy %&gt;% \n  gt() %&gt;% \n  tab_header(\n      title = \"Tukey Multiple Comparisons of means\",\n      subtitle = \"for fertilizer & density\"\n  )\n\n\n\n\n\n\n\n\n\nTukey Multiple Comparisons of means\n\n\nfor fertilizer & density\n\n\nterm\ncontrast\nnull.value\nestimate\nconf.low\nconf.high\nadj.p.value\n\n\n\n\nfertilizer\n2-1\n0\n0.1761687\n-0.16822506\n0.5205625\n0.4452958212\n\n\nfertilizer\n3-1\n0\n0.5991256\n0.25473179\n0.9435194\n0.0002218678\n\n\nfertilizer\n3-2\n0\n0.4229569\n0.07856306\n0.7673506\n0.0119381379\n\n\ndensity\n2-1\n0\n0.4619560\n0.22752045\n0.6963916\n0.0001741423\n\n\n\n\n\n\n\n\nThis table shows of pairwise differences between each level in the independent variables. Comparisons with p values less than 0.05 are termed significant;\n\nfertilizer types 1 and 3\nfertilizer types 2 and 3\nplanting density groups (binary)\n\n\n\nCode\nplot(tukey_crop, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly the fertilizer comparison of type 1 & 2 confidence interval includes 0; no significant statistical difference\n\n\nSummary Chart of Crop yield data\n\n\nCode\nanova_plot &lt;- \n  ggplot(crop_data_df,\n    aes(\n      x = density, \n      y = yield, \n      group = fertilizer, \n      color = fertilizer\n    )\n  ) +\n  geom_point(\n    cex = 1.5, \n    pch = 1.0, \n    position = position_jitter(w = 0.1, h = 0.1)\n  )\n\nanova_plot &lt;- anova_plot + \n  stat_summary(fun.data = 'mean_se', geom = 'errorbar', width = 0.2, color = \"grey50\") +\n  stat_summary(fun.data = 'mean_se', geom = 'pointrange') \n\nanova_plot &lt;- anova_plot +\n  facet_wrap(~ fertilizer)\n\nanova_plot &lt;- anova_plot +\n  theme_classic() +\n  labs(title = \"Crop yield averages based on fertilizer types and planting density\",\n      x = \"Planting density (1 = low density, 2 = high density)\",\n      y = \"Yield Average\")\n\nanova_plot"
  },
  {
    "objectID": "lessons_original/04_regression_mls.html",
    "href": "lessons_original/04_regression_mls.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Multiple regression generally explains the relationship between multiple independent or predictor variables and one dependent or criterion variable. A dependent variable is modeled as a function of several independent variables with corresponding coefficients, along with the constant term. Multiple regression requires two or more predictor variables.\nEquation\n\\[\n\\hat{Y}= b_0 +b_1x_1 +b_2x_2 + ... b_nx_n + \\epsilon\n\\]\nwhere\n\\(\\hat{Y}\\) is the predicted or expected value of the dependent variable, and is always numeric and continuous.\n\\(b_0\\) is the value of \\(\\hat{Y}\\) when all of the independent variables \\(x_1 -x_n\\) are equal to zero,\n\\(x_1 -x_n\\) are distinct independent or predictor variables\n\\(b_1 - b_n\\) are the estimated regression coefficients. Each regression coefficient represents the change in \\(\\hat{Y}\\) relative to a one unit change in the respective independent variable.\nIn the multiple regression situation, \\(b_1\\), for example, is the change in \\(\\hat{Y}\\) relative to a one unit change in \\(x_1\\), holding all other independent variables constant (i.e., when the remaining independent variables are held at the same value or are fixed).\nAssumptions\n\nIndependence: The Y-values are statistically independent of each other as well as the errors. As with simple linear regression, this assumption is violated if several Y observations are made on the same subject.\nLinearity: The mean value of Y for each specific combination of values of the independent variables (\\(X_1, X_2...X_n\\)) is a linear function of the intercept and parameters (\\(\\beta_0, \\beta_1,...\\))\nHomoscedasticity: The variance of Y is the same for any fixed combination of independent variables.\nNormal Distribution: The residual values are normally distributed with mean zero.\nMulticollinearity: Multicollinearity cannot exist among the predictors (the variables are not correlated)\n\n\n\n\n\nCode\n\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(car)\nlibrary(tidymodels)\nlibrary(lmtest)\nlibrary(tidyverse)\n\n\n\n\n\nThe miles/gallon a car takes (dependent) based on the gross horsepower and weight of the car.\n\n\nCode\n\ndata(\"mtcars\")\nhead(mtcars)\n                   mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\n\n\n\n\n\n\n\nIs there a linear relationship between the dependent variable and each of the independent variables?\n\n\nCode\n\n# is there a linear relationship between MPG and HP?\nggplot(data = mtcars) +\n  aes(\n    x = hp,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\n\n# is there a linear relationship between MPG and WT?\nggplot(data = mtcars) +\n  aes(\n    x = wt,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nAre the variables correlated? If the VIF score &gt; 10 then there is multicollinearity.\n\n\nCode\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# checking the VIF of both predictors\nvif(mod)\n  hp   wt \n1.77 1.77 \n\n# checking the correlation between the predictors\ncor(mtcars$hp, mtcars$wt)\n[1] 0.659\n\n\n\n\n\nChecking the residual plots for homoscedasticity. The line below is relatively horizontal.\n\n\nCode\n\nmtcars_df &lt;- mtcars %&gt;% \n  mutate(res_sqrt = sqrt(abs(rstandard(mod))))\n\nggplot(mtcars_df, aes(fitted(mod), res_sqrt)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\nChecking normality via histogram and QQ plot of the residuals.\n\n\nCode\n\n# histogram\nhist(mtcars_df$res_sqrt)\n\n\n\n\n\n\n\n\n\nCode\n\n# qq plot\nplot(mod, 2)\n\n\n\n\n\n\n\n\n\nCode\n\n# cook's d -&gt; are the outliers in the qq plot driving the relationship in the model? \ncooks.distance(mod)\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n           1.59e-02            5.46e-03            2.07e-02            4.72e-05 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n           2.74e-04            2.16e-02            1.26e-02            1.68e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n           2.19e-03            1.55e-03            1.22e-02            1.42e-03 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n           1.46e-04            6.27e-03            2.79e-05            1.78e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n           4.24e-01            1.57e-01            9.37e-03            2.08e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n           2.79e-02            2.09e-02            2.75e-02            9.94e-03 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n           1.44e-02            5.92e-04            5.67e-06            7.35e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n           8.92e-03            5.73e-03            2.72e-01            5.60e-03 \n\n# Plot Cook's Distance with a horizontal line at 4/n to see which observations\n# exceed this thresdhold\nn &lt;- nrow(mtcars)\nplot(cooks.distance(mod), main = \"Cooks Distance for Influential Obs\")\nabline(h = 4/n, lty = 2, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nCook’s distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.\nWe can clearly see that four observation in the dataset exceed the 4/n threshold. Thus, we would identify these two observations as influential data points that have a negative impact on the regression model.\nSince there are very few extreme values according to cook’s d values, we can assume they are not the driving force between the relationship of the predictor and outcome.\nTentative model:\nmpg = \\(b_0\\) + \\(b_1\\times\\)horsepower + \\(b_2\\times\\)weight + \\(\\epsilon\\)\n\n\n\n\nDoes the horsepower and weight of a car contribute significantly to the miles per gallon of the car?\n\nWhat is the relationship between horsepower and weight, with miles per gallon of the car?-\n\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\n\\[\nH_1: \\beta_1 \\not= \\beta_2 \\not= 0\n\\]\n\n\nCode\ncar_model &lt;- lm(mpg ~ hp + wt, data = mtcars_df)\n\n# get the F statistic and p-value\nsummary(car_model) \n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879   23.28  &lt; 2e-16 ***\nhp          -0.03177    0.00903   -3.52   0.0015 ** \nwt          -3.87783    0.63273   -6.13  1.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.59 on 29 degrees of freedom\nMultiple R-squared:  0.827, Adjusted R-squared:  0.815 \nF-statistic: 69.2 on 2 and 29 DF,  p-value: 9.11e-12\n\n# creating a table of the model\ncar_model %&gt;%\n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n37.2273\n1.59879\n23.28\n2.57e-20\n\n\nhp\n-0.0318\n0.00903\n-3.52\n1.45e-03\n\n\nwt\n-3.8778\n0.63273\n-6.13\n1.12e-06\n\n\n\n\n\n\n\n\nThe first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary.\nIn our example, it can be seen that p-value of the F-statistic is &lt; 9.109e-12, which is less than 0.05. This means that we can reject the null and accept that either horsepower and/or weight of a car predict the miles per gallon.\nTo see which predictor variables are significant, we can examine the coefficients table, which shows the estimate and the associated t-statistic p-values:\n\n\nCode\nsummary(car_model)$coefficient \n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  37.2273    1.59879   23.28 2.57e-20\nhp           -0.0318    0.00903   -3.52 1.45e-03\nwt           -3.8778    0.63273   -6.13 1.12e-06\n\nsummary(car_model)$coefficient %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"Estimate\") %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nIntercept\n37.2273\n1.59879\n23.28\n2.57e-20\n\n\nHorsepower\n-0.0318\n0.00903\n-3.52\n1.45e-03\n\n\nWeight\n-3.8778\n0.63273\n-6.13\n1.12e-06\n\n\n\n\n\n\n\n\nFull model: mpg = 37.23 + (-0.03)\\(\\times\\)horsepower + (-3.88)\\(\\times\\)weight\nFor a given predictor, the t-statistic value evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.\nIt can be seen that horsepower and weight are significantly associated to changes in mpg.\n\n\nCode\n# The confidence interval of the model\nconfint(car_model)\n              2.5 %  97.5 %\n(Intercept) 33.9574 40.4972\nhp          -0.0502 -0.0133\nwt          -5.1719 -2.5837\n\nconfint(car_model) %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"2.5 %\") %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nVariable\n2.5 %\n97.5 %\n\n\n\n\nIntercept\n33.9574\n40.4972\n\n\nHorsepower\n-0.0502\n-0.0133\n\n\nWeight\n-5.1719\n-2.5837\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\): The full model and the nested model fit the data equally well. Thus, you should use the nested model.\n\\(H_1\\): The full model fits the data significantly better than the nested model. Thus, you should use the full model.\nA likelihood ratio test compares the goodness of fit of two nested regression models.\nA nested model is simply one that contains a subset of the predictor variables in the overall regression model.\nWe could then carry out another likelihood ratio test to determine if a model with only one predictor variable is significantly different from a model with the two predictors:\n\n\nCode\n\n#fit full model\nmodel_full &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ hp, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ hp\n  #Df LogLik Df Chisq Pr(&gt;Chisq)    \n1   4  -74.3                        \n2   3  -87.6 -1  26.6    2.5e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ wt, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ wt\n  #Df LogLik Df Chisq Pr(&gt;Chisq)    \n1   4  -74.3                        \n2   3  -80.0 -1  11.4    0.00074 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOur p-values are less than 0.05 for both Chi-squared values, suggesting that the full model offers significant improvement in fit over the model with just one of the predictors.\n\n\n\nIn multiple linear regression, the \\(R^2\\) represents the correlation coefficient between the observed values of the outcome variable (y) and the fitted (i.e., predicted) values of y. For this reason, the value of R will always be positive and will range from zero to one.\nAn \\(R^2\\) value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.\n\n\nCode\nsummary(car_model)$adj.r.squared %&gt;% \n  as_tibble() %&gt;% \n  rename(\"Adjusted R Squared\" = value) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nAdjusted R Squared\n\n\n\n\n0.815\n\n\n\n\n\n\n\n\nOur model explains 81% of the variation in mpg values caused by horsepower and weight values."
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#packages",
    "href": "lessons_original/04_regression_mls.html#packages",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Code\n\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(car)\nlibrary(tidymodels)\nlibrary(lmtest)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#data",
    "href": "lessons_original/04_regression_mls.html#data",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The miles/gallon a car takes (dependent) based on the gross horsepower and weight of the car.\n\n\nCode\n\ndata(\"mtcars\")\nhead(mtcars)\n                   mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.46 20.2  1  0    3    1"
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#testing-the-assumptions",
    "href": "lessons_original/04_regression_mls.html#testing-the-assumptions",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Is there a linear relationship between the dependent variable and each of the independent variables?\n\n\nCode\n\n# is there a linear relationship between MPG and HP?\nggplot(data = mtcars) +\n  aes(\n    x = hp,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\n\n# is there a linear relationship between MPG and WT?\nggplot(data = mtcars) +\n  aes(\n    x = wt,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nAre the variables correlated? If the VIF score &gt; 10 then there is multicollinearity.\n\n\nCode\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# checking the VIF of both predictors\nvif(mod)\n  hp   wt \n1.77 1.77 \n\n# checking the correlation between the predictors\ncor(mtcars$hp, mtcars$wt)\n[1] 0.659\n\n\n\n\n\nChecking the residual plots for homoscedasticity. The line below is relatively horizontal.\n\n\nCode\n\nmtcars_df &lt;- mtcars %&gt;% \n  mutate(res_sqrt = sqrt(abs(rstandard(mod))))\n\nggplot(mtcars_df, aes(fitted(mod), res_sqrt)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\nChecking normality via histogram and QQ plot of the residuals.\n\n\nCode\n\n# histogram\nhist(mtcars_df$res_sqrt)\n\n\n\n\n\n\n\n\n\nCode\n\n# qq plot\nplot(mod, 2)\n\n\n\n\n\n\n\n\n\nCode\n\n# cook's d -&gt; are the outliers in the qq plot driving the relationship in the model? \ncooks.distance(mod)\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n           1.59e-02            5.46e-03            2.07e-02            4.72e-05 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n           2.74e-04            2.16e-02            1.26e-02            1.68e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n           2.19e-03            1.55e-03            1.22e-02            1.42e-03 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n           1.46e-04            6.27e-03            2.79e-05            1.78e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n           4.24e-01            1.57e-01            9.37e-03            2.08e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n           2.79e-02            2.09e-02            2.75e-02            9.94e-03 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n           1.44e-02            5.92e-04            5.67e-06            7.35e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n           8.92e-03            5.73e-03            2.72e-01            5.60e-03 \n\n# Plot Cook's Distance with a horizontal line at 4/n to see which observations\n# exceed this thresdhold\nn &lt;- nrow(mtcars)\nplot(cooks.distance(mod), main = \"Cooks Distance for Influential Obs\")\nabline(h = 4/n, lty = 2, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nCook’s distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.\nWe can clearly see that four observation in the dataset exceed the 4/n threshold. Thus, we would identify these two observations as influential data points that have a negative impact on the regression model.\nSince there are very few extreme values according to cook’s d values, we can assume they are not the driving force between the relationship of the predictor and outcome.\nTentative model:\nmpg = \\(b_0\\) + \\(b_1\\times\\)horsepower + \\(b_2\\times\\)weight + \\(\\epsilon\\)"
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#hypothesis-testing",
    "href": "lessons_original/04_regression_mls.html#hypothesis-testing",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Does the horsepower and weight of a car contribute significantly to the miles per gallon of the car?\n\nWhat is the relationship between horsepower and weight, with miles per gallon of the car?-\n\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\n\\[\nH_1: \\beta_1 \\not= \\beta_2 \\not= 0\n\\]\n\n\nCode\ncar_model &lt;- lm(mpg ~ hp + wt, data = mtcars_df)\n\n# get the F statistic and p-value\nsummary(car_model) \n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879   23.28  &lt; 2e-16 ***\nhp          -0.03177    0.00903   -3.52   0.0015 ** \nwt          -3.87783    0.63273   -6.13  1.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.59 on 29 degrees of freedom\nMultiple R-squared:  0.827, Adjusted R-squared:  0.815 \nF-statistic: 69.2 on 2 and 29 DF,  p-value: 9.11e-12\n\n# creating a table of the model\ncar_model %&gt;%\n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n37.2273\n1.59879\n23.28\n2.57e-20\n\n\nhp\n-0.0318\n0.00903\n-3.52\n1.45e-03\n\n\nwt\n-3.8778\n0.63273\n-6.13\n1.12e-06\n\n\n\n\n\n\n\n\nThe first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary.\nIn our example, it can be seen that p-value of the F-statistic is &lt; 9.109e-12, which is less than 0.05. This means that we can reject the null and accept that either horsepower and/or weight of a car predict the miles per gallon.\nTo see which predictor variables are significant, we can examine the coefficients table, which shows the estimate and the associated t-statistic p-values:\n\n\nCode\nsummary(car_model)$coefficient \n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  37.2273    1.59879   23.28 2.57e-20\nhp           -0.0318    0.00903   -3.52 1.45e-03\nwt           -3.8778    0.63273   -6.13 1.12e-06\n\nsummary(car_model)$coefficient %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"Estimate\") %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nIntercept\n37.2273\n1.59879\n23.28\n2.57e-20\n\n\nHorsepower\n-0.0318\n0.00903\n-3.52\n1.45e-03\n\n\nWeight\n-3.8778\n0.63273\n-6.13\n1.12e-06\n\n\n\n\n\n\n\n\nFull model: mpg = 37.23 + (-0.03)\\(\\times\\)horsepower + (-3.88)\\(\\times\\)weight\nFor a given predictor, the t-statistic value evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.\nIt can be seen that horsepower and weight are significantly associated to changes in mpg.\n\n\nCode\n# The confidence interval of the model\nconfint(car_model)\n              2.5 %  97.5 %\n(Intercept) 33.9574 40.4972\nhp          -0.0502 -0.0133\nwt          -5.1719 -2.5837\n\nconfint(car_model) %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"2.5 %\") %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nVariable\n2.5 %\n97.5 %\n\n\n\n\nIntercept\n33.9574\n40.4972\n\n\nHorsepower\n-0.0502\n-0.0133\n\n\nWeight\n-5.1719\n-2.5837"
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#likelihood-ratio-test",
    "href": "lessons_original/04_regression_mls.html#likelihood-ratio-test",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "\\(H_0\\): The full model and the nested model fit the data equally well. Thus, you should use the nested model.\n\\(H_1\\): The full model fits the data significantly better than the nested model. Thus, you should use the full model.\nA likelihood ratio test compares the goodness of fit of two nested regression models.\nA nested model is simply one that contains a subset of the predictor variables in the overall regression model.\nWe could then carry out another likelihood ratio test to determine if a model with only one predictor variable is significantly different from a model with the two predictors:\n\n\nCode\n\n#fit full model\nmodel_full &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ hp, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ hp\n  #Df LogLik Df Chisq Pr(&gt;Chisq)    \n1   4  -74.3                        \n2   3  -87.6 -1  26.6    2.5e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ wt, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ wt\n  #Df LogLik Df Chisq Pr(&gt;Chisq)    \n1   4  -74.3                        \n2   3  -80.0 -1  11.4    0.00074 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOur p-values are less than 0.05 for both Chi-squared values, suggesting that the full model offers significant improvement in fit over the model with just one of the predictors."
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#model-accuracy",
    "href": "lessons_original/04_regression_mls.html#model-accuracy",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In multiple linear regression, the \\(R^2\\) represents the correlation coefficient between the observed values of the outcome variable (y) and the fitted (i.e., predicted) values of y. For this reason, the value of R will always be positive and will range from zero to one.\nAn \\(R^2\\) value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.\n\n\nCode\nsummary(car_model)$adj.r.squared %&gt;% \n  as_tibble() %&gt;% \n  rename(\"Adjusted R Squared\" = value) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nAdjusted R Squared\n\n\n\n\n0.815\n\n\n\n\n\n\n\n\nOur model explains 81% of the variation in mpg values caused by horsepower and weight values."
  },
  {
    "objectID": "lessons_original/04_correlation.html",
    "href": "lessons_original/04_correlation.html",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "",
    "text": "Code\n#install.packages(\"skimr\")\nlibrary(skimr)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(ggplot2)\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\n#install.packages(\"patchwork\")\nlibrary(patchwork)\nlibrary(tidyverse)\n#install.packages(\"gtsummary\")\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(corrplot)\ncorrplot 0.92 loaded\nlibrary(xtable)\n#install.packages(\"modelsummary\")\nlibrary(modelsummary)\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n\nChange the default backend persistently:\n\n  config_modelsummary(factory_default = 'gt')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\nlibrary(psych)\n#install.packages(\"rstatix\")\nlibrary(rstatix)\n#install.packages(\"PerformanceAnalytics\")\nlibrary(PerformanceAnalytics)\nLoading required package: xts\nLoading required package: zoo\n\nAttaching package: 'zoo'\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\nAttaching package: 'xts'\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n#install.packages(\"ppcor\")\nlibrary(ppcor)\nLoading required package: MASS\n\nAttaching package: 'MASS'\nThe following object is masked from 'package:rstatix':\n\n    select\nThe following object is masked from 'package:gtsummary':\n\n    select\nThe following object is masked from 'package:patchwork':\n\n    area\nThe following object is masked from 'package:dplyr':\n\n    select\n#install.packages(\"remotes\")\nlibrary(remotes)\n#install.packages(\"conflicted\")\nlibrary(conflicted)"
  },
  {
    "objectID": "lessons_original/04_correlation.html#hypothesis-testing-in-correlations",
    "href": "lessons_original/04_correlation.html#hypothesis-testing-in-correlations",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Hypothesis testing in correlations",
    "text": "Hypothesis testing in correlations\nThe bivariate Pearson Correlation produces a sample correlation coefficient, r, which measures the strength and direction of linear relationships between pairs of continuous variables in a sample. By extension, the Pearson Correlation evaluates whether there is statistical evidence for a linear relationship among the same pairs of variables in the population, represented by a population correlation coefficient, ρ (“rho”), which is unknown. The sample pearson correlation coefficient, r, is the estimate of the unknown population coefficient (ρ). Therefore,\n\nThe hypothesis test lets us decide whether the value of the population correlation coefficient ρ is “close to zero” or “significantly different from zero”. We decide this based on the sample correlation coefficient r and the sample size n .\n\nTwo-tailed significance test:\nThe null hypothesis for a two-tailed significance test of a correlation is: \\[\nH_0: ρ = 0\n\\] rho is zero or approximately zero. This means that the relationship in the population is equal to 0; (“there is no association”)\nThe alternative hypothesis for a two-tailed significance test of a correlation is: \\[\nH_1: ρ ≠ 0\n\\] rho is not zero, which means that the relationship in the population is not equal to 0; (“there is an association in the population”)\nOne-tailed significance test:\nThe null hypothesis for a one-tailed significance test of a correlation is: \\[\nH_0: ρ = 0\n\\] which means that the relationship in the population is equal to 0; (“there is no association”)\nThe alternative hypothesis for a one-tailed significance test of a correlation where ρ &gt; 0 is: \\[\nH_1: ρ &gt; 0\n\\] which means that there is a positive relationship in the population.\nOR\nThe alternative hypothesis for a one-tailed significance test of a correlation where ρ &lt; 0 is: \\[\nH_1: ρ &lt; 0\n\\] which means that there is a negative relationship in the population.\nThese hypotheses are in terms of ρ."
  },
  {
    "objectID": "lessons_original/04_correlation.html#assumptions-of-a-correlation-matrix",
    "href": "lessons_original/04_correlation.html#assumptions-of-a-correlation-matrix",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Assumptions of a Correlation Matrix:",
    "text": "Assumptions of a Correlation Matrix:\n1. The two variables should be measured at the interval or ratio level (e.i., continuous).\na.  If interested in the relationship between two categorical variables or ranks (categorical is misleading) you can use \"Spearman\" as the type of correlation. But if you have two binary variables (i.e. living/dead, black/white, success/failure), then use a Phi Correlation coefficient.\n\nb.  If interested in a mix, one can use various functions like \"mixedCor\" in the `psych` package.\n\nc.  If interested in how the levels of a variable like male or female affect the correlation of two variables, we conduct a partial correlation which allow us to control for a third or more other variables using the function \"pcor.test\" in the `ppcor` package.\n2. There should exist a linear relationship between the two variables.\n3. Variables should be roughly normally distributed.\n4. Each observation in the dataset should have a pair of values.\n5. There should be no extreme outliers in the dataset."
  },
  {
    "objectID": "lessons_original/04_correlation.html#checking-assumptions",
    "href": "lessons_original/04_correlation.html#checking-assumptions",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Checking Assumptions",
    "text": "Checking Assumptions\n\nLevel of Measurement\nUsing the skimr package, we will get data set information such as the distribution (histogram) of, and the type/level of measurement for the variables in the “mtcars” data set.\n\n\nCode\nskim(mtcars)\n\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nThe skimr function nicely displays the data. It indicates that all the variables are currently in numeric format but based on the descriptive information and on the information provided by ?mtcars, only 6 variables meet the level of measurement assumption (interval, ratio), we will then create a data frame from mtcars named cars_df where we leave these 6 variables as continuous double/numeric and categorize the other 5 variables. We present the summary using the gtsummary package.\n\n\nCode\ncars_df &lt;- within(mtcars, {\n   vs &lt;- factor(vs, labels = c(\"V\", \"S\"))\n   am &lt;- factor(am, labels = c(\"automatic\", \"manual\"))\n   cyl  &lt;- ordered(cyl)\n   gear &lt;- ordered(gear)\n   carb &lt;- ordered(carb)\n})\n\ncars_df %&gt;%\n  gtsummary::tbl_summary(\n    label = list(\n      mpg ~ \"Miles/(US) gallon\",\n      cyl ~ \"Number of cylinders\",\n      disp ~ \"Displacement (cu.in.)\",\n      hp ~  \"Gross horsepower\",\n      drat ~ \"Rear axle ratio\",\n      wt ~  \"Weight (1000 lbs)\",\n      qsec ~ \"1/4 mile time\",\n      vs ~  \"Engine (0 = V-shaped, 1 = straight)\",\n      am ~ \"Transmission (0 = automatic, 1 = manual)\",\n      gear ~ \"Number of forward gears\"),\n    type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous() ~ c(\n      \"{median} ({p25}, {p75})\",\n      \"{min}, {max}\"\n    )\n  ) %&gt;%\n  add_n() %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Variable**\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nN = 321\n\n\n\n\nMiles/(US) gallon\n32\n\n\n\n\n    Median (IQR)\n\n\n19.2 (15.4, 22.8)\n\n\n    Range\n\n\n10.4, 33.9\n\n\nNumber of cylinders\n32\n\n\n\n\n    4\n\n\n11 (34%)\n\n\n    6\n\n\n7 (22%)\n\n\n    8\n\n\n14 (44%)\n\n\nDisplacement (cu.in.)\n32\n\n\n\n\n    Median (IQR)\n\n\n196 (121, 326)\n\n\n    Range\n\n\n71, 472\n\n\nGross horsepower\n32\n\n\n\n\n    Median (IQR)\n\n\n123 (97, 180)\n\n\n    Range\n\n\n52, 335\n\n\nRear axle ratio\n32\n\n\n\n\n    Median (IQR)\n\n\n3.70 (3.08, 3.92)\n\n\n    Range\n\n\n2.76, 4.93\n\n\nWeight (1000 lbs)\n32\n\n\n\n\n    Median (IQR)\n\n\n3.33 (2.58, 3.61)\n\n\n    Range\n\n\n1.51, 5.42\n\n\n1/4 mile time\n32\n\n\n\n\n    Median (IQR)\n\n\n17.71 (16.89, 18.90)\n\n\n    Range\n\n\n14.50, 22.90\n\n\nEngine (0 = V-shaped, 1 = straight)\n32\n\n\n\n\n    V\n\n\n18 (56%)\n\n\n    S\n\n\n14 (44%)\n\n\nTransmission (0 = automatic, 1 = manual)\n32\n\n\n\n\n    automatic\n\n\n19 (59%)\n\n\n    manual\n\n\n13 (41%)\n\n\nNumber of forward gears\n32\n\n\n\n\n    3\n\n\n15 (47%)\n\n\n    4\n\n\n12 (38%)\n\n\n    5\n\n\n5 (16%)\n\n\ncarb\n32\n\n\n\n\n    1\n\n\n7 (22%)\n\n\n    2\n\n\n10 (31%)\n\n\n    3\n\n\n3 (9.4%)\n\n\n    4\n\n\n10 (31%)\n\n\n    6\n\n\n1 (3.1%)\n\n\n    8\n\n\n1 (3.1%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\nNow we will examine only the 6 variables meeting the measurement assumption.\n\n\nLinear Relationship\n\npairs(\n  ~mpg + disp + hp + drat + wt + qsec,\n  data = cars_df,\n  main=\"Simple Scatterplot Matrix\")\n\n\n\n\n\n\n\n\nBased on this simple scatterplot matrix, all variables seem to follow a linear relationship, therefore, we will examine the normality and outliers assumption of these variables.\n\n\nNormality distribution and extreme outliers\n\n\nCode\npar(mfrow = c(2, 3))\n\nqqnorm(cars_df$mpg,\n       main = \"MPG Q-Q plot\",\n       ylab = \"Miles per Gallon\")\nqqline(cars_df$mpg)\n\nqqnorm(cars_df$disp, \n         main = \"Disp Q-Q plot\",\n         ylab = \"Displacement\")\nqqline(cars_df$disp)\n\nqqnorm(cars_df$hp, \n       main = \"hp Q-Q plot\",\n       ylab = \"Gross horsepower\")\nqqline(cars_df$hp)\n\nqqnorm(cars_df$drat, \n       main = \"drat Q-Q plot\",\n       ylab = \"Rear axle ratio\")\nqqline(cars_df$drat)\n\nqqnorm(cars_df$wt, \n       main = \"wt Q-Q plot\",\n       ylab = \"Weight\")\nqqline(cars_df$wt)\n\nqqnorm(cars_df$qsec,\n       main = \"qsec Q-Q plot\",\n       ylab = \"1/4 Mile Time\")\nqqline(cars_df$qsec)\n\n\n\n\n\n\n\n\n\nCode\n\n# A combination of ggqqplot and plot_annotation can also be used for the qq plot chart creation\n\n\n\n\nCode\npar(mfrow = c(2, 3))\n\nMPG_b &lt;- boxplot(cars_df$mpg,\n                 main = \"MPG boxplot\",\n                 ylab = \"Miles per Gallon\")\n\nDISP_b &lt;- boxplot(cars_df$disp,\n                  main = \"Disp boxplot\",\n                  ylab = \"Displacement\")\n\nHP_b &lt;- boxplot(cars_df$hp,\n                main = \"hp boxplot\",\n                ylab = \"Gross HP\")\n\nDRAT_b &lt;- boxplot(cars_df$drat,\n                  main = \"drat boxplot\",\n                  ylab = \"drat\")\n\nWT_b &lt;- boxplot(cars_df$wt,\n                main = \"wt boxplot\",\n                ylab = \"Weight\")\n\nqsec_b &lt;- boxplot(cars_df$qsec,\n       main = \"qsec boxplot\",\n       ylab = \"1/4 Mile Time\")\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 3))\n\nMPG_h &lt;- hist(cars_df$mpg,\n              main = \"MPG Histogram\",\n              xlab = \"Miles per Gallon\")\n\nDISP_h &lt;- hist(cars_df$disp,\n               main = \"disp Histogram\", \n               xlab = \"Displacement\")\n\nHP_h &lt;- hist(cars_df$hp,\n             main = \"hp Histogram\",\n             xlab = \"Gross HP\")\n\nDRAT_h &lt;- hist(cars_df$drat,\n               main = \"drat Histogram\",\n               xlab = \"drat\")\n\nWT_h &lt;- hist(cars_df$wt,\n             main = \"wt Histogram\",\n             xlab = \"Weight\")\n\nQSEC_h &lt;- hist(cars_df$qsec,\n          main = \"qsec Histogram\",\n          ylab = \"1/4 Mile Time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant procedures\n\n\n\n\nWe will run the correlation matrix using a “Pearson Correlation” for these data without resolving or reviewing some of the issues found with non-normality and extreme outliers for procedural and visual methods used to develop correlation matrices. However,\n\nNormality of variables: The qqplots, boxplots, and histograms visually hightlight that most of the variables have non-normality issues, for some of the variables, the issues with non-normality might not be too extreme for use in a pearson correlation. However, for three of the variables (displacement (disp), horse power (hp), and weight (wt)), the violation of non-normality might require one of the following strategies because non normally distributed data inflate the significance of “r” and outliers can have great influence on Pearson’ corrrelations as it might result in misleading results (like no association or very low association):\n\nStrategies to assist non normal distributions: Data transformations (such as log), Spearman rho correlation, Kendall tau’s correlation.\n\nPresence of outliers The qqplots, and boxplots visually hightlight that most of the outliers present should be further analyzed as these can have great influence on Pearson’ corrrelations because they might provide misleading results/associations (like no association or very low association):\n\nStrategies for outliers: Removal (if it is a confirmed measurement failure or a collection error), The use of Spearman correlation, adjustment/revisions."
  },
  {
    "objectID": "lessons_original/04_correlation.html#computation-of-a-correlation-matrix",
    "href": "lessons_original/04_correlation.html#computation-of-a-correlation-matrix",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Computation of a Correlation Matrix",
    "text": "Computation of a Correlation Matrix\nThe function in R to create a basic correlation matrix is cor(). This function uses “pearson” as the default coefficient/method. The other two methods (for ranked correlation matrices) are “kendall” or “spearman”.\nIf the data set has missing values, one can ask (use =) for “everything”, “all.obs”, “complete.obs”, “na.or.complete”, or “pairwise.complete.obs”. The easiest and most commonly used approach is “use = complete.obs” in which missing values are handled by case wise deletion (and if there are no complete cases, that gives an error).\nOur data set (mtcars2) is composed of only the 6 “continuous” variables of the data set ’mtcars”, and does not have missing values, therefore we can use the default method of “pearson” and do not need to specify a use for missing values.\n\ncars_cor &lt;- round(cor(mtcars2, method = \"pearson\"),2)\nkable(cars_cor,\n      caption = \"Basic correlation matrix\")\n\n\nBasic correlation matrix\n\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\n\nmpg\n1.00\n-0.85\n-0.78\n0.68\n-0.87\n0.42\n\n\ndisp\n-0.85\n1.00\n0.79\n-0.71\n0.89\n-0.43\n\n\nhp\n-0.78\n0.79\n1.00\n-0.45\n0.66\n-0.71\n\n\ndrat\n0.68\n-0.71\n-0.45\n1.00\n-0.71\n0.09\n\n\nwt\n-0.87\n0.89\n0.66\n-0.71\n1.00\n-0.17\n\n\nqsec\n0.42\n-0.43\n-0.71\n0.09\n-0.17\n1.00\n\n\n\n\n\nWe can also display the lower triangle of the pearson coefficient matrix by using the upper.tri() function.\n\nupper&lt;-cars_cor\nupper[upper.tri(cars_cor)]&lt;-\"\"\nupper&lt;-as.data.frame(upper)\nkable(upper,\n      caption = \"Lower triangle of matrix correlation\")\n\n\nLower triangle of matrix correlation\n\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\n\nmpg\n1\n\n\n\n\n\n\n\ndisp\n-0.85\n1\n\n\n\n\n\n\nhp\n-0.78\n0.79\n1\n\n\n\n\n\ndrat\n0.68\n-0.71\n-0.45\n1\n\n\n\n\nwt\n-0.87\n0.89\n0.66\n-0.71\n1\n\n\n\nqsec\n0.42\n-0.43\n-0.71\n0.09\n-0.17\n1\n\n\n\n\n\nAnother way (easier) to compute and visualize the lower triangle of a correlation matrix could be accomplished by using the function datasummary_correlation() in the modelsummary package.\n\n\ndatasummary_correlation(mtcars2)\n\n \n\n  \n    \n    \n    tinytable_nggij7exkkyfvmdlrqkd\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                mpg\n                disp\n                hp\n                drat\n                wt\n                qsec\n              \n        \n        \n        \n                \n                  mpg \n                  1   \n                  .   \n                  .   \n                  .   \n                  .   \n                  .\n                \n                \n                  disp\n                  -.85\n                  1   \n                  .   \n                  .   \n                  .   \n                  .\n                \n                \n                  hp  \n                  -.78\n                  .79 \n                  1   \n                  .   \n                  .   \n                  .\n                \n                \n                  drat\n                  .68 \n                  -.71\n                  -.45\n                  1   \n                  .   \n                  .\n                \n                \n                  wt  \n                  -.87\n                  .89 \n                  .66 \n                  -.71\n                  1   \n                  .\n                \n                \n                  qsec\n                  .42 \n                  -.43\n                  -.71\n                  .09 \n                  -.17\n                  1\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nHowever, this table is difficult to interpret as p-values for the pearson correlation coefficients are not included. The package Hmisc has functions available to provide p-values and to format the results of a correlation matrix. The function rcorr() provides the p-values of a correlation matrix. It is necessary to convert the data frame to a matrix to run this procedure:\n\ncars_cor_p &lt;- rcorr(as.matrix(mtcars2))\n\ncars_cor_p\n       mpg  disp    hp  drat    wt  qsec\nmpg   1.00 -0.85 -0.78  0.68 -0.87  0.42\ndisp -0.85  1.00  0.79 -0.71  0.89 -0.43\nhp   -0.78  0.79  1.00 -0.45  0.66 -0.71\ndrat  0.68 -0.71 -0.45  1.00 -0.71  0.09\nwt   -0.87  0.89  0.66 -0.71  1.00 -0.17\nqsec  0.42 -0.43 -0.71  0.09 -0.17  1.00\n\nn= 32 \n\n\nP\n     mpg    disp   hp     drat   wt     qsec  \nmpg         0.0000 0.0000 0.0000 0.0000 0.0171\ndisp 0.0000        0.0000 0.0000 0.0000 0.0131\nhp   0.0000 0.0000        0.0100 0.0000 0.0000\ndrat 0.0000 0.0000 0.0100        0.0000 0.6196\nwt   0.0000 0.0000 0.0000 0.0000        0.3389\nqsec 0.0171 0.0131 0.0000 0.6196 0.3389       \n\nAnother way in which a correlation matrix could be constructed displaying p-values is with the function corr.test in the psych package. The default alpha = 0.05 and the default method is “pearson”. Both could be changed to for instance, alpha = 0.01 and method = “spearman”. However, the R documentation for this function indicates that non parametric method calculation is slow.\n\n\nCode\nmtcars2_p &lt;- corr.test(mtcars2)$p\n\nkable(mtcars2_p)\n\n\n\n\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\n\nmpg\n0.000\n0.000\n0.00\n0.00\n0.000\n0.053\n\n\ndisp\n0.000\n0.000\n0.00\n0.00\n0.000\n0.053\n\n\nhp\n0.000\n0.000\n0.00\n0.05\n0.000\n0.000\n\n\ndrat\n0.000\n0.000\n0.01\n0.00\n0.000\n0.678\n\n\nwt\n0.000\n0.000\n0.00\n0.00\n0.000\n0.678\n\n\nqsec\n0.017\n0.013\n0.00\n0.62\n0.339\n0.000\n\n\n\n\n\nWe can use the package `rstatix to: 1. Create a pearson coefficient correlation matrix with the function cor_mat() 2. Create a matrix of p-values (95% significance is default) with the function cor_pmat(), but p-values are not adjusted or rounded so it is not the best display 3. Create a visual representation of the matrix\n\n# Correlation matrix between all variables\nmtcars2_corr &lt;- mtcars2 %&gt;% \n  cor_mat()\nmtcars2_corr\n# A tibble: 6 × 7\n  rowname   mpg  disp    hp   drat    wt   qsec\n* &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 mpg      1    -0.85 -0.78  0.68  -0.87  0.42 \n2 disp    -0.85  1     0.79 -0.71   0.89 -0.43 \n3 hp      -0.78  0.79  1    -0.45   0.66 -0.71 \n4 drat     0.68 -0.71 -0.45  1     -0.71  0.091\n5 wt      -0.87  0.89  0.66 -0.71   1    -0.17 \n6 qsec     0.42 -0.43 -0.71  0.091 -0.17  1    \n\nmtcars2_corr_p &lt;- mtcars2 %&gt;% \n  cor_pmat()\nmtcars2_corr_p\n# A tibble: 6 × 7\n  rowname      mpg     disp           hp       drat        wt       qsec\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 mpg     0        9.38e-10 0.000000179  0.0000178  1.29e- 10 0.0171    \n2 disp    9.38e-10 0        0.0000000714 0.00000528 1.22e- 11 0.0131    \n3 hp      1.79e- 7 7.14e- 8 0            0.00999    4.15e-  5 0.00000577\n4 drat    1.78e- 5 5.28e- 6 0.00999      0          4.78e-  6 0.62      \n5 wt      1.29e-10 1.22e-11 0.0000415    0.00000478 2.27e-236 0.339     \n6 qsec    1.71e- 2 1.31e- 2 0.00000577   0.62       3.39e-  1 0         \n\nmtcars2_corr %&gt;%\n  #cor_get_pval() %&gt;%\n  cor_reorder() %&gt;%\n  pull_lower_triangle %&gt;%\n  cor_plot(label = TRUE)\n\n\n\n\n\n\n\n\nThe rquery.cormat in the corrplot package also provides the results of a correlation matrix showing r, p-values, and a visual representation. It can also provide formated results.\n\n\n\n\n\n\n\n\nThe package corrplot allows us to visually format the correlation matrix so that it can be a full matrix or the bottom or top triangle. Additionally, it can display the correlation coefficients, or another with p-values, or a visual representation of significant values.\n\n\ntestmtcars2 = cor.mtest(mtcars2, conf.level = 0.95)\n\n\ncorrplot(cars_cor,\n         type = 'full',\n         method = 'circle',\n         addCoef.col = 'black',\n         )\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\ncorrplot(cars_cor,\n         type = 'lower',\n         p.mat = testmtcars2$p,\n         number.font = 1,\n         tl.cex = 0.9,\n         pch.cex = 2,\n         insig = 'p-value', \n         sig.level = -1\n         )\ncorrplot(cars_cor,\n         type = 'upper',\n         tl.pos = 'n',\n         p.mat = testmtcars2$p,\n         method = 'color',\n         diag = FALSE, \n         sig.level = c(0.001, 0.01, 0.05),          \n         pch.cex = 0.9,\n         insig = 'label_sig',\n         pch.col = 'grey20'\n)\n\n\n\n\n\n\n\n\nUsing the add = TRUE call inside the upper triangle corrplot function, one could stack matrices, but this method is complicated and requires adjusting sizes and labeling. To create stack full matrices it is best to use the function corrplot.mixed also in the corrplot package. Sadly, some of the features of corrplot are not available, but you can still have very informative visual outputs.\n\n\nCode\ncorrplot.mixed(cars_cor)\n\n\n\n\n\n\n\n\n\nThe following matrix visualization using the function chart.Correlation in the package PerformanceAnalytics provides many of the assumption tests for a correlation. The top triangle is the (absolute) value of the correlation coefficient (the default method is “pearson” but it could be changed to “kendall” or “spearman”) plus the result of the cor.test as stars (this can be changed to other symbols). In the bottom triangle of the matrix are the bivariate scatterplots, with a fitted line. This is a quick way to visualize a correlation matrix coefficient and significant relationships while examining assumptions. A must to run this function is to use the data set developed with the six continuous variables of interest (mtcars2).\n\nchart.Correlation(mtcars2,\n                  histogram = TRUE,\n                  pch = 19)\n\n\n\n\n\n\n\n\n*  The same visualization could be built using the **pairs()** function in base R, but **pairs()** is not very user friendly to construct a correlation matrix.\n*  In addition, the function **pair.panel** in the `psych` package returns something similar. \n    *  Of interest about this function is that it also allows for regression visualization as regression lines and coinfidence intervals could be part of the output.\n\n\n\n\n\n\nNote\n\n\n\n\nA simple correlation matrix could be ran using the full data set and then select the variables significantly correlated and then see if any of them meet the assumptions.\nThe cpairs() in the gclus package allows for a plot matrix while ordering and coloring the subplots by correlation."
  },
  {
    "objectID": "lessons_original/04_correlation.html#spearman-ranking-of-variables-non-parametric",
    "href": "lessons_original/04_correlation.html#spearman-ranking-of-variables-non-parametric",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Spearman (ranking of variables, non-parametric)",
    "text": "Spearman (ranking of variables, non-parametric)\n\n# Load data\ndata(\"mtcars\")\ncars_partial &lt;- mtcars\ncars_spearman &lt;- cor.test(mtcars$cyl, mtcars$carb, method = \"spearman\")\n\n# This is simply an example of how a spearman procedure is done. Although we can \n# use continuous variables, the exact p-value cannot be computed with ties. \n# Therefore, it is better to use ranked data in order to compute the exact p-value."
  },
  {
    "objectID": "lessons_original/04_correlation.html#partial-correlation",
    "href": "lessons_original/04_correlation.html#partial-correlation",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Partial Correlation",
    "text": "Partial Correlation\nThe Partial correlation is the correlation of two variables while controlling for a third or more other variables. The partial correlation coefficient is a measure of the strength of the linear relationship between two variables after entirely controlling for the effects of other variables.\nTo conduct the partial correlation test\n\n# partial correlation between \"mpg\" and \"wt\"  given \"am\"\npcor.test(cars_partial$mpg,cars_partial$wt,cars_partial$am)\n  estimate  p.value statistic  n gp  Method\n1   -0.784 1.87e-07     -6.79 32  1 pearson\n\nThis is the calculation of the correlation between miles per gallon and weight while controlling the variable “am” = Transmission (0 = automatic, 1 = manual)."
  },
  {
    "objectID": "lessons_original/04_correlation.html#hypothesis-test-between-mpg-and-wt",
    "href": "lessons_original/04_correlation.html#hypothesis-test-between-mpg-and-wt",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Hypothesis test between mpg and wt",
    "text": "Hypothesis test between mpg and wt\nWe will test if a vehicle’s weight is related to its fuel efficiency.\n\\[\nH_0: ρ = 0,\n\\] \\[\nH_1: ρ ≠ 0\n\\]\n\nggscatter(cars_df, x = \"mpg\", y = \"wt\", \n          add = \"reg.line\",\n          conf.int = TRUE, \n          cor.coef = TRUE, \n          cor.method = \"pearson\",\n          xlab = \"Miles/(US) gallon\",\n          ylab = \"Weight (1000 lbs)\"\n          )\n\n\n\n\n\n\n\n\nWe can then conduct a Pearson correlation test using the cor.test function.\n\ncars_ct &lt;- cor.test(cars_df$wt, cars_df$mpg, \n                    method = \"pearson\")\ncars_ct\n\n    Pearson's product-moment correlation\n\ndata:  cars_df$wt and cars_df$mpg\nt = -10, df = 30, p-value = 1e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.934 -0.744\nsample estimates:\n   cor \n-0.868 \n\nTo test the Hypothesis:\nThe test statistic t is rounded to -10 and the corresponding p-value is 1e-10.\nSince the p-value is less than .05, we have sufficient evidence to say that the correlation between a vehicle’s weight and miles per gallon is statistically significant.\nWe can also test the hypothesis manually:\nUsing a t-test:\nn(ordered pairs) = 32 r(sample correlation coefficient) = -0.868 alpha = 0.05\n\\[\nt = \\frac {r}{\\sqrt{\\frac {1-r^2}{n-2}}}\n\\]\n\\[\nt = \\frac {-0.868}{\\sqrt{\\frac {0.2466}{30}}} = -9.574\n\\]\nThe critical values \\(-t_O = - 2.0423\\) and \\(t_o = 2.0423\\) form the rejection regions for the standardized test statistic. Since t = -9.574 and it is less than \\(-t_O = - 2.0423\\), we reject the null hypothesis and we can conclude that there is enough evidence at the 95% level of significance that there is a significant linear correlation between fuel efficiency and a vehicle’s weight.\nAlternatively, we can use a table of critical values for pearson’s r. In this two-tailed example, with df = 30 and alpha = .05 the critical value is .349, then we can make a decision, |r| = 0.868 is greater than 0.349, which means that we reject the null hypothesis and conclude that there is a relationship between vehicle weight and miles per gallon, r(30) = -0.868, p&lt;0.05\nFurthermore,\n\n\nThe direction of the relationship is negative (i.e., heavier vehicles have lower numbers of miles per gallon or are less fuel efficient), meaning that when one variable increases the other variable decreases.\nThe magnitude, or strength, of the association is approximately strong (5 &lt; | r |)."
  },
  {
    "objectID": "lessons_original/04_correlation.html#equations-properties-and-uses",
    "href": "lessons_original/04_correlation.html#equations-properties-and-uses",
    "title": "Correlation Matrix, Covariance, and OLS Regression",
    "section": "Equations, properties, and uses",
    "text": "Equations, properties, and uses\nMathematical equations, properties, and uses\nRemember the equation for the variance for one variable\n\\[\nvar(x) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})^2}}\n\\]\nCovariance: Relation between two variables where, (x_i are the values of the X-variable), (y_i* are the values of the Y-variable), (x_bar is the mean of the X-variable), (y_bar* is the mean of the Y-variable), and (n are the number of data points)\n\\[\nCov(x,y) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}\n\\]\n\nCovariance Properties\n\n(x, y) &gt; 0: the covariance for both variables is positive or moving in the same direction.\n(x, y) &lt; 0: the covariance for both variables is negative or moving in the opposite direction.\n(x, y) = 0: there is no relationship between the variables.\n\nCovariance Matrix: Covariance between variables in a data set\n\\[\nC = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(X_i - \\bar{X})(X_i-\\bar{X})}^T}\n\\]\nThe general formula to represent a covariance matrix\n\\[\n\\begin{bmatrix} Var(x_{1}) & ... & Cov(x_{1},x_{n})\\\\ : &. & :\\\\ :& \\: \\: \\: \\: \\: \\: \\: \\: \\: \\: .& :\\\\ Cov(x_{n},x_{1}) & ... & Var(x_{n}) \\end{bmatrix}.\n\\]\nUses of a covariance matrix\n\nCalculate the Mahalanobis distance.,\nKalman filters.,\nGaussian mixture models.\nPCA (principal component analysis)\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is of importance and allows us to see the relationship between covariance and correlation in this simplified equation for the Pearson Correlation Coefficient, r. [video explaining standarization and mathematical properties of r]: tilestats2021?\n\n\n\\[\nr = \\frac {Cov(x,y)}{(\\sqrt{var(x)})(\\sqrt{var(y)})}\n\\]\n\n\nCorrelation Matrix\n\n\n\nSample correlation matrix\n\n\nCorrelation Properties: direction\n\nr = 1: A perfect positive correlation.\nr = 0: Zero or no correlation.\nr = -1: A perfect negative relationship. where,\n\nThe sign of the correlation coefficient indicates the direction of the association.\n\n\nCorrelation Properties: strength\n\n.1 &lt; | r | &lt; .3 (small / weak correlation).\n.3 &lt; | r | &lt; .5 (medium / moderate correlation).\n.5 &lt; | r | (large / strong correlation).\n\nThe magnitude of the correlation coefficient indicates the strength of the association.\n\n\nCorrelation uses\n\nTo summarize the correlation among various variables of a data set.\nTo perform regression testing (like multicollinarity)\nBasis of other analyses (like regression models)\n\nCorrelation, r and Covariance\n\n\n\nVisual contrast\n\n\nWe will concentrate on the Correlation Matrix which provides the Pearson Correlation Coefficient or (its variations for non-parametric data such as, spearman and kendall)."
  },
  {
    "objectID": "lessons_original/04_regression_ols.html",
    "href": "lessons_original/04_regression_ols.html",
    "title": "Ordinary Least Squares Regression",
    "section": "",
    "text": "Code\n#install.packages(\"skimr\")\nlibrary(skimr)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(ggplot2)\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\n#install.packages(\"patchwork\")\nlibrary(patchwork)\nlibrary(tidyverse)\n#install.packages(\"gtsummary\")\nlibrary(gtsummary)\n#StandWithUkraine\nlibrary(Hmisc)\nlibrary(corrplot)\ncorrplot 0.92 loaded\nlibrary(xtable)\n#install.packages(\"modelsummary\")\nlibrary(modelsummary)\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n\nChange the default backend persistently:\n\n  config_modelsummary(factory_default = 'gt')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\nlibrary(psych)\n#install.packages(\"rstatix\")\nlibrary(rstatix)\n#install.packages(\"PerformanceAnalytics\")\nlibrary(PerformanceAnalytics)\nLoading required package: xts\nLoading required package: zoo\n\nAttaching package: 'zoo'\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\nAttaching package: 'xts'\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n#install.packages(\"ppcor\")\nlibrary(ppcor)\nLoading required package: MASS\n\nAttaching package: 'MASS'\nThe following object is masked from 'package:rstatix':\n\n    select\nThe following object is masked from 'package:gtsummary':\n\n    select\nThe following object is masked from 'package:patchwork':\n\n    area\nThe following object is masked from 'package:dplyr':\n\n    select\n#install.packages(\"remotes\")\nlibrary(remotes)\n#install.packages(\"conflicted\")\nlibrary(conflicted)"
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#how-to-develop-the-best-fitting-line",
    "href": "lessons_original/04_regression_ols.html#how-to-develop-the-best-fitting-line",
    "title": "Ordinary Least Squares Regression",
    "section": "How to develop the best fitting line?",
    "text": "How to develop the best fitting line?\n\n\n\nSample residual/error terms plot\n\n\nThe best fitting line is one that minimizes errors in prediction or one with the Minimum sum of squared residuals (SSR). For more details, you can watch this video:khanacademy2018?\n\\[\nSSR = \\sum_{i = 1}^{n}{(y_i - \\hat{y_i})^2}\n\\]\n\\(residual_i = y_i - \\hat{y_i}\\)\nIt is important to note that prior to calculating the residuals, we must visualize and examine the data, which was done in the previous example. Then, we must run the regression line. We can utilize lm() to perform the OLS regression which will provide us with the model summary, including the following:\n\nPr(&gt;|t|) Multiple R-Squared Adjusted R-Squared Residual Standard Error F-statistic P-value\n\nOnce the model summary is given, we can then move on to creating the residual plots. When performing this step, we have to check the assumptions of homoscedasticity and normality.\n* Residuals = error terms\n* $$ Residual = observed value - predicted value $$\n* The larger the error term in absolute value, the worse the prediction\n* Squaring residuals solve issues arising from some residuals being negative and some positive.\n\nAssumptions\n\nLinearity: Linear relationship between the dependent variable and the independent variables.\nIndependence: The observations must be independent of each other.\nHomoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\nNormality: The residuals / errors should be normally distributed.\nNo multicollinearity: In the case of multiple regression (2+ independent variables), the independent variables should not be highly correlated with each other.\n\n\n\n\n\n\n\n\nBe careful about outliers\n\n\n\nOutliers can influence the estimates of the relationship."
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#example-of-an-ols-regression-in-r",
    "href": "lessons_original/04_regression_ols.html#example-of-an-ols-regression-in-r",
    "title": "Ordinary Least Squares Regression",
    "section": "Example of an OLS regression in R",
    "text": "Example of an OLS regression in R\nIn R, the lm function command allows us to develop an OLS regression.\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(MPGReg)\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.543 -2.365 -0.125  1.410  6.873 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   37.285      1.878   19.86  &lt; 2e-16 ***\nwt            -5.344      0.559   -9.56  1.3e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.05 on 30 degrees of freedom\nMultiple R-squared:  0.753, Adjusted R-squared:  0.745 \nF-statistic: 91.4 on 1 and 30 DF,  p-value: 1.29e-10\n\nThe OLS regression model is then:\n\\(\\widehat{MPG_i} = 37.285 - 5.344*WT_i\\)\n\nInterpretation: \\(\\beta_0\\)\n\nThe model predicts that vehicles with no weight will have 37.285 miles per gallon, on average.\nThis is not a very meaningful intercept as vehicles with “0” weight do not exist. A meaningful intercept can be created by subtracting a constant from the x variable to move the intercept.In R as part of the lm command, this can be done by surrounding the independent variable with I() which applies the function inside and treats it as a new variable. For our example we used the rounded lowest weight of the data (1.5) to predict miles per gallon.\n\nThis procedure does not change the slope of the line\n\n\n\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg2 &lt;- lm(mpg ~ I(wt-1.5), data = mtcars)\nsummary(MPGReg2)\n\nCall:\nlm(formula = mpg ~ I(wt - 1.5), data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.543 -2.365 -0.125  1.410  6.873 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   29.268      1.101   26.59  &lt; 2e-16 ***\nI(wt - 1.5)   -5.344      0.559   -9.56  1.3e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.05 on 30 degrees of freedom\nMultiple R-squared:  0.753, Adjusted R-squared:  0.745 \nF-statistic: 91.4 on 1 and 30 DF,  p-value: 1.29e-10\n\n\nThen, the meaningful intercept model predicts that vehicles with a weight of 1500 pounds have 29.268 miles per gallon, on average.\nInterpretation: \\(\\beta_1\\)\n\nThe model predicts that on average, an increase of 1,000 pounds in the weight of a vehicle is associated with a decrease of 5.344 miles per gallon.\n\n\n\n# define residuals \nres &lt;- resid(MPGReg)\n\n# produce residual vs. fitted plot \nplot(fitted(MPGReg), res)\n\n# add a horizontal line at 0\nabline(0,0)\n\n\n\n\n\n\n\n\n# create Q-Q- plot for residuals\nqqnorm(res)\n\n# add a straight diagonal line to the plot\nqqline(res)\n\n\n\n\n\n\n\n\nBased on the graph above, it is visually clear that normality may not be met due to some outliers. This means that we must explore our data even deeper as it is possible that transformation of our data utilizing one of the following methods must take place:\n\nLog transformation Square Root Transformation Cube Root Transformation\n\nOnce the data is transformed, we can run the residual plot over again in order to achieve normality. For the sake of this presentation, we are only using an example with known limitations such as non-normality."
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#hypothesis-testing-in-ols-regression",
    "href": "lessons_original/04_regression_ols.html#hypothesis-testing-in-ols-regression",
    "title": "Ordinary Least Squares Regression",
    "section": "Hypothesis testing in OLS regression",
    "text": "Hypothesis testing in OLS regression\nThe null hypothesis in this case would be that the slope is zero indicating no relationship between x and y. Or in our example, we can state that there is no relationship between a vehicle’s weight and its miles per gallon. The alternative hypothesis is then that the slope is not zero.\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_1: \\beta_1 ≠ 0\n\\]\nWe can test this hypothesis by using the lm summary printout which provides the p-value for the wt coefficient. This indicates that there is indeed a significant relationship between the weight of the car and its efficiency (miles per gallon used). R provides a t-value for the ‘wt’ coefficient which has a p-value of p &lt; 0.000 as seen below:\n                  Estimate        Std. Error       t value      Pr(&gt;|t|)   \n    wt           -5.3445             0.5591       -9.559       1.29e-10"
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#r-squared-value",
    "href": "lessons_original/04_regression_ols.html#r-squared-value",
    "title": "Ordinary Least Squares Regression",
    "section": "R-Squared Value",
    "text": "R-Squared Value\nR-squared is “a measure of how much of the variation in the dependent variable is explained by the independent variables in the model. It ranges from 0 to 1, with higher values indicating a better fit”ordinary?.\nAdjusted R-squared is “similar to R-squared, but it takes into account the number of independent variables in the model. It is a more conservative estimate of the model’s fit, as it penalizes the addition of variables that do not improve the model’s performance”ordinary?."
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#f-statistic",
    "href": "lessons_original/04_regression_ols.html#f-statistic",
    "title": "Ordinary Least Squares Regression",
    "section": "F-Statistic",
    "text": "F-Statistic\nThe F-statistic “tests the overall significance of the model by comparing the variation in the dependent variable explained by the model to the variation not explained by the model. A large F-statistic indicates that the model as a whole is significant”interpre?."
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#visual-representation",
    "href": "lessons_original/04_regression_ols.html#visual-representation",
    "title": "Ordinary Least Squares Regression",
    "section": "Visual representation",
    "text": "Visual representation\nThe visual representation of this model using ggplot is the following:\n\nggplot(mtcars, aes(x = wt, y = mpg))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+ #se is option for coinfidence bar\n  labs(x= \"Weight (per 1,000 pounds)\",\n       y = \"Miles per gallon\")+\n  theme_bw()\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs shown in the figure above, we see the summary statistics represented in a visual manner with the line of best fit. As indicated previously, we see a steep negative correlation between weight of the car and the miles per gallon (efficiency) utilized."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html",
    "href": "lessons_original/04_regression_polynomial.html",
    "title": "Introduction to Polynomial Regression",
    "section": "",
    "text": "Polynomial regression is a type of regression analysis that models the non-linear relationship between the predictor variable(s) and response variable1. It is an extension of simple linear regression that allows for more complex relationships between predictor and response variables1.\nIn simple terms, it allows us to fit a curve to our data instead of a straight line.\n\n\nPolynomial regression is useful when the relationship between the independent and dependent variables is nonlinear.\nIt can capture more complex relationships than linear regression, making it suitable for cases where the data exhibits curvature.\n\n\n\n\nLinearity: There is a curvilinear relationship between the independent variable(s) and the dependent variable.\nIndependence: The predictor variables are independent of each other.\nHomoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s).\nNormality: The errors should be normally distributed with mean zero and a constant variance.\n\n\n\n\nConsider independent samples \\(i = 1, \\ldots, n\\). The general formula for a polynomial regression representing the relationship between the response variable (\\(y\\)) and the predictor variable (\\(x\\)) as a polynomial function of degree \\(d\\) is:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... \\beta_dx_i^d + \\epsilon_i,\n\\]\nwhere:\n\n\\(y_i\\) represents the response variable,\n\\(x_i\\) represents the predictor variable,\n\\(\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_d\\) are the coefficients to be estimated, and\n\\(\\epsilon_i\\) represents the errors.\n\nFor large degree \\(d\\), polynomial regression allows us to produce an extremely non-linear curve. Therefore, it is not common to use \\(d &gt; 3\\) because the larger value of \\(d\\), the more overly flexible polynomial curve becomes, which can lead to overfitting them model to the data.\nThe coefficients in polynomial function can be estimated using least square linear regression because it can be viewed as a standard linear model with predictors \\(x_i, \\,x_i^2, \\,x_i^3, ..., x_i^d\\). Hence, polynomial regression is also known as polynomial linear regression.\n\n\n\n\nStep 0: Load required packages\nStep 1: Load and inspect the data\nStep 2: Visualize the data\nStep 3: Fit the model\nStep 4: Assess Assumptions\nStep 5: Describe model output"
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#when-is-a-polynomial-regression-used",
    "href": "lessons_original/04_regression_polynomial.html#when-is-a-polynomial-regression-used",
    "title": "Introduction to Polynomial Regression",
    "section": "",
    "text": "Polynomial regression is useful when the relationship between the independent and dependent variables is nonlinear.\nIt can capture more complex relationships than linear regression, making it suitable for cases where the data exhibits curvature."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#assumptions-of-polynomial-regression",
    "href": "lessons_original/04_regression_polynomial.html#assumptions-of-polynomial-regression",
    "title": "Introduction to Polynomial Regression",
    "section": "",
    "text": "Linearity: There is a curvilinear relationship between the independent variable(s) and the dependent variable.\nIndependence: The predictor variables are independent of each other.\nHomoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s).\nNormality: The errors should be normally distributed with mean zero and a constant variance."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#mathematical-equation",
    "href": "lessons_original/04_regression_polynomial.html#mathematical-equation",
    "title": "Introduction to Polynomial Regression",
    "section": "",
    "text": "Consider independent samples \\(i = 1, \\ldots, n\\). The general formula for a polynomial regression representing the relationship between the response variable (\\(y\\)) and the predictor variable (\\(x\\)) as a polynomial function of degree \\(d\\) is:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... \\beta_dx_i^d + \\epsilon_i,\n\\]\nwhere:\n\n\\(y_i\\) represents the response variable,\n\\(x_i\\) represents the predictor variable,\n\\(\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_d\\) are the coefficients to be estimated, and\n\\(\\epsilon_i\\) represents the errors.\n\nFor large degree \\(d\\), polynomial regression allows us to produce an extremely non-linear curve. Therefore, it is not common to use \\(d &gt; 3\\) because the larger value of \\(d\\), the more overly flexible polynomial curve becomes, which can lead to overfitting them model to the data.\nThe coefficients in polynomial function can be estimated using least square linear regression because it can be viewed as a standard linear model with predictors \\(x_i, \\,x_i^2, \\,x_i^3, ..., x_i^d\\). Hence, polynomial regression is also known as polynomial linear regression."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#performing-a-polynomial-regression-in-r",
    "href": "lessons_original/04_regression_polynomial.html#performing-a-polynomial-regression-in-r",
    "title": "Introduction to Polynomial Regression",
    "section": "",
    "text": "Step 0: Load required packages\nStep 1: Load and inspect the data\nStep 2: Visualize the data\nStep 3: Fit the model\nStep 4: Assess Assumptions\nStep 5: Describe model output"
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#hypotheses",
    "href": "lessons_original/04_regression_polynomial.html#hypotheses",
    "title": "Introduction to Polynomial Regression",
    "section": "2.1 Hypotheses",
    "text": "2.1 Hypotheses\nFor this example, we are investigating the following:\n\nResearch Question: Is there a significant quadratic relationship between the weight of a car (wt) and its miles per gallon (mpg) in the mtcars dataset?\nNull hypothesis (\\(H_0\\)): There is no significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\nAlternative hypothesis (\\(H_A\\)): There is a significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nIn this case, the null hypothesis assumes that the coefficients of the quadratic polynomial terms are zero, indicating no relationship between the weight of the car and miles per gallon. The alternative hypothesis, on the other hand, suggests that at least one of the quadratic polynomial terms is non-zero, indicating a significant relationship between the weight of the car and miles per gallon.\nBy performing the polynomial regression analysis and examining the model summary and coefficients, we can evaluate the statistical significance of the relationship and determine whether to reject or fail to reject the null hypothesis."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#step-0-install-and-load-required-package",
    "href": "lessons_original/04_regression_polynomial.html#step-0-install-and-load-required-package",
    "title": "Introduction to Polynomial Regression",
    "section": "2.2 Step 0: Install and load required package",
    "text": "2.2 Step 0: Install and load required package\nIn R, we’ll use the lm() function from the base package to perform polynomial regression. Also, since we want to visualize our data, we will be loading the ggplot2 package for use.\n\n# For data visualization purposes\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)"
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#step-1-load-and-inspect-the-data",
    "href": "lessons_original/04_regression_polynomial.html#step-1-load-and-inspect-the-data",
    "title": "Introduction to Polynomial Regression",
    "section": "2.3 Step 1: Load and inspect the data",
    "text": "2.3 Step 1: Load and inspect the data\nFor this example, we will use the built-in mtcars dataset (from the standard R package datasets) which is publicly available and contains information about various car models.\n\n# Load mtcars dataset\ndata(mtcars)\n\n\n# Print the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#step-2-visualize-the-data",
    "href": "lessons_original/04_regression_polynomial.html#step-2-visualize-the-data",
    "title": "Introduction to Polynomial Regression",
    "section": "2.4 Step 2: Visualize the data",
    "text": "2.4 Step 2: Visualize the data\nBefore fitting a polynomial regression model, it’s helpful to visualize the data to identify any non-linear patterns. For our example, we will use a scatter plot to visualize the relationship between the independent and dependent variables:\n\n# Scatter plot of mpg (dependent variable) vs. wt (independent variable)\nggplot(mtcars) +\n  theme_minimal() +\n  aes(x = wt, y = mpg) + \n  labs(x = \"Weight (lbs/1000)\", y = \"Miles per Gallon\") +\n  geom_point()"
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#step-3-fit-models",
    "href": "lessons_original/04_regression_polynomial.html#step-3-fit-models",
    "title": "Introduction to Polynomial Regression",
    "section": "2.5 Step 3: Fit Models",
    "text": "2.5 Step 3: Fit Models\nLet’s create a function so we can build multiple models. We will build a standard linear model and a quadratic model (degrees 1 and 2, respectively).\n\n# Function to fit and evaluate polynomial regression models\nfit_poly_regression &lt;- function(degree) {\n  formula &lt;- as.formula(paste(\"mpg ~ poly(wt, \", degree, \")\"))\n  lm(formula, data = mtcars)\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1 &lt;- fit_poly_regression(1)\nmodel_2 &lt;- fit_poly_regression(2)\n\nTo fit a polynomial regression model, we’ll use the lm() function and create polynomial terms using the poly() function. In this example, we’ll fit a standard linear (degree = 1) and a quadratic polynomial (degree = 2) to the mtcars dataset."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#step-4-assess-assumptions",
    "href": "lessons_original/04_regression_polynomial.html#step-4-assess-assumptions",
    "title": "Introduction to Polynomial Regression",
    "section": "2.6 Step 4: Assess Assumptions",
    "text": "2.6 Step 4: Assess Assumptions\nBefore we can interpret the model, we have to check the assumptions. We will check these assumptions via plots:\n\nResiduals vs. Fitted values (used to check the linearity assumption),\na Q-Q plot of the Residuals (used to check the normality of the residuals),\na Scale-Location plot (used to check for heteroskedasticity), and\nResiduals vs. Leverage values (identifies overly influential values, if any exist).\n\n\npar(mfrow = c(2, 2))\nplot(model_1, which = c(1, 2, 3, 5))\n\n\n\n\n\n\n\nplot(model_2, which = c(1, 2, 3, 5))\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nWe see that Model 2 (the quadratic one) satisfies the “linearity” assumption, because the red line in the “Residuals vs Fitted” graph is flat. However, the Q-Q plot shows that the residuals are not normally distributed, so we should take additional steps to transform the response feature (such as via a square root or log transformation, or something similar)."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#step-5.-describe-model-output",
    "href": "lessons_original/04_regression_polynomial.html#step-5.-describe-model-output",
    "title": "Introduction to Polynomial Regression",
    "section": "2.7 Step 5. Describe Model Output",
    "text": "2.7 Step 5. Describe Model Output\nAlthough we recognize that this model is not correct (because the residuals are not approximately normal), we will give an example of how to interpret this output.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.483 -1.998 -0.773  1.462  6.238 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.0906     0.4686  42.877  &lt; 2e-16 ***\npoly(wt, 2)1 -29.1157     2.6506 -10.985 7.52e-12 ***\npoly(wt, 2)2   8.6358     2.6506   3.258  0.00286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 29 degrees of freedom\nMultiple R-squared:  0.8191,    Adjusted R-squared:  0.8066 \nF-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11"
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#bonus-step-visualize-the-final-model",
    "href": "lessons_original/04_regression_polynomial.html#bonus-step-visualize-the-final-model",
    "title": "Introduction to Polynomial Regression",
    "section": "2.8 Bonus Step: Visualize the Final Model",
    "text": "2.8 Bonus Step: Visualize the Final Model\nFinally, let’s plot the scatter plot with the polynomial regression line to visualize the fit:\n\n# Create a data frame with data points and predictions \nplot_data &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg = mtcars$mpg, \n  mpg_predicted = predict(model_2, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data) +\n  theme_minimal() + \n  aes(x = wt, y = mpg) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line\",\n    x = \"Weight (wt)\",\n    y = \"Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#references",
    "href": "lessons_original/04_regression_polynomial.html#references",
    "title": "Introduction to Polynomial Regression",
    "section": "3.1 References",
    "text": "3.1 References\n\nField, A. (2013). Discovering Statistics Using IBM SPSS Statistics. (4th ed.). Sage Publications.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. (2nd ed.). Publisher. (pp. 290-300)"
  }
]