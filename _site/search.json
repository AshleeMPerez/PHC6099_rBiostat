[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHC 6099: R Computing for Health Sciences",
    "section": "",
    "text": "These are the written lecture materials for the class PHC 6099 at Florida International University’s Stempel College of Public Health. This is the second semester of the “R” course sequence (the first semester is PHC6701; the text for that class is available here: https://gabrielodom.github.io/PHC6701_r4ds/) The source code and data sets for this book are available here: https://github.com/gabrielodom/PHC6099_rBiostat."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lessons_original/01_ggplot2.html",
    "href": "lessons_original/01_ggplot2.html",
    "title": "How to create a scatterplot",
    "section": "",
    "text": "Scatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#introduction",
    "href": "lessons_original/01_ggplot2.html#introduction",
    "title": "How to create a scatterplot",
    "section": "",
    "text": "Scatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#data",
    "href": "lessons_original/01_ggplot2.html#data",
    "title": "How to create a scatterplot",
    "section": "Data",
    "text": "Data\nData was obtained from GapMinder for each health and non-health indicator and combined into one data set. The data includes information on over 180 countries and territories from the years 1800 to 2099. Countries and territories with missing information were not excluded from the data set as the lack of information can also be looked into and shed light on why data was not collected or provided.\nTo determine whether a country’s health and income outcomes are influenced by population sizes and GDP per capita, the data will be used to create a series of graphs to view different trends. It is important to note certain analysis’ will only be done on specific countries and on certain years. Predictive values were provided up until 2099 however, we will focus on years with full and current data.\n\n\nCode\n# Contains colour palette for ggplot\nlibrary(viridis)\n\n# Contains \"gganimate\"\nlibrary(ggplot2)\nlibrary(gganimate)\n\nlibrary(tidyverse)\n\n\n# Reads csv file\ngapminder_data &lt;-  \n  # read_csv(\"clean_data/gapminder_scatterplot.csv\")\n  read_csv(\"../data/gapminder_2024spring.csv\")"
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#how-to-create-a-scatter-plot",
    "href": "lessons_original/01_ggplot2.html#how-to-create-a-scatter-plot",
    "title": "How to create a scatterplot",
    "section": "How to create a scatter-plot",
    "text": "How to create a scatter-plot\n\nIntro to ggplot2\nGgplot2 is a package used to create graphs and visualize data. The main three components of ggplot2 are the data, aesthetics and geom layers.\n\nThe data layer - states what data will be used to graph\nThe aesthetics layer - specifies the variables that are being mapped\nThe geom layer - specifies the type of graph to be produced\n\n\n\n\n\n\n\n\nBasic scatter-plot using ggplot2\nIn order to create a scatter-plot using ggplot, you must specify what data you will be using, state which variables will be mapped and how under aesthetics. What differentiates the scatter-plot from any other type of graph will be specified under the geom layer. For the scatter-plot, geom_point will be used.\nIn this example, we will analyze the relationship between fertility rates and gdp per capita for each country in 2011.\n\nfig_bubble_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(x = gdp_per_capita, y = fertility) +\n  geom_point()\n\nfig_bubble_2011 \n\n\n\n\n\n\n\n\n\n\nElevating your scatter-plot\nIn the example above, we have mapped out fertility as our y-axis and gdp per capita as our x-axis. However, at it’s very basic level, there is not enough information provided to accurately analyze the relationship between the two. For this reason, we can add additional layers that will provide more information to properly analyze the scatter-plot.\n\n\nCode\nfig_bubble_pretty_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    # will change the size of the point based on population size \n    size = population, \n    # will assign colors based on the continent the country is in \n    color = continent\n  ) +\n  # gives a range as to how big or small the points of population should be\n  scale_size(range = c(1, 20)) + \n  # removes N/A from the legend and titles it Continent \n  scale_colour_discrete(na.translate = F, name = \"Continent\") +\n  # removes population size from the legend \n  guides(size = \"none\") +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    # transforms numbers from scientific notation to regular number \n    labels = scales::comma\n  ) +\n  labs(\n    title = \"Fertility rate descreases as GDP per capita increases in 2011\",\n    y = \"Fertility rates\",\n    caption = \"Source: Gapminder\"\n  ) +\n  # the ylim was set based on the fertility, lowest was 1.15 & highest was 7.25\n  ylim(1.2, 8.0) +\n  # alpha increases transparency of the points to ensure they can all be seen\n  geom_point(alpha = 0.5) \n\nfig_bubble_pretty_2011\n\n\n\n\n\n\n\n\n\n[@ggplot-2011-adv] builds on the previous scatterplot of Fertility Rates (y axis) against GDP per capita (x axis) for 2011. The bubble size depicts respective country populations, and continents are coded by colors according to the key. This figure displays a negative relationship between GDP per capita and Fertility Rates. It supports the Hypothesis which states that as GDP per capita increases, Fertility Rates decreases. This trend can be confirmed for all continents, however, the degree to which fertility rates drop between continents varies. Most European country appear below a fertility rate of 2 babies per woman. The Americas appear to follow closely behind (under 4), followed by Oceania and Asia. A significant number of African countries still maintained higher fertility rates with lower GDP per capita for 2011.\n\n\nFacets\nHere is an example of wanting to create four separate graphs to see the relationship between fertility rates and GDP per capita based on the years 1860, 1910, 1960 and 2010. In this example we omitted the facet argument.\n\n\nCode\nfig_bubble_multiple &lt;-\n  ggplot(data = filter(gapminder_data, year %in% c(1860, 1910, 1960, 2010))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE, name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    subtitle = \"throughout 1860, 1910, 1960 and 2010\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) \n\nfig_bubble_multiple\n\n\n\n\n\n\n\n\n\nWithout having used the facet argument, all points of all four years have been included into one graph. This graph does not provide us with the information we were looking for.\n\n\nCode\nfig_bubble_multiple_facet &lt;-\n  ggplot(data = filter(gapminder_data, year %in% c(1860, 1910, 1960, 2010))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE , name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    subtitle = \"throughout 1860, 1910, 1960 and 2010\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) +\n  # specifiying we want the graphs split based on year\n  facet_wrap(~ year)\n\nfig_bubble_multiple_facet\n\n\n\n\n\n\n\n\n\nNow that we’ve specified the facet argument, we now have four seperate graphs that can be properly analysed. In [@ggplot-facet-years] we see an increasingly negative relationship between the two variables over time. This observation is congruent with the hypothesis that as GDP per capita increases, fertility decreases.\nThis global trend can be attributed to the increasing proportion of women in the workforce in the mid to late 20th century. As a result of World War II (1939-1945), women took on roles outside the home to compensate for men at war. Despite increased GDP per capita, this may have contributed to reduced fertility (babies per woman) over time. During 1860 - 1910, the scatter-plot figures remained in the upper left quadrant with the numbers remaining between 2 - 8 babies per woman. In 1960, a clear disparity among continents is seen. Most European countries’ fertility rates fell below 5, while their GDP per capita increased. Most African countries maintained high fertility rates above 5, but little change is seen in GDP per capita. The Asian continent shows the most variation among countries during that year. Some smaller Asian countries continued to maintain high fertility rates as GDP per capita increased in 1960. However, others displayed a drastic decrease in fertility rates by 1960. The Americas followed a steady decline over the years. By 2010, an overall negative relationship can be seen with most countries’ fertility rates below 5 babies per woman.\n\nfig_bubble_row_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent, nrow = 1)\n\nfig_bubble_row_2011 \n\n\n\n\n\n\n\n\nIn the graph above, we see an example of seperating the single graph into graphs based on continent. It has also been specified to have all graphs appear in one single row through the nrow argument. However, this graph is also unclear and cannot be used to compare the relationship between fertility and gdp per capita.\n\nfig_bubble_facet_2011 &lt;-\n  ggplot(data = filter(gapminder_data, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent)\n\nfig_bubble_facet_2011 \n\n\n\n\n\n\n\n\nIn the next example above, we removed the nrow argument and the system automatically seperated the graphs into three columns with two rows. However, again, there is no way to clearly determine any relationship between fertility and gdp per capita."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#scatterplot-animation",
    "href": "lessons_original/01_ggplot2.html#scatterplot-animation",
    "title": "How to create a scatterplot",
    "section": "Scatterplot animation",
    "text": "Scatterplot animation\nGGplot2 contains the “gganimate” package that allows for animation of data. It enhances data visualization through real-time outputs. In this case the gapminder data will be filtered to 2011 and below (full data available).\n\n\nCode\ngapminder_df &lt;- \n  gapminder_data %&gt;% \n  # Excludes data beyond 2011 (last year with complete data)\n  filter(year &lt;= \"2011\")\n\nfig_animate &lt;- \n  ggplot(gapminder_df) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent \n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  # Assigns color palette \n  scale_color_viridis_d() +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  geom_point(show.legend = TRUE, alpha = 0.7) +\n  # Assigns the gganimate features\n  transition_time(year) +\n  ease_aes('linear', interval = 2.0) +\n  # Prints time of current frame\n  labs(title = \"Year: {frame_time}\", x = \"GDP per capita\", y = \"Fertility\")\n  \n\nfig_animate\n\n\nNULL\n\n\n@plot-animate depicts the changes between Fertility and GDP per capita as the years increase from 1799 to 2011 (last full data year). This allows real-time visualization of the decrease in fertility and increase in GDP per capita.\n\n\nCode\nfacet_animate &lt;- \n  ggplot(gapminder_df) +\n  aes(\n    x = gdp_per_capita, \n    y = fertility,\n    size = population, \n    colour = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  # Groups output by continents\n  facet_wrap(~continent) +\n  labs(\n    title = 'Year: {closest_state}', \n    x = 'GDP per capita', \n    y = 'fertility'\n  ) +\n  geom_point(alpha = 0.7, show.legend = TRUE) +\n  # Contains gganimate features\n  transition_states(year, transition_length = 3, state_length = 1) +\n  # Animation pattern, time between each state\n  ease_aes('linear', interval = 2.0)\n\nfacet_animate\n\n\nNULL\n\n\nIn @animate-facets, the ggplot data for various continents as time passes is shown to support the initial hypothesis."
  },
  {
    "objectID": "lessons_original/01_ggplot2.html#conclusion",
    "href": "lessons_original/01_ggplot2.html#conclusion",
    "title": "How to create a scatterplot",
    "section": "Conclusion",
    "text": "Conclusion\nA global negative trend is depicted between GDP per capita and fertility over time. Such changes were due to wars as well as social, cultural and economic changes that incentivize smaller families especially in Asian countries. Most European, American and Asian countries depicted significant decreases in fertility rates over time as GDP per capita increased. On the other hand, African countries remain in the top rank for fertility over the years. These differences are depicted in the population pyramid changes of developed vs developing countries. Public health policies can be tailored to incentivizing increased fertility in developed countries to ensure generation continuity, and effective family planning strategies in developing countries."
  },
  {
    "objectID": "lessons_original/01_rayshader.html",
    "href": "lessons_original/01_rayshader.html",
    "title": "R Rayshader Overview",
    "section": "",
    "text": "R rayshader is an R package that allows users to generate high-quality 3D maps, visualizations, and animations.\nrayshader also allows the user to translate ggplot2 objects into beautiful 3D data visualizations.\n\nTo install rayshader, you can use the following code in R:\n\n# remotes::install_github(\n#   \"tylermorganwall/rayshader\"\n# )\n\n# remotes::install_cran(\"rayrender\")"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#overview",
    "href": "lessons_original/01_rayshader.html#overview",
    "title": "R Rayshader Overview",
    "section": "",
    "text": "R rayshader is an R package that allows users to generate high-quality 3D maps, visualizations, and animations.\nrayshader also allows the user to translate ggplot2 objects into beautiful 3D data visualizations.\n\nTo install rayshader, you can use the following code in R:\n\n# remotes::install_github(\n#   \"tylermorganwall/rayshader\"\n# )\n\n# remotes::install_cran(\"rayrender\")"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#functions",
    "href": "lessons_original/01_rayshader.html#functions",
    "title": "R Rayshader Overview",
    "section": "Functions",
    "text": "Functions\n\nrayshader 0.35. 1 has 56 functions and 4 datasets\nseven functions related to mapping\nalso has functions to add water and generate overlays\nalso included are functions to add additional effects and information to 3D visualizations\nfunctions for converting rasters to matrices\nfunctions to display and save your visualizations\nrayshader has a function to generate 3D plots using ggplot2 objects"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#example",
    "href": "lessons_original/01_rayshader.html#example",
    "title": "R Rayshader Overview",
    "section": "Example",
    "text": "Example\nFirst we load all the required libraries. These libraries are required for various functions and operations used in creating 3D maps with rayshader.\n\nlibrary(rayshader)\nlibrary(rayrender) \nlibrary(reshape2)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#example-1",
    "href": "lessons_original/01_rayshader.html#example-1",
    "title": "R Rayshader Overview",
    "section": "Example",
    "text": "Example\nThen, we download and load the data\n\n# Here, I load a map with the raster package.\nloadzip &lt;- tempfile() \n\ndownload.file(\"https://tylermw.com/data/dem_01.tif.zip\", loadzip)\n\nlocaltif &lt;- raster::raster(\n  unzip(loadzip, \"dem_01.tif\")\n)\n\nunlink(loadzip)\n\n# write_rds(localtif, \"../data/01_rayshader_eg_20240503.rds\")\n\nIn this code snippet, we create a temporary file (loadzip) to store the downloaded zip file from the specified URL. The download.file() function is used to download the file, and unzip() is used to extract the “dem_01.tif” file from the downloaded zip. Finally, we load the raster data into the localtif object."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#create-map",
    "href": "lessons_original/01_rayshader.html#create-map",
    "title": "R Rayshader Overview",
    "section": "Create Map",
    "text": "Create Map\nTo create a map first we need to convert this raster data file into a matrix using raster_to_matrix()\n\n#And convert it to a matrix:\nelmat &lt;- raster_to_matrix(localtif)\n\nLoading required package: raster\n\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nThen we use sphere_shade() and plot_map() to create our base map\n\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nHere, elmat is a matrix created from the raster data using the raster_to_matrix() function. sphere_shade() applies shading to the elevation matrix, giving it a 3D effect. The texture parameter specifies the type of texture to be applied. In this case, it uses the “desert” texture. Finally, plot_map() is used to display the shaded map."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#add-water-layer",
    "href": "lessons_original/01_rayshader.html#add-water-layer",
    "title": "R Rayshader Overview",
    "section": "Add Water Layer",
    "text": "Add Water Layer\nWe can add a water layer to the map using detect_water() and add_water()\n\n# detect_water and add_water adds a water layer to the map:\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nIn this code snippet, detect_water() function detects water areas in the elevation matrix. Then, add_water() adds a water layer to the map using the detected water areas. The color parameter specifies the color of the water. Finally, plot_map() is used to display the map with the water layer."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#add-shadow-layer",
    "href": "lessons_original/01_rayshader.html#add-shadow-layer",
    "title": "R Rayshader Overview",
    "section": "Add Shadow Layer",
    "text": "Add Shadow Layer\nWe can also add shadow layer in the map.\n\n# And here we add an ambient occlusion shadow layer, which models lighting\n#   from atmospheric scattering:\n\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  add_shadow(ray_shade(elmat), 0.5) %&gt;%\n  add_shadow(ambient_shade(elmat), 0) %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nHere, add_shadow() is used to add a shadow layer to the map. ray_shade() calculates shadows based on the elevation matrix (elmat). The zscale parameter controls the strength of the shadows. ambient_shade() generates ambient lighting for the map. The second parameter of add_shadow() specifies the opacity of the shadows. Finally, plot_map() displays the map with shadows."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#convert-to-3d",
    "href": "lessons_original/01_rayshader.html#convert-to-3d",
    "title": "R Rayshader Overview",
    "section": "Convert to 3D",
    "text": "Convert to 3D\nWe can convert this 2D map into 3D mapping using plot_3d() (by passing a texture map into the plot_3d function)\n\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  add_shadow(ray_shade(elmat, zscale = 3), 0.5) %&gt;%\n  add_shadow(ambient_shade(elmat), 0) %&gt;%\n  plot_3d(\n    elmat, zscale = 10, fov = 0, theta = 135,\n    zoom = 0.75, phi = 45, windowsize = c(1000, 800)\n  )\nSys.sleep(0.2)\nrender_snapshot()\n\n\n\n\n\n\n\n\nWe can add a scale bar, as well as a compass using render_scalebar() and render_compass()\n\nrender_camera(fov = 0, theta = 60, zoom = 0.75, phi = 45)\nrender_scalebar(\n  limits = c(0, 5, 10),\n  label_unit = \"km\",\n  position = \"W\",\n  y = 50,\n  scale_length = c(0.33,1)\n)\nrender_compass(position = \"E\")\nrender_snapshot(clear = TRUE)\n\n\n\n\n\n\n\n\nHere, render_camera() sets the camera properties for the 3D map. render_scalebar() adds a scale bar to the map. The limits parameter specifies the limits of the scale bar, label_unit provides the label for the scale, position sets the position of the scale bar, y controls the vertical position, and scale_length determines the length of the scale bar. render_compass() adds a compass to the map, and render_snapshot() captures the final image of the map."
  },
  {
    "objectID": "lessons_original/01_rayshader.html#d-plotting-with-rayshader-and-ggplot2",
    "href": "lessons_original/01_rayshader.html#d-plotting-with-rayshader-and-ggplot2",
    "title": "R Rayshader Overview",
    "section": "3D plotting with rayshader and ggplot2",
    "text": "3D plotting with rayshader and ggplot2\nRayshader can also be used to make 3D plots out of ggplot2 objects using the plot_gg() function\n\nggdiamonds = ggplot(diamonds) +\n  stat_density_2d(\n    aes(\n      x = x, y = depth, fill = stat(nlevel)\n    ), \n    geom = \"polygon\", n = 200, bins = 50,contour = TRUE\n  ) +\n  facet_wrap(clarity~.) +\n  scale_fill_viridis_c(option = \"A\")\n\npar(mfrow = c(1, 2))\n\nplot_gg(ggdiamonds, width = 5, height = 5, raytrace = FALSE, preview = TRUE)\n\nWarning: `stat(nlevel)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(nlevel)` instead.\n\n\n\n\n\n\n\n\nplot_gg(\n  ggdiamonds, \n  width = 5, \n  height = 5, \n  multicore = TRUE, \n  scale = 250, \n  zoom = 0.7, \n  theta = 10, \n  phi = 30, \n  windowsize = c(800, 800)\n)\nSys.sleep(0.2)\nrender_snapshot(clear = TRUE)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#contour-plot",
    "href": "lessons_original/01_rayshader.html#contour-plot",
    "title": "R Rayshader Overview",
    "section": "Contour Plot",
    "text": "Contour Plot\nRayshader will automatically ignore lines and other elements that should not be mapped to 3D.\nHere’s a contour plot of the volcano dataset.\n\n# Contours and other lines will automatically be ignored. Here is the volcano\n#   dataset:\n\nggvolcano &lt;- volcano %&gt;% \n  melt() %&gt;%\n  ggplot() +\n  geom_tile(aes(x = Var1, y = Var2, fill = value)) +\n  geom_contour(aes(x = Var1, y = Var2, z = value), color = \"black\") +\n  scale_x_continuous(\"X\", expand = c(0, 0)) +\n  scale_y_continuous(\"Y\", expand = c(0, 0)) +\n  scale_fill_gradientn(\"Z\", colours = terrain.colors(10)) +\n  coord_fixed()\n\npar(mfrow = c(1, 2))\nplot_gg(ggvolcano, width = 7, height = 4, raytrace = FALSE, preview = TRUE)\n\nWarning: Removed 1861 rows containing missing values or values outside the scale range\n(`geom_contour()`).\n\n\n\n\n\n\n\n\n\n\nplot_gg(\n  ggvolcano,\n  multicore = TRUE, \n  raytrace = TRUE, \n  width = 7, \n  height = 4, \n  scale = 300, \n  windowsize = c(1400, 866), \n  zoom = 0.6, \n  phi = 30, \n  theta = 30\n)\n\nWarning: Removed 1861 rows containing missing values or values outside the scale range\n(`geom_contour()`).\n\nSys.sleep(0.2)\n\nrender_snapshot(clear = TRUE)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#mtcars-data-example",
    "href": "lessons_original/01_rayshader.html#mtcars-data-example",
    "title": "R Rayshader Overview",
    "section": "mtcars Data Example",
    "text": "mtcars Data Example\nRayshader also detects when the user passes the color aesthetic, and maps those values to 3D\n\nmtplot = ggplot(mtcars) + \n  geom_point(\n    aes(x = mpg, y = disp, color = cyl)\n  ) + \n  scale_color_continuous(limits = c(0, 8))\n\npar(mfrow = c(1, 2))\nplot_gg(mtplot, width = 3.5, raytrace = FALSE, preview = TRUE)\n\n\n\n\n\n\n\nplot_gg(mtplot)\nSys.sleep(0.2)\nrender_snapshot(clear = TRUE)"
  },
  {
    "objectID": "lessons_original/01_rayshader.html#reference",
    "href": "lessons_original/01_rayshader.html#reference",
    "title": "R Rayshader Overview",
    "section": "Reference",
    "text": "Reference\n\nhttps://www.rayshader.com/\nhttps://www.youtube.com/watch?v=zgFXVhmKNbU"
  },
  {
    "objectID": "lessons_original/01_skimr.html",
    "href": "lessons_original/01_skimr.html",
    "title": "Skimr Package",
    "section": "",
    "text": "Skimr is an R package designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The function is modifiable where you can add additional variables, which are not a part of default summary function within R. Skimr allows us to quickly assess data quality by feature and type in a quick report. This is a critical step in Data Exploration, where Understanding our data helps us to generate a hypothesis and determine what data analysis are appropriate.\nThis presentation will cover the simplest and most effective ways to explore data in R.\n\n\nTo begin we will upload the packages necessary for the lesson, this includes the following:\n\nreadr() to import our data file\nknitr() that houses the kable() feature that allows us to construct and customize tables.\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n\nCode\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n\n\n\nFor this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n\nCode\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\""
  },
  {
    "objectID": "lessons_original/01_skimr.html#packages",
    "href": "lessons_original/01_skimr.html#packages",
    "title": "Skimr Package",
    "section": "",
    "text": "To begin we will upload the packages necessary for the lesson, this includes the following:\n\nreadr() to import our data file\nknitr() that houses the kable() feature that allows us to construct and customize tables.\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n\nCode\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lessons_original/01_skimr.html#census-data",
    "href": "lessons_original/01_skimr.html#census-data",
    "title": "Skimr Package",
    "section": "",
    "text": "For this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n\nCode\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\""
  },
  {
    "objectID": "lessons_original/01_skimr.html#separate-dataframes-by-type",
    "href": "lessons_original/01_skimr.html#separate-dataframes-by-type",
    "title": "Skimr Package",
    "section": "4.1 Separate dataframes by type",
    "text": "4.1 Separate dataframes by type\nThe data frames produced by skim() are wide and sparse, filled with columns that are mostly NA. For that reason, it can be convenient to work with “by type” subsets of the original data frame. These smaller subsets have their NA columns removed.\nFeatures:\n\npartition() - Creates a list of smaller data frames. Each entry in the list is a data type from the original dataframe\nbind() - Takes the list and rebuilds the original dataframe.\nyank() - Extract a subtable from a dataframe with a particular type.\n\nThe following syntax is using partition() to separate the large census_df.\n\n\nCode\n# split the character and numeric data\nseparate_df &lt;- partition(skim(census_2010))\n# check only the character data\nseparate_df$character\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\n\nCode\n\n# create summary statistics for only numeric variables\nnumeric_separate_df &lt;- separate_df[2]\n# pull out the desired summary statistics in the nested list\nhead(numeric_separate_df$numeric[\"mean\"]) %&gt;% \n  kable(digits = 1) \n\n\n\n\n\nmean\n\n\n\n\n49.8\n\n\n2.7\n\n\n5.2\n\n\n30.3\n\n\n101.9\n\n\n193387.1\n\n\n\n\n\nThe following syntax is using bind() to combine the smaller character and numeric lists into the desired df.\n\n\nCode\n# combine the character and numeric data\nhead(bind(separate_df))\n\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\n\n\n\nCode\n\n# confirm that the bound table is the same as the original skimmed table\nidentical(bind(separate_df), skim(census_2010)) \n[1] TRUE\n\n\nThe following syntax is using yank() to extract a specific table eg.character to examine.\n\n\nCode\n# Extract character data\nyank(skim(census_2010), \"character\")\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0"
  },
  {
    "objectID": "lessons_original/01_skimr.html#skimr-with-dplyr",
    "href": "lessons_original/01_skimr.html#skimr-with-dplyr",
    "title": "Skimr Package",
    "section": "4.2 Skimr with Dplyr",
    "text": "4.2 Skimr with Dplyr\nSkimr functions can be used in combination with Dplyr functions to examine specific variables within the census dataset.\nThe following example used skim() with filter() to display the variable CENSUS2010POP. The dataframe was further customized to display variable name and data type using select().\n\n\nCode\n# use dplyr functions on the statistics summary table\ncensus_filter &lt;- skim(census_2010) %&gt;% \n  filter(skim_variable == \"CENSUS2010POP\")\ncensus_filter\n\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCENSUS2010POP\n0\n1\n193387\n1176201\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\n\n\n\nCode\n\ncensus_select &lt;- skim(census_2010) %&gt;% \n  select(skim_type, skim_variable)\nhead(census_select)\n# A tibble: 6 × 2\n  skim_type skim_variable\n  &lt;chr&gt;     &lt;chr&gt;        \n1 character STNAME       \n2 character CTYNAME      \n3 numeric   SUMLEV       \n4 numeric   REGION       \n5 numeric   DIVISION     \n6 numeric   STATE        \n\n\nYou can also customize the output of the skim() function by using various arguments. For example, you can use the numeric argument to specify which variables should be treated as numeric variables, or use the ranges argument to specify custom ranges for variables.\nUsing skim() in combination with mutate() we will compute a new variable to add to our skim dataframe.\n\n\nCode\n# create a new variable calculate the change in birth rate from 2010 to 2011\ncensus_2010 %&gt;% \n  # new variable\n  mutate(net_birth = BIRTHS2011 - BIRTHS2010) %&gt;% \n  # move the variable to the beginning of the dataset\n  relocate(net_birth, .after = CENSUS2010POP) %&gt;% \n  # summary statistics table\n  skim() %&gt;% \n  # only the first fifteen variables\n  head(n = 15) %&gt;% \n  # change the formatting \n  kable(digit = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nSTNAME\n0\n1\n4\n20\n0\n51\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSUMLEV\n0\n1\nNA\nNA\nNA\nNA\nNA\n4.98e+01\n1.25e+00\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nnumeric\nREGION\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.67e+00\n8.10e-01\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nnumeric\nDIVISION\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.19e+00\n1.97e+00\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nnumeric\nSTATE\n0\n1\nNA\nNA\nNA\nNA\nNA\n3.03e+01\n1.52e+01\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nnumeric\nCOUNTY\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.02e+02\n1.08e+02\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nnumeric\nCENSUS2010POP\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.93e+05\n1.18e+06\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nnumeric\nnet_birth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.87e+03\n1.18e+04\n-3\n96\n232\n639\n386443\n▇▁▁▁▁\n\n\nnumeric\nESTIMATESBASE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.93e+05\n1.18e+06\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.94e+05\n1.18e+06\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2011\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.95e+05\n1.19e+06\n90\n11277\n26417\n72387\n37700034\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2012\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.97e+05\n1.20e+06\n81\n11195\n26362\n72496\n38056055\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2013\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.98e+05\n1.21e+06\n89\n11180\n26519\n72222\n38414128\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2014\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.00e+05\n1.22e+06\n87\n11121\n26483\n72257\n38792291\n▇▁▁▁▁"
  },
  {
    "objectID": "lessons_original/01_skimr.html#adding-variables",
    "href": "lessons_original/01_skimr.html#adding-variables",
    "title": "Skimr Package",
    "section": "4.3 Adding Variables",
    "text": "4.3 Adding Variables\n\nbase - An sfl that sets skimmers for all column types.\nappend - Whether the provided options should be in addition to the defaults already in skim. Default is TRUE.\n\nAs mentioned, skim() is designed to display default statistics, however you can use this function to change the summary statistics that it returns.\nskim_with() is type closure: a function that returns adds a new variable to the table. This lets you have several skimming functions in a single R session, but it also means that you need to assign the return of skim_with() before you can use it.\nYou assign values within skim_with() by using the sfl() helper (skimr function list). It identifies which skimming functions you want to remove, by setting them to NULL. Assign an sfl to each column type that you wish to modify.\nFor example, we will add the following variables to the dataframe: median, min, max, IQR, length.\n\n\nCode\nmy_skim &lt;- skim_with(\n  numeric = sfl(median, min, max, IQR),\n  character = sfl(length), \n  append = TRUE\n)\n\n# add new variables into the summary table\ncensus_2010 %&gt;% \n  my_skim() %&gt;% \n  head(n = 10)\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nlength\n\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n3193\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n3193\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nmedian\nmin\nmax\nIQR\n\n\n\n\nSUMLEV\n0\n1\n4.98e+01\n1.25e+00\n40\n50\n50\n50\n50\n▁▁▁▁▇\n50\n40\n50\n0\n\n\nREGION\n0\n1\n2.67e+00\n8.10e-01\n1\n2\n3\n3\n4\n▁▆▁▇▂\n3\n1\n4\n1\n\n\nDIVISION\n0\n1\n5.19e+00\n1.97e+00\n1\n4\n5\n7\n9\n▂▇▅▆▃\n5\n1\n9\n3\n\n\nSTATE\n0\n1\n3.03e+01\n1.52e+01\n1\n18\n29\n45\n56\n▃▇▆▆▇\n29\n1\n56\n27\n\n\nCOUNTY\n0\n1\n1.02e+02\n1.08e+02\n0\n33\n77\n133\n840\n▇▁▁▁▁\n77\n0\n840\n100\n\n\nCENSUS2010POP\n0\n1\n1.93e+05\n1.18e+06\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n26424\n82\n37253956\n60105\n\n\nESTIMATESBASE2010\n0\n1\n1.93e+05\n1.18e+06\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n26446\n82\n37254503\n60192\n\n\nPOPESTIMATE2010\n0\n1\n1.94e+05\n1.18e+06\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n26467\n83\n37334079\n60446"
  },
  {
    "objectID": "lessons_original/01_table1.html",
    "href": "lessons_original/01_table1.html",
    "title": "Demographics table with table1",
    "section": "",
    "text": "Code\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"lag\", \"dplyr\", quiet = TRUE)\n\nsuppressPackageStartupMessages(library(tidyverse))\n\n# suppress \"`summarise()` has grouped output by \" messages\noptions(dplyr.summarise.inform = FALSE)"
  },
  {
    "objectID": "lessons_original/01_table1.html#necessary-packages",
    "href": "lessons_original/01_table1.html#necessary-packages",
    "title": "Demographics table with table1",
    "section": "Necessary Packages",
    "text": "Necessary Packages\nThe htmlTable package allows for the usage of the table1() function to create a table 1, while also making life easy when attempting to copy this table into a Word document.\nThe boot package was created to aid in performing bootstrapping analysis. With it comes numerous data sets, specifically clinical trial data sets to make this possible. However, there is no code book provided within the package when the data is downloaded as a csv file. This is a link on Github that explains and elaborates on every data within the package itself2.\n\n#install.packages(\"htmlTable\")\n#install.packages(\"boot\")\n\n# Load libraries\nlibrary(htmlTable)\nlibrary(table1)\nlibrary(boot)"
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html",
    "href": "lessons_original/02_fisher_exact_test.html",
    "title": "Fisher’s Exact Test",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated, which are also somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n\n\n\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:\\((H_0)\\) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: \\((H_1)\\) There is a relationship between the categorical variables, the variables are dependent."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#introduction",
    "href": "lessons_original/02_fisher_exact_test.html#introduction",
    "title": "Fisher’s Exact Test",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated, which are also somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n\n\n\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:\\((H_0)\\) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: \\((H_1)\\) There is a relationship between the categorical variables, the variables are dependent."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#fishers-exact-test-equation",
    "href": "lessons_original/02_fisher_exact_test.html#fishers-exact-test-equation",
    "title": "Fisher’s Exact Test",
    "section": "Fisher’s Exact Test Equation",
    "text": "Fisher’s Exact Test Equation\nFisher’s exact test for a one-tailed p-value is calculated using the following formula:\n\\[   p = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!} \\] - n = population size/ total frequency\n- a + b = “successes” values in the contingency table\n- a + c = sample size / draws from the population\n- a = sample successes\n\nFormula description\nthis test is usually used as a one-tailed test but it can also be used as a two tailed test as well, \\(a\\),\\(b\\),\\(c\\), and \\(d\\) are the individual frequencies on the 2x2 contingency table and \\(n\\) is our total frequency. This particular test is used to obtain the probability of the combination of frequencies that we can actually obtain.\n\n\nWhat is a contingency table?\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables. The table() function is used to create a contingency table in R. When the variables of interest are summarized in a contingency table it is easier to run the Fisher’s Exact test.\n\nExample: Creating a contingency table\nLets say we have information on the gender of participants in a clinical trial and the type of drug administered to them we can create the following contingency table for further analysis.\n\n# Example R code to create a contingency table\n\n# Creating a data frame\n df &lt;- data.frame(\n   \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\"),\n   \"Gender\" = c(\"Male\", \"Male\", \"Female\")\n )\n \n# Creating contingency table using table()\nctable &lt;- table(df)\nprint(ctable)\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n\n\n\n\n\nPerforming Fisher’s Exact Test in R\nWe will need to install the ggstatplot package to visualize the statistical results.\n\n# install.packages(\"ggstatplot\") \n# install.packages(\"summarytools\")\n# install.packages(\"gmodels\")\n# install.packages(tidyverse)\n\n\n\nData Source: GMP2017\nFor this example we will be using the Greater Manchester Police’s UK stop and search data from 2017(December) sourced from the Sage Research Methods Dataset Part 2 (https://methods.sagepub.com/dataset/fishers-exact-gmss-2017). This data has information on stop and search events, gender and ethnicity. For this example we would like to access whether there is a significant relationship between gender and stop and search events (having controlled drugs vs harmful weapons)?\n\nGMP17 &lt;- read.csv(\n  \"../data/02_dataset-gmss-2017-subset1_jittered_20240503.csv\"\n)\n\n\n\nLoad in libraries\n\nlibrary(gmodels)\nlibrary(ggstatsplot)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(katex)\nlibrary(tidyverse)\n\n\n\nDescriptive summary\n\n\nCode\nhead(GMP17)\n\n\n  Gender Ethnicity ObjectSearch\n1      1         1            1\n2      1         1           -9\n3      1         1            1\n4      1         1            1\n5      1         1           -9\n6      1         1            1\n\n\nCode\nstr(GMP17)\n\n\n'data.frame':   186 obs. of  3 variables:\n $ Gender      : int  1 1 1 1 1 1 1 1 1 -9 ...\n $ Ethnicity   : int  1 1 1 1 1 1 2 1 1 1 ...\n $ ObjectSearch: int  1 -9 1 1 -9 1 1 1 -9 -9 ...\n\n\nCode\n# determining the number of rows\nNROW(GMP17)\n\n\n[1] 186\n\n\n\n\nAssessing frequencies to answer research question\nFor this analysis we will use the Gender variable and the ObjectSearch variable\n\n# Dropping the Ethnicity variable to remain with variables of interest for for the 2x2 table\n\nnewGMP17 &lt;-GMP17[ -c(2) ]\n \nhead(newGMP17)\n\n  Gender ObjectSearch\n1      1            1\n2      1           -9\n3      1            1\n4      1            1\n5      1           -9\n6      1            1\n\n\nThe data contains missing values categorized as -9 that we need to drop and we need to rename our variables based on the data dictionary provided https://methods.sagepub.com/dataset/download/fishers-exact-gmss-2017/guide/codebook.\n\n# Exclude rows that have missing data in both variables\nnewGMP17_nom &lt;- subset(newGMP17, Gender &gt; 0)\nnewGMP17_nom2 &lt;- subset(newGMP17_nom, ObjectSearch  &gt; 0)\nsummary(newGMP17_nom2)\n\n     Gender       ObjectSearch  \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :1.000  \n Mean   :1.052   Mean   :1.267  \n 3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000  \n\nnrow(newGMP17_nom2)\n\n[1] 116\n\n\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$Gender &lt;- recode_factor(\n  newGMP17_nom2$Gender,\n  \"1\" = \"Male\",\n  \"2\" = \"Female\"\n)\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$ObjectSearch &lt;- recode_factor(\n  newGMP17_nom2$ObjectSearch,\n  \"1\" = \"Controlled_Drugs\",\n  \"2\" = \"Harmful_Objects\"\n)\n\n\n# Creating the contingency table for subset data\ncGMP17 = table(newGMP17_nom2)\nprint(cGMP17)\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male                 83              27\n  Female                2               4\n\n\n\n\nVisualizing data using mosaic plot\n\nwe can use the mosaic plot to represent the data.\n\n\nmosaicplot(\n  cGMP17,\n  main = 'Mosaic Plot',\n  color = TRUE\n)\n\n\n\n\n\n\n\n\n\n\nRunning the Fisher’s exact test using fisher.test()\nWhat if we just run a Chi-square test?\nUsing our GMP17 dataset we can try to run a Chi-square test instead of the Fisher’s Exact test and see what happens.\nThe R output gives us a warning that the Chi Square is not appropriate hence we should use another test in this case the Fisher’s Exact Test.\n\nchisq.test(cGMP17)$expected\n\nWarning in chisq.test(cGMP17): Chi-squared approximation may be incorrect\n\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male          80.603448       29.396552\n  Female         4.396552        1.603448\n\n\n\n\nRunning the test\n\n# running the fisher's exact test\n\ntest &lt;- fisher.test(cGMP17)\ntest\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  cGMP17\np-value = 0.04297\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.8133673 70.2637501\nsample estimates:\nodds ratio \n  6.030297 \n\n\nUsing the gt summary to view results.\n\nnewGMP17_nom2 |&gt; \n  tbl_summary(by = Gender) |&gt; \n  add_p() |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 1161\nMale, N = 1101\nFemale, N = 61\np-value2\n\n\n\n\nObjectSearch\n\n\n\n\n\n\n0.043\n\n\n    Controlled_Drugs\n85 (73%)\n83 (75%)\n2 (33%)\n\n\n\n\n    Harmful_Objects\n31 (27%)\n27 (25%)\n4 (67%)\n\n\n\n\n\n1 n (%)\n\n\n2 Fisher’s exact test\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of results\nThe most important test statistic is the p - value therefore we can retrieve the specific result using the following code;\n\ntest$p.value \n\n[1] 0.04297268\n\n\nOdds ratio = 6.33, 95% CI = 0.85-73.59], we reject the null hypothesis (p &lt; 0.05) and conclude that there is a strong association between the two categorical independent variables (gender and object search events)\nTherefore the odds ratio indicates that the odds of having controlled drugs at a stop and search is 6.33 times as likely for males compared to females. In other words, males are more likely of having controlled drugs at a stop and search than females.\n\n\nVisualizing statistical results with plots using ggstatsplot\n\nwe download the ggsattsplot package to visualize the results in a plot.\n\n\n# Fisher's exact test \n\ntest &lt;- fisher.test(cGMP17)\n\n# combine plot and statistical test with ggbarstats\n\nggbarstats(\n  newGMP17_nom2, Gender, ObjectSearch,\n  results.subtitle = FALSE,\n  subtitle = paste0(\n    \"Fisher's exact test\", \", p-value = \",\n    ifelse(test$p.value &lt; 0.001, \"&lt; 0.001\", round(test$p.value, 3))\n  )\n)\n\n\n\n\n\n\n\n\nFrom the plot, it is clear that the proportion of males among object search events is higher compared to females, suggesting that there is a relationship between the two variables.\nThis is confirmed thanks to the p-value displayed in the subtitle of the plot. As previously, we reject the null hypothesis and we conclude that the variables gender and stop and search events are not independent (p-value = 0.038)."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#what-if-we-have-more-than-two-levels",
    "href": "lessons_original/02_fisher_exact_test.html#what-if-we-have-more-than-two-levels",
    "title": "Fisher’s Exact Test",
    "section": "What if we have more than two levels?",
    "text": "What if we have more than two levels?\nUsing the drug example used previously lets say we have 3 drugs ‘Drug A, Drug B or Drug C’ and we want to see if there is any relationship with gender ‘Male/Female’.\n\n# Creating a data frame\ndf &lt;- data.frame (\n  \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\", \"Drug C\", \"Drug C\"),\n  \"Gender\" = c(\"Male\", \"Male\", \"Female\", \"Female\", \"Female\")\n)\n \n# Creating contingency table using table()\nctable &lt;- table(df)\nprint(ctable)\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n  Drug C      2    0\n\n\n\n# Running the Fisher's Exact test for the 3x2 table\nfisher.test(ctable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value = 0.6\nalternative hypothesis: two.sided\n\n\nThe p-value is non-significant [p = 0.6], we fail to reject the null hypothesis (p &lt; 0.05) and conclude that there is no association between the drug treatments and gender. If the results had been significant we would have gone ahead and conducted a post hoc analysis using pairwise_fisher_test to asses each combination.\nSummary\nThis article describe the assumptions and hypotheses of the Fisher’s Exact test. It also provides examples on how it can be applied."
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#references",
    "href": "lessons_original/02_fisher_exact_test.html#references",
    "title": "Fisher’s Exact Test",
    "section": "References",
    "text": "References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "What is bootstrapping?\nBootstrapping is a technique from Efron (1979) that is built on a simple idea: if the data we have is a sample from a population, why don’t we sample from our own data to make more samples? Now, because we don’t have access to any new data, we’re going to take samples of our data set with replacement.\n\n\nThe purpose of bootstrapping is to increase the sample size for our analysis when the sample we have been given is small."
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#when-to-use-bootstrapping",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#when-to-use-bootstrapping",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "The purpose of bootstrapping is to increase the sample size for our analysis when the sample we have been given is small."
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#distribution",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#distribution",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.1 Distribution",
    "text": "3.1 Distribution\nBoxplots and histograms will be useful to understand the distribution of the data.\nOur data is not normal based on the distribution.\n\n\nCode\n# check the boxplot of the data\nboxplot(\n  new_penguins_df$flipper_length_mm ~ new_penguins_df$island, las = 1, \n  ylab = \"Flipper Length (mm)\",\n  xlab = \"Island\",\n  main = \"Flipper Length by Island\"\n)\n\n\n\n\n\n\n\n\n\nCode\n\n# check the histogram of the data\nhist(\n  x = new_penguins_df$flipper_length_mm,\n  main = \"Distribution of Flipper Length (mm)\",\n  xlab = \"Flipper Length\"\n)"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#bootstrapping-test",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#bootstrapping-test",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.2 Bootstrapping Test",
    "text": "3.2 Bootstrapping Test\nWe need the difference in means in order to conduct our permutation test. We will test whether the difference is significant so that we can reject the null. This indicates that there is a different in flipper length among the same species that come from different islands.\n\n\nCode\n# set a seed so that our random results can be replicated by other people:\nset.seed(20150516)\n\n# take a random re-sample of the data that is the *same size*\nN &lt;- length(new_penguins_df$flipper_length_mm)\n\n# a random sample:\nsample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n [1] 184 192 198 195 195 176 188 183 184 193 199 198 184 190 198 195 195 199 193\n[20] 197 198 189 197 188 189 199 200 190 183 198 194 190 191 196 189 195 198 197\n[39] 191 184 198 180 195 186 193 193 191 195 190 198 189 181 197 196 182 200 188\n[58] 184 202 189 197 186 181 195 181 191 185 193 196 185 192 199 186 196 180 190\n[77] 190 195 197 193 191 181 195 190 186 189 192 187 190 195 195 182 172 194 181\n\n# number of bootstrap samples\nB_int &lt;- 10000\n\n# create a list of these thousands of samples \nbootstrapSamples_ls &lt;- map(\n  .x = 1:B_int,\n  .f = ~{\n    sample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n  }\n)\n\n# subset of the random samples \nbootstrapSamples_ls[1:3]\n[[1]]\n [1] 183 190 189 188 181 198 181 172 187 189 189 193 180 197 191 190 196 191 195\n[20] 181 193 190 190 186 188 195 190 197 198 190 180 198 194 188 195 191 203 199\n[39] 190 189 195 186 189 199 202 197 189 190 194 190 181 190 190 181 186 196 174\n[58] 185 174 202 191 184 181 184 193 190 190 190 191 196 189 195 195 198 193 190\n[77] 197 184 186 188 193 190 191 195 198 180 191 185 189 192 183 192 199 186 195\n\n[[2]]\n [1] 187 194 187 189 184 188 187 187 184 197 193 191 187 189 190 172 187 186 180\n[20] 193 191 195 195 180 184 189 197 191 187 186 186 187 184 188 190 193 198 190\n[39] 195 198 184 197 195 195 195 198 194 191 198 197 198 186 194 195 189 186 181\n[58] 180 191 180 191 193 196 191 202 191 187 181 199 172 181 191 195 195 194 198\n[77] 191 191 190 192 190 199 195 193 195 197 188 181 190 185 186 191 174 193 195\n\n[[3]]\n [1] 191 196 203 195 185 195 193 186 186 202 186 203 187 180 185 186 192 202 186\n[20] 192 200 195 184 185 195 193 199 190 189 185 181 181 188 197 181 190 188 185\n[39] 187 184 184 195 199 186 200 186 192 195 190 182 189 191 203 193 195 191 191\n[58] 199 195 198 187 191 195 190 190 187 189 192 186 199 193 190 187 181 190 191\n[77] 190 190 183 193 190 197 181 190 187 198 187 190 200 184 190 184 186 191 193"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-1",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-1",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.3 Building Confidence Intervals for Various Statistics: Example 1",
    "text": "3.3 Building Confidence Intervals for Various Statistics: Example 1\n\n\nCode\n\n# The Sample Mean\nbootMeans_num &lt;-\n  bootstrapSamples_ls %&gt;%\n  # the map_dbl() function takes in a list and returns an atomic vector of type\n  #   double (numeric)\n  map_dbl(mean)\n\n# a normally distributed histogram using the samples from bootstrapping\nhist(bootMeans_num)\n\n\n\n\n\n\n\n\n\nCode\n\n# 95% confidence interval?\nquantile(bootMeans_num, probs = c(0.025, 0.975))\n 2.5% 97.5% \n  189   191"
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-2",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#building-confidence-intervals-for-various-statistics-example-2",
    "title": "Bootstrap Confidence Intervals",
    "section": "3.4 Building Confidence Intervals for Various Statistics: Example 2",
    "text": "3.4 Building Confidence Intervals for Various Statistics: Example 2\nSource: https://www.geeksforgeeks.org/bootstrap-confidence-interval-with-r-programming/\n\n\nCode\n\n# Custom function to find correlation between the bill length and depth \ncorr.fun &lt;- function(data, idx) {\n  \n# vector of indices that the boot function uses\n  df &lt;- data[idx, ]\n\n# Find the spearman correlation between\n# the 3rd (length) and 5th (depth) columns of dataset\n  cor(df[, 3], df[, 4], method = 'spearman')\n}\n\n# Setting the seed for reproducability of results\nset.seed(42)\n\n# Calling the boot function with the dataset\nbootstrap &lt;- boot(iris, corr.fun, R = 1000)\n\n# Display the result of boot function\nbootstrap\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = iris, statistic = corr.fun, R = 1000)\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1*    0.938 -0.00272     0.00944\n\n# Plot the bootstrap sampling distribution using ggplot\nplot(bootstrap)\n\n\n\n\n\n\n\n\n\nCode\n\n# Function to find the bootstrap CI\nboot.ci(\n  boot.out = bootstrap,\n    type = \"perc\"\n)\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.914,  0.952 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html",
    "href": "lessons_original/03_two_sample_ttest.html",
    "title": "Two sample t test",
    "section": "",
    "text": "This is also called the independent sample t test. It is used to see whether the unknown population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If you have more than two groups then analysis of variance (ANOVA) will be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test or a permutation test.(cressie1986?).\nThere are two types of t tests, the first being the Student’s t test, which assumes the variance of the two groups is equal, the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.\n\n\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n\n\n\n\n(H_0): the mean of group A (m_A) is equal to the mean of group B (m_B)- two tailed test,\n(H_0): (m_A)\\ge (m_B)- one tailed test.\n(H_0): (m_A)\\le (m_B)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n(H_1): (m_A)\\neq(m_B)- two tailed test.\n(H_1): (m_A)&lt;(m_B)- one tailed test.\n(H_1): (m_A)&gt; (m_B)- one tailed test.\n\n\n\n\nFor the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\nt ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\nm_A and m_B are the mean values of A and B,\nn_A and n_B are the seize of group A and B,\nS^2 is the estimator for the pooled variance,\nwith the degrees of freedom (df) = n_A + n_B - 2,\nand S^2 is calculated as follows:\nS^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.(fralick?)\n\n\n\n\n\n\n\ntidyverse: data manipulation and visualization.\nrstatix: providing pipe friendly R functions for easy statistical analyses.\ncar: providing variance tests.\n\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n\n\n\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.(cortez2008?)\n\n\nCode\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n\n\nCode\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n\nCode\n# check the data\nglimpse(stu_math)\n\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\nH_0: There is no statistical difference between the final grades between boys and girls.\nH_1: There is a statistically significant difference in the final grades between the two groups.\n\n\nCode\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n\nCode\n# summarizing our data\n summary(math)\n\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n\nCode\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n\nCode\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n\nCode\n# plotting distribution for females\nplotNormalHistogram(\n  female, breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n\nCode\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n\nCode\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n\nCode\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n\n[1] 9.966346\n\n\nCode\nmean(math$G3[math$sex==\"M\"])\n\n\n[1] 10.91444\n\n\nCode\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n\n[1] 11.20541\n\n\nCode\nmean(math2$G3[math2$sex==\"M\"])\n\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n\nCode\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n\nCode\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n\n\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s t test.\n\n\n\nSince the default is the Welch t test we use the \\color{blue}{\\text{var.eqaul = TRUE }} code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n\nCode\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\n\nCode\nstat.test$statistic\n\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n\nCode\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOverall, N = 3571\nF, N = 1851\nM, N = 1721\nDifference2\n95% CI2,3\np-value2\n\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n1 Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p = 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96 or greater).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\nd= {m_A-m_B \\over SD_pooled}\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n\nCode\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\color{blue}{\\text{alternative =}} option to determine if one group is \\color{blue}{\\text{\"less\"}} or \\color{blue}{\\text{\"greater\"}}. For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n\nCode\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\color{blue}{\\text{paired= TRUE}} to our t_test() to run the analysis for matched pairs data.\n\n\n\n\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#assumptions",
    "href": "lessons_original/03_two_sample_ttest.html#assumptions",
    "title": "Two sample t test",
    "section": "",
    "text": "Measurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#hypotheses",
    "href": "lessons_original/03_two_sample_ttest.html#hypotheses",
    "title": "Two sample t test",
    "section": "",
    "text": "(H_0): the mean of group A (m_A) is equal to the mean of group B (m_B)- two tailed test,\n(H_0): (m_A)\\ge (m_B)- one tailed test.\n(H_0): (m_A)\\le (m_B)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n(H_1): (m_A)\\neq(m_B)- two tailed test.\n(H_1): (m_A)&lt;(m_B)- one tailed test.\n(H_1): (m_A)&gt; (m_B)- one tailed test."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#statistical-hypotheses-formula",
    "href": "lessons_original/03_two_sample_ttest.html#statistical-hypotheses-formula",
    "title": "Two sample t test",
    "section": "",
    "text": "For the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\nt ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\nm_A and m_B are the mean values of A and B,\nn_A and n_B are the seize of group A and B,\nS^2 is the estimator for the pooled variance,\nwith the degrees of freedom (df) = n_A + n_B - 2,\nand S^2 is calculated as follows:\nS^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.(fralick?)"
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#example",
    "href": "lessons_original/03_two_sample_ttest.html#example",
    "title": "Two sample t test",
    "section": "",
    "text": "tidyverse: data manipulation and visualization.\nrstatix: providing pipe friendly R functions for easy statistical analyses.\ncar: providing variance tests.\n\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n\n\n\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.(cortez2008?)\n\n\nCode\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n\n\nCode\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n\nCode\n# check the data\nglimpse(stu_math)\n\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\nH_0: There is no statistical difference between the final grades between boys and girls.\nH_1: There is a statistically significant difference in the final grades between the two groups.\n\n\nCode\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n\nCode\n# summarizing our data\n summary(math)\n\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n\nCode\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n\nCode\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n\nCode\n# plotting distribution for females\nplotNormalHistogram(\n  female, breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n\nCode\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n\nCode\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n\nCode\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n\n[1] 9.966346\n\n\nCode\nmean(math$G3[math$sex==\"M\"])\n\n\n[1] 10.91444\n\n\nCode\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n\n[1] 11.20541\n\n\nCode\nmean(math2$G3[math2$sex==\"M\"])\n\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n\nCode\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n\nCode\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n\n\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s t test.\n\n\n\nSince the default is the Welch t test we use the \\color{blue}{\\text{var.eqaul = TRUE }} code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n\nCode\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\n\nCode\nstat.test$statistic\n\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n\nCode\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOverall, N = 3571\nF, N = 1851\nM, N = 1721\nDifference2\n95% CI2,3\np-value2\n\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n1 Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p = 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96 or greater).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\nd= {m_A-m_B \\over SD_pooled}\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n\nCode\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\color{blue}{\\text{alternative =}} option to determine if one group is \\color{blue}{\\text{\"less\"}} or \\color{blue}{\\text{\"greater\"}}. For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n\nCode\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\color{blue}{\\text{paired= TRUE}} to our t_test() to run the analysis for matched pairs data."
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#conclusion",
    "href": "lessons_original/03_two_sample_ttest.html#conclusion",
    "title": "Two sample t test",
    "section": "",
    "text": "This article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results."
  },
  {
    "objectID": "lessons/00_lesson_template.html",
    "href": "lessons/00_lesson_template.html",
    "title": "The Method",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\n1 Introduction to &lt;the method&gt;\n\n\n2 Mathematical definition of &lt;the method&gt;\n\n\n3 Data source and description\n\n\n4 Cleaning the data to create a model data frame\n\n\n5 Assumptions of &lt;the method&gt;\n\n\n6 Checking the assumptions with plots\n\n\n7 Code to run &lt;the method&gt;\n\n\n8 Code output\nNOTE: this section will be created automatically by the Quarto document. You should not create a section specifically for this. When you run the code in the previous section, you will get the output automatically.\n\n\n9 Brief interpretation of the output"
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html",
    "href": "lessons/02_z-test_one_prop.html",
    "title": "Z-Test for One Proportion",
    "section": "",
    "text": "The one-sample \\(Z\\)-test is used to compare a sample proportion to a population proportion."
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#independence-and-randomness",
    "href": "lessons/02_z-test_one_prop.html#independence-and-randomness",
    "title": "Z-Test for One Proportion",
    "section": "6.1 Independence and Randomness",
    "text": "6.1 Independence and Randomness\nBecause the samples were collected at random via an FDA approved clinical trial protocol, we assume that all the participants were randomly selected and are independent of each other."
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#extreme-proportions",
    "href": "lessons/02_z-test_one_prop.html#extreme-proportions",
    "title": "Z-Test for One Proportion",
    "section": "6.2 “Extreme” Proportions",
    "text": "6.2 “Extreme” Proportions\nAccording to Ling et al. (2020), the 12-month abstinence proportion of all 533 participants in their study was 40.5 percent. As we can see here, our abstinence rates are 39.4. Neither these proportions are smaller than 5% or greater than 95%.\n\n(pExpected &lt;- 0.508 * (425/533))\n\n[1] 0.4050657\n\n# Count the number of TRUE values\n(nAbstinent &lt;- sum(outcomesCTN0094$kosten1993_isAbs))\n\n[1] 1402"
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#type-and-counts-of-data",
    "href": "lessons/02_z-test_one_prop.html#type-and-counts-of-data",
    "title": "Z-Test for One Proportion",
    "section": "6.3 Type and Counts of Data",
    "text": "6.3 Type and Counts of Data\nWe observe binary data, and we see at least 10 successes and at least 10 failures."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html",
    "href": "lessons_original/04_anova_random_intercept.html",
    "title": "Random Intercept Model",
    "section": "",
    "text": "Code\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#libraries-used",
    "href": "lessons_original/04_anova_random_intercept.html#libraries-used",
    "title": "Random Intercept Model",
    "section": "",
    "text": "Code\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#what-is-a-random-intercept-model",
    "href": "lessons_original/04_anova_random_intercept.html#what-is-a-random-intercept-model",
    "title": "Random Intercept Model",
    "section": "2 What is a Random Intercept model",
    "text": "2 What is a Random Intercept model\nBefore talking about a random intercept model, let’s understand why they are necessary and important in the real world by discussing a variance component model first. This will make sense as we go along in this lecture.\n\n2.1 Variance component model\nWe are familiar with a fixed level of a factor or variable. Which means that the factor level in an experiment is the only thing we are interested. For example, let’s say we are interested in measuring the difference in resistance resulting from putting identical resistors to three different temperatures for a period of 24 hours. Let’s say we have three different groups, and each of these three different groups have a sample size of 5. So each of the three treatment groups was replicated 5 times.\n\n\n\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\n6.9\n8.3\n8.0\n\n\n5.4\n6.8\n10.5\n\n\n5.8\n7.8\n8.1\n\n\n4.6\n9.2\n6.9\n\n\n4.0\n6.5\n9.3\n\n\nmean\nmean\nmean\n\n\n5.34\n7.72\n8.56\n\n\n\nIn this example, the level of the temperature is considered fixed meaning, the three temperatures were the only ones that we are interested in. This is called a fixed effects model.\na fixed effect model is a statistical model in which the parameters are fixed or non-random. This can also be referred to a regression model, in which group mean are “fixed” (non-random) or in simpler, terms something that is “fixed” in analysis is constant like sex assigned at birth or ethnicity.\n y_i = \\beta_0 + X_i\\beta_i + \\alpha_u + \\epsilon_i \nNow, let’s say we want to look at different levels of factors that were chosen because of random sampling, like number of operators working that day, lot batches, days etc. So in this case we are now regarding factors not related to themselves (variables) but we are now trying to represent all possible levels that these factors may take, the appropriate model is now a random effects model.\nfitting these random effects models are important because we want to obtain estimates of different contributions that experimental factors make to the variability of our data! (we can represent this as the variance) this is what is called variance component\n\n\n2.2 Why this is relevant\nWell, a variance component model helps us see how much variance in our response at the different levels. But what if you are interested in seeing the effects of the explanatory variables? Or, what if your observations are NOT randomly sampled from simple random sample but instead from a cluster or a multi-level sampling design? Random intercept models or random effects models are important.\n\n\n2.3 Example 1: School level data\nLet’s say we have some data on exam results of students within a school and we use a variance component model and see that 15% of the variance is at the school level. Like for example, differing school districts, differing school policies etc. However, is it fair to really say that 15% of the variance in example scores is caused by schools? you could also say that maybe that part of the variance could be cause by the students being different themselves as well before taking the exam.\nIn this case, it might be important to control for the previous exams the students took, so you can look at the variance that is due to the things that happened when the students were at that school."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#fitting-a-single-level-regression-model",
    "href": "lessons_original/04_anova_random_intercept.html#fitting-a-single-level-regression-model",
    "title": "Random Intercept Model",
    "section": "3 Fitting a single-level regression model",
    "text": "3 Fitting a single-level regression model\nwhen we want to control for something (like previous exams students took) we can fit a single-level regression model that looks something like this:\ny_1 = \\beta_0 + \\beta_1x_i + e_i where\n\ny_1 is your dependent variable\n\\beta_0 is your intercept and\n\\beta_1 is your slope parameter (which is also your slope treatment effect).\nand e_i is your random error\n\nWhen you have clustered data fitting this model causes problems. Clustered data is data where you observation or participants are related. Like exam results for students within a school, height of children within a family etc.\nif we try to fit this clustered data:\n\nour standard errors will be wrong.\nthis single level data model doesn’t show up how much variation is at the school level and how much much of the variation is at the student level.\n\nSo fitting this type of data in this regression we wont know how much of an effect the school level has on the exam score, after controlling for the previous score."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#solution-fitting-a-random-intercept-model",
    "href": "lessons_original/04_anova_random_intercept.html#solution-fitting-a-random-intercept-model",
    "title": "Random Intercept Model",
    "section": "4 Solution: Fitting a Random Intercept model",
    "text": "4 Solution: Fitting a Random Intercept model\nSo what we can do is combine the variance component and single-level regression model to build a random intercept model. So this random intercept model has 2 random terms. the level one random term: e_{ij} \\sim N(0, \\sigma_e^2) and the N(0, \\sigma_u^2) and has two parts:\n\na fixed part\na random part\n\n y_{ij} = \\overbrace{\\beta_0 + \\beta_1X_{ij}}^{\\text{fixed part}} + \\underbrace{u_j + e_{ij}}_{\\text{random part}} where the fixed parts includes our parameters that we estimate as our coefficients, and the random part is the parameter we estimate as the variance e_{ij} \\sim N(0, \\sigma_e^2) and the N(0, \\sigma_u^2) and these are allowed to vary and u_j and e_{ij} are normally distributed.\nwhere:\n\ny_{ij} is your dependent variable at i individual and j level\nN(0, \\sigma_u^2) is the measurement at the school level\nand e_{ij} \\sim N(0, \\sigma_e^2) is the measurement at the student level\nand i subscript is for the students\nand j is the school subscript\n\nwe can also write this equation like so:\n Y_{ij} = \\mu + b_i + \\varepsilon_{ij} \nwhere\n\nY_{ij} is your dependent outcome of intested for a subject i at school j\n\\mu is the population average mean\nb_i is the random students effects (you have a random effect for every student)\n\\varepsilon_{ij} is your random error."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#final-key-points",
    "href": "lessons_original/04_anova_random_intercept.html#final-key-points",
    "title": "Random Intercept Model",
    "section": "5 Final key points",
    "text": "5 Final key points\n\nrandom intercept models are used for answering questions about clustered data, and at different levels. For example, what is the relationship between exam scores at 11 and at age 16? how much variation is there between students progress from 11 to 16 at the school level?\nb_i is the error associated with the students.\n\\varepsilon_{ij} is the random error.\nfor a random intercept model, each individual will have a random intercept, but the sample slope."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#assumptions-of-a-random-effects-model",
    "href": "lessons_original/04_anova_random_intercept.html#assumptions-of-a-random-effects-model",
    "title": "Random Intercept Model",
    "section": "6 Assumptions of a random effects model:",
    "text": "6 Assumptions of a random effects model:\n\nunobserved cluster effects is not correlated with observed variables (all u_{ij} terms are not correlated with the your predictors.)\nthe within and between effects are the same.\nyour error term is independent with your constant term.\nyou have homoscedasticity\nb_i and \\varepsilon are independent of each other"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#hypothesis-of-a-random-effects-model",
    "href": "lessons_original/04_anova_random_intercept.html#hypothesis-of-a-random-effects-model",
    "title": "Random Intercept Model",
    "section": "7 hypothesis of a random effects model:",
    "text": "7 hypothesis of a random effects model:\nhypothesis testing for a random effects model runs as follows:\n H_0: \\sigma^2_u = 0 H_1: \\sigma^2_u \\not = 0 the null hypothesis states that if \\sigma^2_u is true, then the random component is not needed in this model. so you can fit a single level regression model. to do this, you would can do a likelihood ratio test comparing the two model to see if sigma is significant. In other words, seeing if there is no difference in intercepts. If there is NO difference in intercepts (or the slopes are similar), then a random intercept model or random component is not needed."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "href": "lessons_original/04_anova_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "title": "Random Intercept Model",
    "section": "8 Example 2: Planktonic larval duration (PLD)",
    "text": "8 Example 2: Planktonic larval duration (PLD)\nthis is example is from O’Connor et al (2007). A brief intro on this study, temperature is important for the development of organisms. In marine species, temperature is very important and linked to growth. So the time spent as a planktonic larvae can have associations on mortality and regulation on the species. Previous research has looked at the association between species comparison but not within species comparisons. What if we are interest in within and between species variation?\n\n8.1 load PLD data\n\n\nCode\nPLD &lt;- read_table(\"../data/04_PLD.txt\")\n\n\nI am curious about the structure of this data and how it briefly looks.\n\n\nCode\n#strcuture \nstr(PLD)\n\n\nspc_tbl_ [214 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ phylum : chr [1:214] \"Annelida\" \"Annelida\" \"Annelida\" \"Annelida\" ...\n $ species: chr [1:214] \"Circeus.spirillum\" \"Circeus.spirillum\" \"Circeus.spirillum\" \"Hydroides.elegans\" ...\n $ D.mode : chr [1:214] \"L\" \"L\" \"L\" \"P\" ...\n $ temp   : num [1:214] 5 10 15 15 20 25 30 5 10 17 ...\n $ pld    : num [1:214] 16 16 4 8 6.5 5 3.2 15 9.5 4.5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   phylum = col_character(),\n  ..   species = col_character(),\n  ..   D.mode = col_character(),\n  ..   temp = col_double(),\n  ..   pld = col_double()\n  .. )\n\n\nCode\n# just the top - seeing how it looks\nhead(PLD)\n\n\n# A tibble: 6 × 5\n  phylum   species           D.mode  temp   pld\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Annelida Circeus.spirillum L          5  16  \n2 Annelida Circeus.spirillum L         10  16  \n3 Annelida Circeus.spirillum L         15   4  \n4 Annelida Hydroides.elegans P         15   8  \n5 Annelida Hydroides.elegans P         20   6.5\n6 Annelida Hydroides.elegans P         25   5  \n\n\nCode\n# brief summary\nsummary(PLD)\n\n\n    phylum            species             D.mode               temp      \n Length:214         Length:214         Length:214         Min.   : 2.50  \n Class :character   Class :character   Class :character   1st Qu.:12.28  \n Mode  :character   Mode  :character   Mode  :character   Median :20.00  \n                                                          Mean   :18.59  \n                                                          3rd Qu.:25.00  \n                                                          Max.   :32.00  \n      pld        \n Min.   :  1.00  \n 1st Qu.: 11.00  \n Median : 19.30  \n Mean   : 26.28  \n 3rd Qu.: 33.45  \n Max.   :129.00  \n\n\nI am curious about how this would look just plotting the variable pld or planktonic larvae duration and the temperature. So i am interested in seeing how the temperature is associated with their their survival duration.\n\n\nCode\n# how to plot in base R \nplot(pld ~ temp, data = PLD,\n     xlab = \"temperature (C)\",\n     ylab = \"PLD survival (Days)\",\n     pch = 16)\n# add lm line in base R\nabline(lm(pld ~ temp, data = PLD))\n\n\n\n\n\n\n\n\n\nyou can also do this in ggplot plot like so:\n\n\nCode\nggplot(data = PLD) +\n  aes(y = pld, x = temp) +\n  stat_smooth(method = \"lm\") +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n8.2 Fit first model: Linear model\nLet’s first fit a linear model and check any assumptions. Why are we fitting a linear model first? It might be important to check the standard error of this model and compare to the next model, that might be a better fit later on.\n\n\nCode\n# fitting linear model first \n\nLinearModel_1 &lt;- lm(pld ~ temp, data = PLD)\n\nsummary(LinearModel_1)\n\n\n\nCall:\nlm(formula = pld ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.158 -11.351  -0.430   7.684  93.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.8390     3.7954  14.449  &lt; 2e-16 ***\ntemp         -1.5361     0.1899  -8.087 4.65e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.36 on 212 degrees of freedom\nMultiple R-squared:  0.2358,    Adjusted R-squared:  0.2322 \nF-statistic:  65.4 on 1 and 212 DF,  p-value: 4.652e-14\n\n\nWe can fit this output in a gtsummary to make it nicer looking:\n\n\nCode\nLinearModel_1 |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ntemp\n-1.5\n-1.9, -1.2\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nI am interested in checking out visually, the equal variance (homoscedasticity) and so i will plot a a base residual graph:\n\n\nCode\n# better to do a scatter plot \nLinearModel_res &lt;- resid(LinearModel_1)\n\n# plot residual \nplot(PLD$temp, LinearModel_res,\n     ylab = \"residuals\",\n     xlab = \"temp\",\n     main = \"Residual graph\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\n\nJust upon visual observation, it seems like this assumption may be violated so i think it might be important to do some transformations.\n\n\n8.3 Log transformation\n\n\nCode\nLinearMode_2Log &lt;- lm(log(pld) ~ log(temp), data= PLD)\n\nsummary(LinearMode_2Log)\n\n\n\nCall:\nlm(formula = log(pld) ~ log(temp), data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0768 -0.3956  0.1802  0.5461  1.9656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.6946     0.3128  15.011  &lt; 2e-16 ***\nlog(temp)    -0.6308     0.1093  -5.771 2.77e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8236 on 212 degrees of freedom\nMultiple R-squared:  0.1358,    Adjusted R-squared:  0.1317 \nF-statistic: 33.31 on 1 and 212 DF,  p-value: 2.767e-08\n\n\n\n\n8.4 residual of new log transformed graph\n\n\nCode\n# better to do a scatter plot \nLinearModel2_res &lt;- resid(LinearMode_2Log)\n\n# plot residual \nplot(PLD$temp, LinearModel2_res,\n     ylab = \"residuals(log)\",\n     xlab = \"temp(log)\",\n     main = \"Residual graph (log transformation)\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\n\nA bit better! Now i kinda want to see the original plot i plotted with PLD and temperature:\n\n\nCode\nplot(log(pld) ~ log(temp), data = PLD,\n     xlab = \"Temperature in C\",\n     ylab = \"PLD in days\")\nabline(LinearMode_2Log)\n\n\n\n\n\n\n\n\n\nin ggplot you can use the facet_wrap() function to separate by phylum:\n\n\nCode\nggplot(data = PLD) +\n  aes(x = log(temp), y = log(pld)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  facet_wrap(~phylum) +\n  theme_classic()"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "href": "lessons_original/04_anova_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "title": "Random Intercept Model",
    "section": "9 Fitting a random intercept model (random intercept, same slope)",
    "text": "9 Fitting a random intercept model (random intercept, same slope)\nI am interested in seeing if the overall temperature and the PLD relationship is similar among species, but not the same. We are interested in plotting a mixed effects model with a random intercept but fixed/same slope. I am only interested in the species-specific plot for now with the phylum Mollusca.\n\n\nCode\n# filter to only mollusca\n\nMollusca_subset &lt;- \n  PLD |&gt; \n  filter(phylum == \"Mollusca\")\n\nggplot(data = Mollusca_subset) +\n  aes(x = log(pld), y = log(temp)) +\n  geom_point() +\n  labs(x = \"Log(temperature)\",\n       y = \"Log(PLD)\") + \n  stat_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, fullrange = TRUE) +\n  theme_classic() +\n  facet_wrap(~ species)\n\n\n\n\n\n\n\n\n\n\n9.1 fitting model\nWe can use the library lme4 to fit a model of a linear regression with a random effect\n\n\nCode\n# creating log -transformed variables \nMollusca_subset$log_pld &lt;- log(Mollusca_subset$pld)\nMollusca_subset$log_temp &lt;- log(Mollusca_subset$temp)\n\n# mixed model with random intercept only \nRandIntModel_Mollusca &lt;- lmer(log_pld ~ log_temp + (1 | species), data = Mollusca_subset)\n\nsummary(RandIntModel_Mollusca)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\n\nthe fixed effects: section in the output is the estimate for the fixed slope, and the grand mean of the intercept. in the fixed effects section the intercept here is the random effect, and you can see this\nin the Random effects section in this output, in this case our random effect was specified in the the Names sections, which tells us the parameter is the intercept.\nthe Group section tells us we have a random intercept for each species. We also have the variance and standard deviation for the random effects (as well as the residuals)\n\nSince we have a random effect at the individual level, we can subset this section out so it is clear to see that these organism will have a random intercept and fixed slope.\n\n\nCode\n# subset of the coefficients for random intercept and fixed slop\n\ncoef(RandIntModel_Mollusca)$species\n\n\n                      (Intercept) log_temp\nChlamys.hastata          7.612748 -1.51751\nCrassostrea.virginica    7.592515 -1.51751\nCrepidula.fornicata.     8.045609 -1.51751\nCrepidula.plana          8.063220 -1.51751\nHaliotis.asinina         5.807247 -1.51751\nHaliotis.fulgens         6.083069 -1.51751\nHaliotis.sorenseni.      6.688210 -1.51751\nMactra.solidissima       7.589660 -1.51751\nMopalia.muscosa          7.061086 -1.51751\nMytilus.edulis           7.141801 -1.51751\nNassarius.obsoletus      7.370159 -1.51751\nOstrea.lurida            6.967626 -1.51751\nPerna.viridis            8.144314 -1.51751\nStrombus.gigas           8.048942 -1.51751\nTivela.mactroides        7.677603 -1.51751\nTonicella.lineata        4.871674 -1.51751\n\n\nSo now we can see that in the Mollusca subset, we have all random intercepts for individual specifies, but the same slope.\n\n\n9.2 Inter class correlation coefficient (ICC)\nfor a random intercept model, we can run a diagnostic called the inter-class correlation coefficient (ICC), which lets us know how much group specific information is available for the random effect. this is somewhat similar to the ANOVA, in which it looks at the variability within groups compared to the variability between groups. Low ICC means that observation within group don’t really cluster.\n ICC = {\\sigma^2_{\\alpha} \\over \\sigma^2 + \\sigma^2_\\alpha} \n\n\nCode\n# creating data frame\nvar &lt;- as.data.frame(VarCorr(RandIntModel_Mollusca))\n\n#check our data frame\nvar\n\n\n       grp        var1 var2       vcov     sdcor\n1  species (Intercept) &lt;NA&gt; 0.86457141 0.9298233\n2 Residual        &lt;NA&gt; &lt;NA&gt; 0.03309183 0.1819116\n\n\nCode\n#ICC equation\nICC &lt;- var$vcov[1] / (var$vcov[1] + var$vcov[2])\n\n# ICC value \nICC\n\n\n[1] 0.9631356\n\n\nIn our model, the \\sigma^2_{\\alpha} is 0.8645 (also the vcov part) and the \\sigma^2 is 0.033. so once we do the mathematics we get 0.9631. Which is the proportion of the total variance in Y that is accounted for by clustering. This is a high value and therefore, suggesting we have within-group variability, so it might be good we are running this random effects model."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#interpretation-of-results",
    "href": "lessons_original/04_anova_random_intercept.html#interpretation-of-results",
    "title": "Random Intercept Model",
    "section": "10 Interpretation of results",
    "text": "10 Interpretation of results\n\n\nCode\nsummary(RandIntModel_Mollusca)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\nInterpretation: Summary of this PLD data includes information about the random effects. Here we can see that the column groups shows the random effect variable. in the name section, you can see that the random effect is our intercept. so we have the variation due to the species. in the Residuals section, this is the variation that cannot be explained by the model (the error). As you will notice our Standard error is smaller compared to the ordinary regression we ran in the previous one. Standard error for this model is 0.15 and the previous standard error for the first model we ran was 0.18.\nSo 0.86 / 0.86 + 0.03 = 0.96 , so the difference between between species can explain 96% of the variance that is is left over after the variance is explained by our fixed effect. since the random effects of the species explain most.\nthere is a very long description on the why the lmer() function doesn’t include the p-value that can be found here.\ninterpretation of temp variable for the fixed part, we can interpret this parameter the same as a single-level regression model, so \\beta_1 is the increase/decrease in response for 1 unit increase/decrease in x. In other words, for one unit increase in the degrees of temperature, there is a -1.5 decrease in Planktonic larval duration. (or, as the temperature increase, the plankton duration is lower.)"
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#conclusion",
    "href": "lessons_original/04_anova_random_intercept.html#conclusion",
    "title": "Random Intercept Model",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nIn this lecture you learned about the importance of a random intercept model, when it is appropriate to use a random intercept model, the difference between an ordinary single-level model, and a random intercept model, the assumptions of the random intercept model, hypothesis testing for the variation, the Interclass correlation coefficient (ICC) and finally, how to interpret results from the fixed part and the random part of a random intercept model."
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#references",
    "href": "lessons_original/04_anova_random_intercept.html#references",
    "title": "Random Intercept Model",
    "section": "12 References",
    "text": "12 References\n\nAbedin, Jaynal, and Kishor Kumar Das. 2015. Data Manipulation with r. Packt Publishing Ltd.\nAnnesley, Thomas M. 2010. “Bars and Pies Make Better Desserts Than Figures.” Clinical Chemistry 56 (9): 1394–1400.\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\nBorer, Elizabeth T, Eric W Seabloom, Matthew B Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14.\nBorghi, John, Stephen Abrams, Daniella Lowenberg, Stephanie Simms, and John Chodacki. 2018. “Support Your Data: A Research Data Management Guide for Researchers.” Research Ideas and Outcomes 4: e26439.\nBroman, Karl W, and Kara H Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10.\nChamberlin, Thomas C. 1890. “The Method of Multiple Working Hypotheses.” Science 15 (366): 92–96.\nsupplementary material for Tom Snijders and Roel Bosker textbook - Shjders, T Bosker R, 1999. Multilevel analysis: an introduction to basic and advanced mltilevel modeling, London, Sage, including updates and corrections data set examples http://stat.gamma.rug.nl/multilevel.htm\nUniversity of Bristol, Random Intercept model, (2018). http://www.bristol.ac.uk/cmm/learning/videos/random-intercepts.html\nMidway, S. (2019). “Data Analysis in R.” https://bookdown.org/steve_midway/DAR/random-effects.html"
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html",
    "title": "Chi-Squared Independence",
    "section": "",
    "text": "The Chi-square test of independence (also known as the Pearson Chi-square test, or simply the Chi-square) is one of the most useful statistics for testing hypotheses when the variables are nominal. It is a non-parametric tool that does not require equality of variances among the study groups or homoscedasticity in the data.(mchugh2013?)\nBeing a non-parametric test tool, Chi-square test can be used when any one of the following conditions pertains to the data:(mchugh2013?)\n\nVariables are nominal or ordinal.\nThe frequency count is &gt;5 for more than 80% of the cells in a contingency table.\nThe sample sizes of the study groups may be of equal size or unequal size but samples do not have equal variance.\nThe original data were measured at an interval or ratio level, but violate one of the following assumptions of a parametric test:\n\nThe distribution of the data was seriously skewed or kurtotic.\nThe data violate the assumptions of equal variance or homoscedasticity.\nFor any reasons , the continuous data were collapsed into a small number of categories, and thus the data are no longer interval or ratio.(miller1982?)\n\n\n\nThe data in the cells should be frequencies, or counts of cases rather than % or some other transformation of the data.\nThe levels (or categories) of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.\nThe study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "title": "Chi-Squared Independence",
    "section": "",
    "text": "The Chi-square test of independence (also known as the Pearson Chi-square test, or simply the Chi-square) is one of the most useful statistics for testing hypotheses when the variables are nominal. It is a non-parametric tool that does not require equality of variances among the study groups or homoscedasticity in the data.(mchugh2013?)\nBeing a non-parametric test tool, Chi-square test can be used when any one of the following conditions pertains to the data:(mchugh2013?)\n\nVariables are nominal or ordinal.\nThe frequency count is &gt;5 for more than 80% of the cells in a contingency table.\nThe sample sizes of the study groups may be of equal size or unequal size but samples do not have equal variance.\nThe original data were measured at an interval or ratio level, but violate one of the following assumptions of a parametric test:\n\nThe distribution of the data was seriously skewed or kurtotic.\nThe data violate the assumptions of equal variance or homoscedasticity.\nFor any reasons , the continuous data were collapsed into a small number of categories, and thus the data are no longer interval or ratio.(miller1982?)\n\n\n\nThe data in the cells should be frequencies, or counts of cases rather than % or some other transformation of the data.\nThe levels (or categories) of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.\nThe study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "title": "Chi-Squared Independence",
    "section": "Hypotheses",
    "text": "Hypotheses\nNull Hypothesis (H0): Outcome variable is independent of type of exposure variables. There is no significant difference in the association of group A/B with outcome variable.\nAlternate Hypothesis: (H1) Outcome variable varies significantly depending upon the type of exposure variable. There is a significant difference in the association of group A/B with outcome variable."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "title": "Chi-Squared Independence",
    "section": "Chi-Squared Independence Test Equation",
    "text": "Chi-Squared Independence Test Equation\n\\(\\chi^2 = \\sum \\frac{{(O_{ij} - E_{ij})^2}}{{E_{ij}}}\\)\nWhere:\n\n\\(\\chi^2\\) The Chi-Squared test statistic.\n\\(\\sum\\chi^2\\) Formula instructions to sum all the cell Chi-square values.\n\\(O_{ij}\\) Observed (the actual frequency in each cell (i, j) of the contingency table.\n\\(E_{ij}\\) Expected frequency in cell (i, j) calculated below.\n\\(\\chi^2{i-j}\\) i-j is the correct notation to represent all the cells, from the first cell (i) to the last cell(j).\n\nCalculating Expected Value\n\\(E = \\frac{M{r} * M{c}}n\\)\nWhere:\n\n\\(E\\) represents the cell expected value,\n\\(M{r}\\) represents the row marginal for that cell,\n\\(M{c}\\) represents the column marginal for that cell, and\n\\(n\\) represents the total sample size.\n\n\nFormula Description\n\nThe first step in calculating a χ2 is to calculate the sum of each row, and the sum of each column. These sums are called the “marginals” and there are row marginal values and column marginal values. \nThe second step is to calculate the expected values for each cell. In the Chi-square statistic, the “expected” values represent an estimate of how the cases would be distributed if there were no effect of exposure variables.\nThen third step is to compute the \\(\\chi^2\\) with above formula."
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "title": "Chi-Squared Independence",
    "section": "Performing Chi-Squared Independence Test in R",
    "text": "Performing Chi-Squared Independence Test in R\nThe first step is to load the required packages that will allow us to conduct the test statistic.\n\n\nCode\n# install.package(\"openintro\")\n# install.package(\"gtsummary\")\n# install.packages(\"rstatix\")\n# install.packages(\"vcd\")\n# install.package(\"tidyverse\")\n\n\n\n\nCode\n\nlibrary(openintro)  # for data\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\nlibrary(gtsummary)  # for tables\nlibrary(vcd)        # for mosaic plot\nLoading required package: grid\nlibrary(rstatix)    # for post hoc tests\n\nAttaching package: 'rstatix'\nThe following object is masked from 'package:stats':\n\n    filter\nlibrary(tidyverse)  # for data wrangling and visualization\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks rstatix::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nData Source\nThe openintro package contains data sets used in open-source textbooks such as Introduction to Modern Statistics (1st Ed). (mineçetinkaya-rundel2023?) It is often used for teaching purposes and create examples for how to run various test statistics and functions using R. This package can be installed using the install.packages(\"openintro\") feature. You can also find more information about this package here.\nFor the purposes of this presentations we will be using the diabetes2 dataset found within this package.\nIn the data there are 699 diabetes patients. Each of the 699 patients in the experiment were randomized to one of the following treatments: (1) continued treatment with metformin (coded as met), (2) formin combined with rosiglitazone (coded as rosi), or or (3) a lifestyle-intervention program (coded as lifestyle).Three treatments were compared to test their relative efficacy (effectiveness) in treating Type 2 Diabetes in patients aged 10-17 who were being treated with metformin. The primary outcome was lack of glycemic control (or not); lacking glycemic control means the patient still needed insulin, which is not the preferred outcome for a patient.(todaystudygroup2012?)\n\n\nCode\n\nstr(diabetes2)\ntibble [699 × 2] (S3: tbl_df/tbl/data.frame)\n $ treatment: Factor w/ 3 levels \"lifestyle\",\"met\",..: 2 3 3 1 2 1 1 3 3 2 ...\n $ outcome  : Factor w/ 2 levels \"failure\",\"success\": 2 1 2 2 2 2 2 2 2 1 ...\nprint(diabetes2)\n# A tibble: 699 × 2\n   treatment outcome\n * &lt;fct&gt;     &lt;fct&gt;  \n 1 met       success\n 2 rosi      failure\n 3 rosi      success\n 4 lifestyle success\n 5 met       success\n 6 lifestyle success\n 7 lifestyle success\n 8 rosi      success\n 9 rosi      success\n10 met       failure\n# ℹ 689 more rows\n\n\n\n\nContingency Tables\nThe first step in a Chi-Squared Independence Test involves creating a contingency table that is used to calculate the expected frequencies for each variable. This will help us summarize the data and show the distribution of the variables. This is done using the table() function as seen in the code below.\n\n\nCode\n\n\n# Create the table\ndiabetes_table &lt;- table(\n  diabetes2$outcome,\n  diabetes2$treatment\n)\n\nprint(diabetes_table)\n         \n          lifestyle met rosi\n  failure       109 120   90\n  success       125 112  143\n\n\n\n\nMosaic Plots\nYou can also use a mosaic plot to visualize the data better. Our data is in interger format so we first need to reformat it into factor form. This can we done with the code below.\n\n\nCode\n\n#reformat treatment\ndiabetes2$treatment &lt;- \n  as.factor(diabetes2$treatment)\n\n#print\nhead(diabetes2$treatment)\n[1] met       rosi      rosi      lifestyle met       lifestyle\nLevels: lifestyle met rosi\n\n\n#recode treatment\ndiabetes2$treatment &lt;- \n  recode_factor(\n    diabetes2$treatment,\n            \"lifestyle\" = \"Lifestyle\",\n            \"met\" = \"Metform\",\n            \"rosi\" = \"Rosiglitazone Plus Metformin\"\n)\n\n#print\nhead(diabetes2$treatment)\n[1] Metform                      Rosiglitazone Plus Metformin\n[3] Rosiglitazone Plus Metformin Lifestyle                   \n[5] Metform                      Lifestyle                   \nLevels: Lifestyle Metform Rosiglitazone Plus Metformin\n\n#reformat outcome\ndiabetes2$outcome &lt;- \n  as.factor(diabetes2$outcome)\n\n#print\nhead(diabetes2$outcome)\n[1] success failure success success success success\nLevels: failure success\n\n\n#recode\ndiabetes2$outcome &lt;- \n  recode_factor(\n    diabetes2$outcome,\n            \"failure\" = \"Failure\",\n            \"success\" = \"Success\"\n)\n\n#print\nhead(diabetes2$outcome)\n[1] Success Failure Success Success Success Success\nLevels: Failure Success\n\n\nNext, we can create the mosaic plot using the mosaic() function.\n\n\nCode\n\n#Creating the mosaic plot\nmosaic(\n  ~ treatment + outcome, \n       data = diabetes2,\n          highlighting = \"outcome\", \n          highlighting_fill = c(\"lightgrey\", \"black\"),\n          main = \"Outcome of Interventions for Type 2 Diabetes\",\n          gp_varnames = gpar(fontsize = 14, fontface = 2),\n          gp_labels = gpar(fontsize = 10)\n)\n\n\n\n\n\n\n\n\n\n\n\nRunning the Chi-Squared Test\nThe next step is to run our actual test statistic. This is done using the chisq.test() function as seen in the code below.The correct argument is used to indicate whether to apply continuity correction when computing the test. We are setting this to TRUE since we are dealing with a 2x2 contingency table where we have two categorical variables, each with two levels.\n\n\nCode\n\nchi_sq_result &lt;- chisq.test(diabetes_table, \n                           correct = TRUE)\n\nprint(chi_sq_result)\n\n    Pearson's Chi-squared test\n\ndata:  diabetes_table\nX-squared = 8, df = 2, p-value = 0.02\n\n\n\nTabulating the chisq output in a publishable format using gt_summary\n\n\nCode\n\ntable1 &lt;-   \n  tbl_summary(\n    diabetes2,\n    by = treatment\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Results of Chi Square Test\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\n\nResults of Chi Square Test\n\n\n\n\n\n\n\n\n\nCharacteristic\nLifestyle, N = 2341\nMetform, N = 2321\nRosiglitazone Plus Metformin, N = 2331\np-value2\n\n\n\n\noutcome\n\n\n\n\n\n\n0.017\n\n\n    Failure\n109 (47%)\n120 (52%)\n90 (39%)\n\n\n\n\n    Success\n125 (53%)\n112 (48%)\n143 (61%)\n\n\n\n\n\n1 n (%)\n\n\n2 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\nTest Options\nThe Chi-Squared Independence test statistic has various options in R. A brief description of those options is summarized in the table below.\n\n\n\n\n\n\n\n\nOption\nDescription\nExample\n\n\n\n\nx\nA numeric vector or matrix. x and y can also both be factors.\nx &lt;- matrix(c(10, 20, 30, 40), nrow = 2)\n\n\ncorrect\nA logical indicating whether to apply continuity correction – This is done when the expected frequencies in the contingency table are small (&lt;5).\ncorrect &lt;- TRUE\n\n\np\nA vector of probabilities of the same length as x. An error is given if any entry of p is negative.\np &lt;- c(0.4, 0.6)\n\n\nrescale.p\nA logical scalar; if TRUE then p is rescaled (if necessary) to sum to 1. If rescale.p is FALSE, and p does not sum to 1, an error is given.\nrescale.p &lt;- FALSE\n\n\nsimulate.p.value\nA logical indicating whether to compute p-values by Monte Carlo simulation.\nsimulate.p.value &lt;- FALSE\n\n\nB\nAn integer specifying the number of replicates used in the Monte Carlo test.*\nB &lt;- 1000\n\n\n\n*The Monte Carlo test is a technique that involves simulation to estimate the p-value or test statistic for hypothesis testing. This is used when using complex models and exact p-values cannot be calculated or when the distribution assumptions are violated. (christianp.robert2010?)"
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "title": "Chi-Squared Independence",
    "section": "Interpretation",
    "text": "Interpretation\nOur results give us a chi-squared test statistic of 8.1645 with a p-value of 0.017. Since p value is smaller than critical p value (0.05), we have enough evidence to reject the null hypothesis and conclude that there is a strong association between the type of treatment on Type 2 diabetes. However, we dont’t know which treatment option is significantly different so we are going to do a post hoc test in below code chunk."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html",
    "href": "lessons_original/03_two_sample_mann_whitney.html",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "",
    "text": "Mann Whitney U test, also known as the Wilcoxon Rank-Sum test, is commonly used to compare the means or medians of two independent groups with the assumption that the at least one group is not normally distributed and when sample size is small.\n\nThe Welch U test should be used when there exists signs of skewness and variance of heterogeneity.fagerland2009?\n\nIt is useful for numerical/continuous variables.\n\nFor example, if researchers want to compare two different groups’ age or height (continuous variables) in a study with non-normally distributed data.sundjaja2023?\n\nWhen conducting this test, aside from reporting the p-value, the spread, and the shape of the data should be described.hart2001?\n\nOverall goal: Identify whether the distribution of two groups significantly differs.\n\n\nNull Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equalchiyau2020?\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly differentchiyau2020?\n\n\n\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups;(Male & Female).\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.\n\n\n\n\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#hypotheses",
    "href": "lessons_original/03_two_sample_mann_whitney.html#hypotheses",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "",
    "text": "Null Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equalchiyau2020?\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly differentchiyau2020?\n\n\n\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups;(Male & Female).\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.\n\n\n\n\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#data",
    "href": "lessons_original/03_two_sample_mann_whitney.html#data",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Data",
    "text": "Data\nIn this example, we will perform the Mann-Whitney U Test using wave 8 (2012-2013) data of a longitudinal epidemiological study titled Hispanic Established Populations For the Epidemiological Study of Elderly (HEPESE).\nThe HEPESE provides data on risk factors for mortality and morbidity in Mexican Americans in order to contrast how these factors operate differently in non-Hispanic White Americans, African Americans, and other major ethnic groups.The data is publicly available and can be obtained from the University of Michigan website.kyriakoss.markides2016?\nUsing this data, we want to explore whether there are significant gender differences in age when Type 2 diabetes mellitus (T2DM) is diagnosed. Type 2 diabetes is a chronic disease condition that has affected 37 million people living in the United States. Type 2 diabetes is the eighth leading cause of death and disability in US. Type 2 diabetes generally occurs among adults aged 45 or older although, young adults and children are also diagnosed with it these days. Diabetes and its complications are preventable when following proper lifestyles and timely medications. 1 in 5 of US people don’t know they have diabetes.national2020?\nResearch has shown that men are more likely to develop type 2 diabetes while women are more likely to experience complications, including heart and kidney disease.meissner2021?\nIn this report, we want to test whether there are significant differences in age at which diabetes is diagnosed among males and females.\nDependent Response Variable\nageAtDx = Age_Diagnosed = Age at which diabetes is diagnosed.\nIndependent Variable\nisMale = Gender\nResearch Question:\nDoes the age at which diabetes is diagnosed significantly differ among Men and Women?\nNull Hypothesis (H0): Mean rank of age at which diabetes is diagnosed is equal among men and women.\nAlternate Hypothesis (H1): Mean rank of age at which diabetes is diagnosed is not equal among men and women."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#packages",
    "href": "lessons_original/03_two_sample_mann_whitney.html#packages",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Packages",
    "text": "Packages\n\ngmodels: It helps to compute and display confidence intervals (CI) for model estimates.warnes2022?\nDescTools: It provides tools for basic statistics e.g. to compute Median CI for an efficient data description.andrisignorell2023?\nggplot2: It helps to create Boxplots.\nqqplotr: It helps to create QQ plot.\ndplyr: It is used to manipulate data and provide summary statistics.\nhaven: It helps to import spss data into r.\nDependencies = TRUE : It indicates that while installing packages, it must also install all dependencies of the specified package.\n\n\n\nCode\n# install.packages(\"gmodels\", dependencies = TRUE)\n# install.packages(\"car\", dependencies = TRUE)\n# install.packages(\"DescTools\", dependencies = TRUE)\n# install.packages(\"ggplot2\", dependencies = TRUE)\n# install.packages(\"qqplotr\", dependencies = TRUE)\n# install.packages(\"gtsummary\", dependencies = TRUE)\n\n\nLoading Library\n\n\nCode\n\nsuppressPackageStartupMessages(library(haven))\nsuppressPackageStartupMessages(library(ggpubr))\nsuppressPackageStartupMessages(library(gmodels))\nsuppressPackageStartupMessages(library(DescTools))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(qqplotr))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyverse))\n\n\nData Importing\n\n\nCode\n\n# Mann_W_U &lt;- read_sav(\"data\\\\36578-0001-Data.sav\")\nMann_W_U &lt;- read_sav(\"../data/03_HEPESE_synthetic_20240510.csv\")\nError: Failed to parse /Users/gabrielodom/Documents/GitHub/PHC6099_rBiostat/data/03_HEPESE_synthetic_20240510.csv: Invalid file, or file has unsupported features."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#data-exploration",
    "href": "lessons_original/03_two_sample_mann_whitney.html#data-exploration",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n\nCode\n# str(Mann_W_U)\nstr(Mann_W_U$isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\nstr(Mann_W_U$ageAtDx)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nAfter inspecting the data, we found that values of our dependent and independent variable values are in character form. We want them to be numerical and categorical, respectively. First, we will convert dependent variable into numerical form and our independent variable into categorical. Next, we will recode the factors as male and female. Also for ease, we will rename our dependent and independent variable.\n\n\nCode\n\n# convert to number and factor\nMann_W_U$ageAtDx &lt;- as.numeric(Mann_W_U$ageAtDx)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\nclass(Mann_W_U$ageAtDx)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nMann_W_U$isMale &lt;- as_factor(Mann_W_U$isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\nclass(Mann_W_U$isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nThe next step is to calculate some of the descriptive data to give us a better idea of the data that we are dealing with. This can be done using the summarise function.\nDescriptive Data\n\n\nCode\nDes &lt;- \n Mann_W_U %&gt;% \n select(isMale, ageAtDx) %&gt;% \n group_by(isMale) %&gt;%\n summarise(\n   n = n(),\n   mean = mean(ageAtDx, na.rm = TRUE),\n   sd = sd(ageAtDx, na.rm = TRUE),\n   stderr = sd/sqrt(n),\n   LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,\n   UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,\n   median = median(ageAtDx, na.rm = TRUE),\n   min = min(ageAtDx, na.rm = TRUE), \n   max = max(ageAtDx, na.rm = TRUE),\n   IQR = IQR(ageAtDx, na.rm = TRUE),\n   LCLmed = MedianCI(ageAtDx, na.rm = TRUE)[2],\n   UCLmed = MedianCI(ageAtDx, na.rm = TRUE)[3]\n )\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nDes\nError in eval(expr, envir, enclos): object 'Des' not found\n\n\n\nn: The number of observations for each gender.\nmean: The mean age when diabetes is diagnosed for each gender.\nsd: The standard deviation of each gender.\nstderr: The standard error of each gender level.  That is the standard deviation / sqrt (n).\nLCL, UCL: The upper and lower confidence intervals of the mean.  This values indicates the range at which we can be 95% certain that the true mean falls between the lower and upper values specified for each gender group assuming a normal distribution. \nmedian: The median value for each gender.\nmin, max: The minimum and maximum value for each gender.\nIQR: The interquartile range of each gender. That is the 75th percentile –  25th percentile.\nLCLmed, UCLmed: The 95% confidence interval for the median.\n\nVisual exploration of data\nThe next step is to visualize the data. This can be done using different functions under the ggplot package.\n1) Box plot\n\n\nCode\nggplot(\n Mann_W_U, \n aes(\n   x = isMale, \n   y = ageAtDx, \n   fill = isMale\n )\n) +\n stat_boxplot(\n   geom = \"errorbar\", \n   width = 0.5\n ) +\n geom_boxplot(\n   fill = \"light blue\"\n ) + \n stat_summary(\n   fun.y = mean, \n   geom = \"point\", \n   shape = 10, \n   size = 3.5, \n   color = \"black\"\n ) + \n ggtitle(\n   \"Boxplot of Gender\"\n ) + \n theme_bw() + \n theme(\n   legend.position = \"none\"\n )\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\n2) QQ plot\n\n\nCode\n \nlibrary(conflicted)\nconflict_prefer(\"stat_qq_line\", \"qqplotr\", quiet = TRUE)\n\n\n# Perform QQ plots by group\nQQ_Plot &lt;- \nggplot(\n data = Mann_W_U, \n aes(\n   sample = ageAtDx, \n   color = isMale, \n   fill = isMale\n )\n) +\n stat_qq_band(\n   alpha = 0.5, \n   conf = 0.95, \n   qtype = 1, \n   bandType = \"boot\"\n ) +\n stat_qq_line(\n   identity = TRUE\n ) +\n stat_qq_point(\n   col = \"black\"\n ) +\n facet_wrap(\n   ~ isMale, scales = \"free\"\n ) +\n labs(\n   x = \"Theoretical Quantiles\", \n   y = \"Sample Quantiles\"\n ) + theme_bw()\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nQQ_Plot\nError in eval(expr, envir, enclos): object 'QQ_Plot' not found\n\n\n\nstat_qq_line: Draws a reference line based on the data quantiles.\nStat_qq_band: Draws confidence bands based on three methods; “pointwise”/“boot”,“Ks” and “ts”.\n\n\"pointwise\" constructs simultaneous confidence bands based on the normal distribution;\n\"boot\" creates pointwise confidence bands based on a parametric boostrap;\n\"ks\" constructs simultaneous confidence bands based on an inversion of the Kolmogorov-Smirnov test;\n\"ts\" constructs tail-sensitive confidence bandsaldor-noiman2013?\n\nStat_qq_Point: It is a modified version of ggplot: : stat_qq with some parameters adjustments and a new option to detrend the points.\n3) Histogram\nA histogram is the most commonly used graph to show frequency distributions.\n\n\nCode\nHist &lt;- \n ggplot(\nMann_W_U,\naes(\n  x = ageAtDx,\n  fill = isMale\n)\n  ) +\ngeom_histogram() +\nfacet_wrap(~ isMale) \nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\nHist\nError in eval(expr, envir, enclos): object 'Hist' not found\n\n\n3b) Density curve in Histogram\n\nA density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.zach2020?\n\n\nCode\nggplot(\n  Mann_W_U, \n  aes(\n    x = ageAtDx,\n    fill = isMale\n  )\n) + \n  geom_density() +\n  labs(\n    x = \"Age When diabetes is diagnosed\",\n    y = \"Density\",\n    fill = \"Gender\",\n    title = \"A Density Plot of Age when diabetes is diagnosed\",\n    caption = \"Data Source: HEPESE Wave 8 (ICPSR 36578)\"\n  ) +\n  facet_wrap(~isMale)\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nThe density curve provided us idea that our data do not have bell shaped distribution and it is slightly skewed towards left.\n4) Statistical test for normality\n\n\nCode\n   Mann_W_U %&gt;%\n group_by(\n   isMale\n ) %&gt;%\n summarise(\n   `W Stat` = shapiro.test(ageAtDx)$statistic,\n    p.value = shapiro.test(ageAtDx)$p.value,\n   options(scipen = 999)\n )\nError in eval(expr, envir, enclos): object 'Mann_W_U' not found\n\n\nInterpretation\nFrom the above table, we see that the value of the Shapiro-Wilk Test is 0.0006 and 0.000002 which are both less than 0.05, therefore we have enough evidence to reject the null hypothesis and confirm that the data significantly deviate from a normal distribution."
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#mann-whitney-u-test",
    "href": "lessons_original/03_two_sample_mann_whitney.html#mann-whitney-u-test",
    "title": "Mann-Whittney-U-Test-Example",
    "section": "Mann Whitney U Test",
    "text": "Mann Whitney U Test\n\n\nCode\n\nresult &lt;-\n wilcox.test(\n   ageAtDx ~ isMale, \n   data = Mann_W_U, \n   na.rm = TRUE, \n   paired = FALSE, \n   exact = FALSE, \n   conf.int = TRUE\n )\nError in wilcox.test.formula(ageAtDx ~ isMale, data = Mann_W_U, na.rm = TRUE, : cannot use 'paired' in formula method\n\ntest_statistics &lt;- result$statistic\nError in eval(expr, envir, enclos): object 'result' not found\n\np_values &lt;- result$p.value\nError in eval(expr, envir, enclos): object 'result' not found\n\nmethod_used &lt;- result$method\nError in eval(expr, envir, enclos): object 'result' not found\n\nresult_df &lt;- \n  data.frame(\n    Test_Statistic = test_statistics,\n    P_Value = p_values,\n    Method = method_used\n  )\nError in eval(expr, envir, enclos): object 'test_statistics' not found\n\ntbl &lt;- \n  tbl_df(\n    data = result_df\n  ) \nError in eval(expr, envir, enclos): object 'result_df' not found\n\ntbl\nfunction (src, ...) \n{\n    UseMethod(\"tbl\")\n}\n&lt;bytecode: 0x7fbccdb2ee10&gt;\n&lt;environment: namespace:dplyr&gt;"
  }
]